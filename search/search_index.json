{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AutoMIL Documentation","text":"<p>AutoMIL is a flexible, open-source, end-to-end pipeline for training and evaluating Multiple Instance Learning (MIL) models for image classification on whole-slide images (WSIs). It provides a modular command-line interface (CLI) that enables straightforward usage and adaptation to diverse WSI datasets. In addition to the CLI, AutoMIL exposes a Python API for programmatic use, allowing users to build their own custom workflows.</p> <p>Project Repository</p> <p>The full code is accessible on  GitHub</p> AutoMIL and AutoML <p>AutoMIL is deeply rooted in the research field of AutoML, focusing on methods and processes to automate the development and deployment of Machine Learning (ML) solutions in order to make them more accessible to the broader public domain without requiring expert knowledge.</p> <p></p>"},{"location":"#features","title":"Features","text":"<ul> <li> - A well documented and easy to use Command Line Interface</li> <li> - A high-level python API for custom development</li> <li> - Modular project structure for easy adaptation to new datasets</li> <li> - Support for multiple MIL algorithms and model architectures</li> <li> Adaptability to various WSI formats and datasets, including large image sizes and pretiled slides</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<p>To get started, go to the Installation Instructions. If you already installed AutoMIL and want to quickly try it out, follow the Quick Start Guide. If you're especially impatient and already have a dataset ready, you can directly jump into training your first model with one of the following commands:</p> <ul> <li> <p><code>automil run-pipeline /dataset annotations.csv /project</code> - Trains and evaluates a model on the dataset located at <code>/dataset</code> with slide-level labels provided in <code>annotations.csv</code> and saves all outputs to <code>/project</code>.</p> </li> <li> <p><code>automil train /dataset annotations.csv /project</code> - Trains a model on the dataset located at <code>/dataset</code> with slide-level labels provided in <code>annotations.csv</code> and saves the trained model to <code>/project</code>.</p> </li> <li> <p><code>automil predict /model /slides</code> - Generates predictions on the slides located in <code>/slides</code> using the (trained) model located at <code>/model</code>.</p> </li> </ul>"},{"location":"#built-upon-slideflow","title":"Built upon Slideflow","text":"<p>AutoMIL is built on top of the  Slideflow framework for WSI data handling and preprocessing. Slideflow provides efficient data loading, tiling, and augmentation functionalities specifically designed for Whole Slide Images, making it an ideal foundation for MIL model training. AutoMILs contribution lies in automating the selection of hyperparamaters, model architectures, and training procedures specifically tailored for MIL tasks on WSI data, as well as providing a single entry-point so the user experience is streamlined.</p>"},{"location":"#project-layout","title":"Project layout","text":"<p>A typical AutoMIL project has the following layout:</p> <pre><code>project_dir/\n|-- tfrecords/ # Directory containing .tfrecords generated during preprocessing\n|-- models/ # Directory containing trained model checkpoints\n|-- bags/ # Directory containing generated bags \n|-- ensemble/ # Directory containing ensemble predictions\n</code></pre>"},{"location":"api/cli/","title":"Command Line Interface (CLI)","text":"<p>The AutoMIL Command Line Interface (CLI) provides a user-friendly way to interact with the framework directly from the terminal. It allows users to perform various tasks such as data preprocessing, model training, evaluation, and prediction without needing to write any code.</p>"},{"location":"api/cli/#automil.cli.run_pipeline","title":"run_pipeline","text":"<pre><code>run_pipeline(\n    slide_dir: str | Path,\n    annotation_file: str | Path,\n    project_dir: str | Path,\n    patient_column: str,\n    label_column: str,\n    slide_column: str | None,\n    resolutions: str,\n    model: str,\n    k: int,\n    split_file: str | None,\n    transform_labels: bool,\n    is_pretiled: bool,\n    verbose: bool,\n)\n</code></pre> <p>Execute the complete AutoMIL pipeline for whole slide image analysis.</p> <p>This command runs the full AutoMIL workflow, including project setup, dataset preparation, model training with k-fold cross-validation, evaluation, and result visualization.</p> <p>Pipeline stages:</p> <ol> <li>Project setup and configuration</li> <li>Dataset preparation and tile extraction</li> <li>Model training with k-fold cross-validation</li> <li>Model evaluation and ensemble creation</li> <li>Result visualization</li> </ol> <p>Parameters:</p> Name Type Description Default <code>slide_dir</code> <code>str | Path</code> <p>Directory containing whole-slide images or pre-extracted tiles.</p> required <code>annotation_file</code> <code>str | Path</code> <p>CSV file containing slide- or patient-level annotations and labels.</p> required <code>project_dir</code> <code>str | Path</code> <p>Output directory where trained models and intermediate files will be written.</p> required <code>patient_column</code> <code>str</code> <p>Name of the column containing patient identifiers.</p> required <code>label_column</code> <code>str</code> <p>Name of the column containing class labels.</p> required <code>slide_column</code> <code>str | None</code> <p>Name of the column containing slide identifiers.</p> required <code>resolutions</code> <code>str</code> <p>Comma-separated list of resolution presets to train on.</p> required <code>model</code> <code>str</code> <p>Model architecture to train.</p> required <code>k</code> <code>int</code> <p>Number of folds used for k-fold cross-validation.</p> required <code>is_pretiled</code> <code>bool</code> <p>Indicates that the input slides are already tiled.</p> required <code>transform_labels</code> <code>bool</code> <p>If enabled, transforms labels to floating-point values.</p> required <code>verbose</code> <code>bool</code> <p>Enables verbose logging output.</p> required"},{"location":"api/cli/#automil.cli.run_pipeline--examples","title":"Examples","text":"<p>Basic usage with default settings:</p> <pre><code>automil run-pipeline /data/slides /data/annotations.csv ./results\n</code></pre> <p>Multi-resolution training with verbose output:</p> <pre><code>automil run-pipeline -r \"Low,High\" -v /data/slides /data/annotations.csv ./results\n</code></pre> <p>Custom model and k-fold settings:</p> <pre><code>automil run-pipeline -m TransMIL -k 5 /data/slides /data/annotations.csv ./results\n</code></pre> <p>Skip tiling if tiles are pre-extracted:</p> <pre><code>automil run-pipeline -p /data/slides /data/annotations.csv ./results\n</code></pre> <p>Custom column names in the annotation file:</p> <pre><code>automil run-pipeline -pc \"patient_name\" -lc \"diagnosis\" -sc \"slide_name\" /data/slides /data/annotations.csv ./results\n</code></pre> <p>Provide a predefined train-test split:</p> <pre><code>automil run-pipeline --split-file /data/split.json /data/slides /data/annotations.csv ./results\n</code></pre>"},{"location":"api/cli/#automil.cli.run_pipeline--annotation-file-requirements","title":"Annotation file requirements","text":"<p>The annotation file must be a CSV file containing at least the following columns:</p> <ul> <li>Patient identifiers (default column name: <code>patient</code>)</li> <li>Slide identifiers (default column name: <code>slide</code>; optional)</li> <li>Class labels (default column name: <code>label</code>)</li> </ul> <p>By default, AutoMIL looks for columns named <code>patient</code>, <code>slide</code>, and <code>label</code>. These defaults can be overridden using the <code>--patient_column</code>, <code>--slide_column</code>, and <code>--label_column</code> options.</p>"},{"location":"api/cli/#automil.cli.run_pipeline--minimal-annotation-file-example","title":"Minimal annotation file example","text":"<pre><code>patient,slide,label\n001,001_1,0\n001,001_2,0\n002,002,1\n003,003,1\n</code></pre>"},{"location":"api/cli/#automil.cli.run_pipeline--expected-slide-directory-structure","title":"Expected slide directory structure","text":"<p><code>SLIDE_DIR</code> should contain whole slide images in supported formats   such as .svs, .tiff, or .png.   Example structure:</p> <pre><code>/data/slides/\n|-- slide1.svs\n|-- slide2.tiff\n|-- slide3.tiff\n</code></pre> PNG Slide Handling <p>If slides are in PNG, AutoMIL will first convert them to TIFF for easier processing.</p>"},{"location":"api/cli/#automil.cli.run_pipeline--using-pretiled-data","title":"Using pretiled data","text":"<p>If tiles have already been extracted from the slides, use the <code>--is_pretiled</code> flag.   In the case of pretiled data, AutoMIL expects the following directory structure for <code>SLIDE_DIR</code>:</p> <pre><code>/data/slides/\n|-- slide1/\n|    |-- tile_0_0.png\n|    |-- tile_0_1.png\n|    |-- ...\n|-- slide2/\n|    |-- tile_0_0.png\n|    |-- tile_0_1.png\n|    |-- ...\n</code></pre> Slide name matching <p>Tile names are arbitrary but slide subdirectories must match the slide names in ANNOTATION_FILE.</p>"},{"location":"api/cli/#automil.cli.run_pipeline--providing-a-train-test-split","title":"Providing a train-test split","text":"<p>Use the <code>--split-file</code> option to provide a JSON file defining train-test splits.   The JSON file will have the following structure:</p> <pre><code>    {\n    \"train\": [\"slide1\", \"slide2\", ...],\n    \"test\":  [\"slide3\", \"slide4\", ...]\n    }\n</code></pre> <p>or:</p> <pre><code>    {\n    \"train\": [\"slide1\", \"slide2\", ...],\n    \"validation\":  [\"slide3\", \"slide4\", ...]\n    }\n</code></pre>"},{"location":"api/cli/#automil.cli.run_pipeline--output-structure","title":"Output structure","text":"<pre><code>project_dir/\n\u251c\u2500\u2500 bags/           # Extracted tile features\n\u251c\u2500\u2500 models/         # Trained model checkpoints  \n\u251c\u2500\u2500 ensemble/       # Ensemble predictions\n\u251c\u2500\u2500 annotations.csv # Processed annotations\n\u2514\u2500\u2500 results.json    # Performance metrics\n</code></pre> Source code in <code>automil/cli.py</code> <pre><code>@AutoMIL.command(\n    name=\"run-pipeline\", \n    context_settings=CONTEXT_SETTINGS,\n    no_args_is_help=True,\n    help=RUN_PIPELINE_HELP\n)\n@click.argument(\"slide_dir\",        type=click.Path(exists=True, file_okay=False))\n@click.argument(\"annotation_file\",  type=click.Path(exists=True, file_okay=True))\n@click.argument(\"project_dir\",      type=click.Path(file_okay=False))\n@click.option(\n    \"-pc\", \"--patient_column\", type=str, default=\"patient\",\n    help=\"Name of the column containing patient IDs\"\n)\n@click.option(\n    \"-lc\", \"--label_column\", type=str, default=\"label\",\n    help=\"Name of the column containing labels\"\n)\n@click.option(\n    \"-sc\", \"--slide_column\", type=str, default=None,\n    help=\"Name of the column containing slide names\"\n)\n@click.option(\n    \"-r\", \"--resolutions\",\n    type=str,\n    default=\"Low\",\n    help=f\"Comma-separated list of resolution presets to train on. \"\n         f\"Available: {', '.join([choice for choice in RESOLUTION_CHOICES])} \"\n         f\"(e.g., 'Low,High')\"\n)\n@click.option(\n    \"-m\", \"--model\",\n    type=(model_choice := click.Choice([choice for choice in MODEL_CHOICES])),\n    default=model_choice.choices[0],\n    help=f\"Model type to train and evaluate\"\n)\n@click.option(\n    \"-k\", type=int, default=3,\n    help=\"number of folds to train per resolution level\"\n)\n@click.option(\n    \"--split-file\", type=click.Path(file_okay=True), default=\"split.json\",\n    help=\"Path to a .json file defining train-test splits\"\n)\n@click.option(\"-t\", \"--transform_labels\", is_flag=True, help=\"Transforms labels to float values (0.0, 1.0, ...)\")\n@click.option(\"-p\", \"--is-pretiled\",      is_flag=True, help=\"Indicated that the input format is pretiled slides\")\n@click.option(\"-v\", \"--verbose\",          is_flag=True, help=\"Enables additional logging messages\")\ndef run_pipeline(\n    slide_dir:       str | Path,\n    annotation_file: str | Path,\n    project_dir:     str | Path,\n    patient_column:  str,\n    label_column:    str,\n    slide_column:    str | None,\n    resolutions:     str,\n    model:           str,\n    k:               int,\n    split_file:      str | None,\n    transform_labels: bool,\n    is_pretiled:      bool,\n    verbose:          bool\n    ):\n    \"\"\"\n    Execute the complete AutoMIL pipeline for whole slide image analysis.\n\n    This command runs the full AutoMIL workflow, including project setup,\n    dataset preparation, model training with k-fold cross-validation,\n    evaluation, and result visualization.\n\n    Pipeline stages:\n\n    1. Project setup and configuration\n    2. Dataset preparation and tile extraction\n    3. Model training with k-fold cross-validation\n    4. Model evaluation and ensemble creation\n    5. Result visualization\n\n    Args:\n        slide_dir (str | Path):\n            Directory containing whole-slide images or pre-extracted tiles.\n\n        annotation_file (str | Path):\n            CSV file containing slide- or patient-level annotations and labels.\n\n        project_dir (str | Path):\n            Output directory where trained models and intermediate files\n            will be written.\n\n        patient_column (str):\n            Name of the column containing patient identifiers.\n\n        label_column (str):\n            Name of the column containing class labels.\n\n        slide_column (str | None):\n            Name of the column containing slide identifiers.\n\n        resolutions (str):\n            Comma-separated list of resolution presets to train on.\n\n        model (str):\n            Model architecture to train.\n\n        k (int):\n            Number of folds used for k-fold cross-validation.\n\n        is_pretiled (bool):\n            Indicates that the input slides are already tiled.\n\n        transform_labels (bool):\n            If enabled, transforms labels to floating-point values.\n\n        verbose (bool):\n            Enables verbose logging output.\n\n    ### Examples\n\n      Basic usage with default settings:\n\n        automil run-pipeline /data/slides /data/annotations.csv ./results\n\n      Multi-resolution training with verbose output:\n\n        automil run-pipeline -r \"Low,High\" -v /data/slides /data/annotations.csv ./results\n\n      Custom model and k-fold settings:\n\n        automil run-pipeline -m TransMIL -k 5 /data/slides /data/annotations.csv ./results\n\n      Skip tiling if tiles are pre-extracted:\n\n        automil run-pipeline -p /data/slides /data/annotations.csv ./results\n\n      Custom column names in the annotation file:\n\n        automil run-pipeline -pc \"patient_name\" -lc \"diagnosis\" -sc \"slide_name\" /data/slides /data/annotations.csv ./results\n\n      Provide a predefined train-test split:\n\n        automil run-pipeline --split-file /data/split.json /data/slides /data/annotations.csv ./results\n\n    ### Annotation file requirements\n\n    The annotation file must be a CSV file containing at least the following columns:\n\n    - Patient identifiers (default column name: `patient`)\n    - Slide identifiers (default column name: `slide`; optional)\n    - Class labels (default column name: `label`)\n\n    By default, AutoMIL looks for columns named `patient`, `slide`, and `label`.\n    These defaults can be overridden using the `--patient_column`,\n    `--slide_column`, and `--label_column` options.\n\n    ### Minimal annotation file example\n        patient,slide,label\n        001,001_1,0\n        001,001_2,0\n        002,002,1\n        003,003,1\n\n    ### Expected slide directory structure\n      `SLIDE_DIR` should contain whole slide images in supported formats\n      such as .svs, .tiff, or .png.\n      Example structure:\n\n        /data/slides/\n        |-- slide1.svs\n        |-- slide2.tiff\n        |-- slide3.tiff\n\n    ??? Note \"PNG Slide Handling\"\n        If slides are in PNG, AutoMIL will first convert them to TIFF for easier processing.\n\n    ### Using pretiled data\n      If tiles have already been extracted from the slides, use the `--is_pretiled` flag.\n      In the case of pretiled data, AutoMIL expects the following directory structure for `SLIDE_DIR`:\n\n        /data/slides/\n        |-- slide1/\n        |    |-- tile_0_0.png\n        |    |-- tile_0_1.png\n        |    |-- ...\n        |-- slide2/\n        |    |-- tile_0_0.png\n        |    |-- tile_0_1.png\n        |    |-- ...\n\n    ??? Note \"Slide name matching\"\n        Tile names are arbitrary but slide subdirectories must match the slide names in ANNOTATION_FILE.\n\n    ### Providing a train-test split\n      Use the `--split-file` option to provide a JSON file defining train-test splits.\n      The JSON file will have the following structure:\n\n            {\n            \"train\": [\"slide1\", \"slide2\", ...],\n            \"test\":  [\"slide3\", \"slide4\", ...]\n            }\n\n      or:\n\n            {\n            \"train\": [\"slide1\", \"slide2\", ...],\n            \"validation\":  [\"slide3\", \"slide4\", ...]\n            }\n\n    ### Output structure\n\n        project_dir/\n        \u251c\u2500\u2500 bags/           # Extracted tile features\n        \u251c\u2500\u2500 models/         # Trained model checkpoints  \n        \u251c\u2500\u2500 ensemble/       # Ensemble predictions\n        \u251c\u2500\u2500 annotations.csv # Processed annotations\n        \u2514\u2500\u2500 results.json    # Performance metrics\n\n    \"\"\"\n    import slideflow as sf\n\n    from .dataset import Dataset\n    from .evaluation import Evaluator\n    from .project import Project\n    from .trainer import Trainer\n    from .util import (INFO_CLR, RESOLUTION_PRESETS, LogLevel, ModelType,\n                       get_vlog)\n    from .util.backend import configure_image_backend, has_png_slides\n    from .util.pretiled import is_input_pretiled\n\n    # Getting a verbose logger\n    vlog = get_vlog(verbose)\n    sf.setLoggingLevel(20) # INFO: 20, DEBUG: 10\n\n    # Logging the executed command\n    command = \" \".join(sys.argv)\n    vlog(f\"Executing command: [{INFO_CLR}]{command}[/]\")\n\n    # Define some paths\n    bags_dir = Path(project_dir) / \"bags\"\n    models_dir = Path(project_dir) / \"models\"\n    ensemble_dir = Path(project_dir) / \"ensemble\"\n\n    # Some type coercion\n    slide_dir = Path(slide_dir)\n    annotation_file = Path(annotation_file)\n    project_dir = Path(project_dir)\n\n    try:\n\n        # === 1. Parsing === #\n        # Parse given string resolutions into list of RESOLUTION_PRESETS\n        resolution_presets: list[RESOLUTION_PRESETS] = []\n        for res in [r.strip() for r in resolutions.split(',')]: resolution_presets.append(RESOLUTION_PRESETS[res])\n        vlog(f\"Using resolution presets: [{INFO_CLR}]{[preset.name for preset in resolution_presets]}[/]\")\n\n        # Parse the model type\n        model_type = ModelType[model]\n        vlog(f\"Using model type: [{INFO_CLR}]{model_type.name}[/]\")\n\n        # === 2. Image Backend Configuration === #\n        png_slides_present = has_png_slides(slide_dir)\n\n        tiff_conversion = configure_image_backend(\n            slide_dir=slide_dir,\n            needs_png_conversion=png_slides_present,\n            verbose=verbose,\n        )\n\n        # === 3. Project Creation And Setup === #\n        project_setup = Project(\n            Path(project_dir),\n            Path(annotation_file),\n            Path(slide_dir),\n            patient_column,\n            label_column,\n            slide_column,\n            transform_labels,\n            verbose,\n        )\n\n        # Prepare slideflow project object\n        project = project_setup.prepare_project()\n        # We'll need the label map and slide ids for the dataset setup\n        label_map = project_setup.label_map\n        slide_ids = project_setup.slide_ids\n\n        project_setup.summary()\n\n        # === 4. Setup Dataset Sources ===\n        # Determine if the slide_dir has pretiled slides\n        if not is_pretiled: # is_pretiled == False means the flag was not set\n            is_pretiled = is_input_pretiled(\n                slide_dir,\n                slide_ids\n            )\n\n        datasets: dict[str, sf.Dataset] = {}\n        for preset in resolution_presets:\n            vlog(f\"Setting up dataset for resolution preset: [{INFO_CLR}]{preset.name}[/]\")\n\n            dataset = Dataset(\n                project,\n                preset,\n                label_map,\n                slide_dir=Path(slide_dir),\n                bags_dir=Path(project_dir) / \"bags\",\n                is_pretiled=is_pretiled,\n                tiff_conversion=tiff_conversion,\n                verbose=verbose\n            )\n            dataset.summary()\n            datasets[preset.name] = dataset.prepare_dataset_source()\n            vlog(f\"Dataset setup complete for resolution preset: [{INFO_CLR}]{preset.name}[/]\")\n\n        # === 5. Prepare (or Load) Train/Test Split === #\n        dataset = datasets[resolution_presets[0].name]\n        train, test = dataset.split(\n            labels=\"label\",\n            val_fraction=0.2,\n            splits=split_file\n        )\n\n        # === 6. Model Training === #\n        for resolution in resolution_presets:\n            vlog(f\"Train/Test split for resolution preset [{INFO_CLR}]{resolution.name}[/]: \"\n                 f\"[{INFO_CLR}]{len(train.slides())}[/] train slides\"\n            )\n\n            train, val = train.split(\n                labels=\"label\",\n                val_fraction=0.2\n            )\n\n            trainer = Trainer(\n                bags_dir,\n                project,\n                train,\n                val,\n                model=model_type,\n                k=k,\n                epochs=300\n            )\n            trainer.train_k_fold()\n            trainer.summary()\n\n        # === 7. Model Evaluation === #\n        evaluator = Evaluator(\n            test,\n            models_dir,\n            ensemble_dir,\n            bags_dir,\n            verbose=verbose\n        )\n\n        evaluator.evaluate_models(generate_attention_heatmaps=True)\n        evaluator.create_ensemble_predictions(\n            output_path=Path(project.root) / \"ensemble_predictions.csv\"\n        )\n\n        evaluator.compare_models()\n        evaluator.generate_plots(\n            save_path=Path(project.root) / \"figures\",\n            model_paths=None\n        )\n\n    except Exception as e:\n        tb = traceback.format_exc()\n        vlog(tb, LogLevel.ERROR)\n        vlog(f\"Error: {e}\", LogLevel.ERROR)\n        return\n</code></pre>"},{"location":"api/cli/#automil.cli.train","title":"train","text":"<pre><code>train(\n    slide_dir: str | Path,\n    annotation_file: str | Path,\n    project_dir: str | Path,\n    patient_column: str,\n    label_column: str,\n    slide_column: str | None,\n    resolutions: str,\n    model: str,\n    k: int,\n    is_pretiled: bool,\n    transform_labels: bool,\n    verbose: bool,\n)\n</code></pre> <p>Train one or more MIL models on a given dataset.</p> <p>This command initializes an AutoMIL project, prepares the dataset, and trains MIL models using k-fold cross-validation. Training can be performed at one or multiple resolution presets.</p> <p>Pipeline stages:</p> <ol> <li>Project setup and configuration</li> <li>Dataset preparation and tile extraction</li> <li>Model training with k-fold cross-validation</li> </ol> <p>Parameters:</p> Name Type Description Default <code>slide_dir</code> <code>str | Path</code> <p>Directory containing whole-slide images or pre-extracted tiles.</p> required <code>annotation_file</code> <code>str | Path</code> <p>CSV file containing slide- or patient-level annotations and labels.</p> required <code>project_dir</code> <code>str | Path</code> <p>Output directory where trained models and intermediate files will be written.</p> required <code>patient_column</code> <code>str</code> <p>Name of the column containing patient identifiers.</p> required <code>label_column</code> <code>str</code> <p>Name of the column containing class labels.</p> required <code>slide_column</code> <code>str | None</code> <p>Name of the column containing slide identifiers.</p> required <code>resolutions</code> <code>str</code> <p>Comma-separated list of resolution presets to train on.</p> required <code>model</code> <code>str</code> <p>Model architecture to train.</p> required <code>k</code> <code>int</code> <p>Number of folds used for k-fold cross-validation.</p> required <code>is_pretiled</code> <code>bool</code> <p>Indicates that the input slides are already tiled.</p> required <code>transform_labels</code> <code>bool</code> <p>If enabled, transforms labels to floating-point values.</p> required <code>verbose</code> <code>bool</code> <p>Enables verbose logging output.</p> required"},{"location":"api/cli/#automil.cli.train--examples","title":"Examples","text":"<p>Basic usage with default settings:</p> <pre><code>automil train /data/slides /data/annotations.csv ./results\n</code></pre> <p>Multi-resolution training with verbose output::</p> <pre><code>automil train -r \"Low,High\" -v /data/slides /data/annotations.csv ./results\n</code></pre> <p>Custom model and 5-fold configuration:</p> <pre><code>automil train -m TransMIL -k 5 /data/slides /data/annotations.csv ./results\n</code></pre> <p>Using pre-tiled slides::</p> <pre><code>automil train -p /data/slides /data/annotations.csv ./results\n</code></pre>"},{"location":"api/cli/#automil.cli.train--annotation-file-requirements","title":"Annotation file requirements","text":"<p>The annotation file must be a CSV file containing at least the following columns:</p> <ul> <li>Patient identifiers (default column name: <code>patient</code>)</li> <li>Slide identifiers (default column name: <code>slide</code>; optional)</li> <li>Class labels (default column name: <code>label</code>)</li> </ul> <p>By default, AutoMIL looks for columns named <code>patient</code>, <code>slide</code>, and <code>label</code>. These defaults can be overridden using the <code>--patient_column</code>, <code>--slide_column</code>, and <code>--label_column</code> options.</p>"},{"location":"api/cli/#automil.cli.train--minimal-annotation-file-example","title":"Minimal annotation file example","text":"<pre><code>patient,slide,label\n001,001_1,0\n001,001_2,0\n002,002,1\n003,003,1\n</code></pre>"},{"location":"api/cli/#automil.cli.train--expected-slide-directory-structure","title":"Expected slide directory structure","text":"<p><code>SLIDE_DIR</code> should contain whole slide images in supported formats   such as .svs, .tiff, or .png.   Example structure:</p> <pre><code>/data/slides/\n|-- slide1.svs\n|-- slide2.tiff\n|-- slide3.tiff\n</code></pre> PNG Slide Handling <p>If slides are in PNG, AutoMIL will first convert them to TIFF for easier processing.</p>"},{"location":"api/cli/#automil.cli.train--using-pretiled-data","title":"Using pretiled data","text":"<p>If tiles have already been extracted from the slides, use the <code>--is_pretiled</code> flag.   In the case of pretiled data, AutoMIL expects the following directory structure for <code>SLIDE_DIR</code>:</p> <pre><code>/data/slides/\n|-- slide1/\n|    |-- tile_0_0.png\n|    |-- tile_0_1.png\n|    |-- ...\n|-- slide2/\n|    |-- tile_0_0.png\n|    |-- tile_0_1.png\n|    |-- ...\n</code></pre> Slide name matching <p>Tile names are arbitrary but slide subdirectories must match the slide names in ANNOTATION_FILE.</p>"},{"location":"api/cli/#automil.cli.train--providing-a-train-test-split","title":"Providing a train-test split","text":"<p>Use the <code>--split-file</code> option to provide a JSON file defining train-test splits.   The JSON file will have the following structure:</p> <pre><code>    {\n    \"train\": [\"slide1\", \"slide2\", ...],\n    \"test\":  [\"slide3\", \"slide4\", ...]\n    }\n</code></pre> <p>or:</p> <pre><code>    {\n    \"train\": [\"slide1\", \"slide2\", ...],\n    \"validation\":  [\"slide3\", \"slide4\", ...]\n    }\n</code></pre>"},{"location":"api/cli/#automil.cli.train--output-structure","title":"Output structure","text":"<pre><code>project_dir/\n\u251c\u2500\u2500 bags/           # Extracted tile features\n\u251c\u2500\u2500 models/         # Trained model checkpoints  \n\u251c\u2500\u2500 ensemble/       # Ensemble predictions\n\u251c\u2500\u2500 annotations.csv # Processed annotations\n\u2514\u2500\u2500 results.json    # Performance metrics\n</code></pre> Source code in <code>automil/cli.py</code> <pre><code>@AutoMIL.command(\n    name=\"train\",\n    context_settings=CONTEXT_SETTINGS,\n    no_args_is_help=True,\n    help=TRAIN_HELP\n)\n@click.argument(\"slide_dir\",        type=click.Path(exists=True, file_okay=False))\n@click.argument(\"annotation_file\",  type=click.Path(exists=True, file_okay=True))\n@click.argument(\"project_dir\",      type=click.Path(file_okay=False))\n@click.option(\n    \"-pc\", \"--patient_column\", type=str, default=\"patient\",\n    help=\"Name of the column containing patient IDs\"\n)\n@click.option(\n    \"-lc\", \"--label_column\", type=str, default=\"label\",\n    help=\"Name of the column containing labels\"\n)\n@click.option(\n    \"-sc\", \"--slide_column\", type=str, default=None,\n    help=\"Name of the column containing slide names\"\n)\n@click.option(\n    \"-r\", \"--resolutions\",\n    type=str,\n    default=\"Low\",\n    help=f\"Comma-separated list of resolution presets to train on. \"\n         f\"Available: {', '.join([choice for choice in RESOLUTION_CHOICES])} \"\n         f\"(e.g., 'Low,High')\"\n)\n@click.option(\n    \"-m\", \"--model\",\n    type=(model_choice := click.Choice([choice for choice in MODEL_CHOICES])),\n    default=model_choice.choices[0],\n    help=f\"Model type to train and evaluate\"\n)\n@click.option(\n    \"-k\", type=int, default=3,\n    help=\"number of folds to train per resolution level\"\n)\n@click.option(\"-p\", \"--is-pretiled\",      is_flag=True, help=\"Indicated that the input format is pretiled slides\")\n@click.option(\"-t\", \"--transform_labels\", is_flag=True, help=\"Transforms labels to float values (0.0, 1.0, ...)\")\n@click.option(\"-v\", \"--verbose\",          is_flag=True, help=\"Enables additional logging messages\")\ndef train(\n    slide_dir:       str | Path,\n    annotation_file: str | Path,\n    project_dir:     str | Path,\n    patient_column:  str,\n    label_column:    str,\n    slide_column:    str | None,\n    resolutions:     str,\n    model:           str,\n    k:               int,\n    is_pretiled:      bool,\n    transform_labels: bool,\n    verbose:          bool\n):\n    \"\"\"\n    Train one or more MIL models on a given dataset.\n\n    This command initializes an AutoMIL project, prepares the dataset,\n    and trains MIL models using k-fold cross-validation. Training can be\n    performed at one or multiple resolution presets.\n\n    Pipeline stages:\n\n    1. Project setup and configuration\n    2. Dataset preparation and tile extraction\n    3. Model training with k-fold cross-validation\n\n    Args:\n        slide_dir (str | Path):\n            Directory containing whole-slide images or pre-extracted tiles.\n\n        annotation_file (str | Path):\n            CSV file containing slide- or patient-level annotations and labels.\n\n        project_dir (str | Path):\n            Output directory where trained models and intermediate files\n            will be written.\n\n        patient_column (str):\n            Name of the column containing patient identifiers.\n\n        label_column (str):\n            Name of the column containing class labels.\n\n        slide_column (str | None):\n            Name of the column containing slide identifiers.\n\n        resolutions (str):\n            Comma-separated list of resolution presets to train on.\n\n        model (str):\n            Model architecture to train.\n\n        k (int):\n            Number of folds used for k-fold cross-validation.\n\n        is_pretiled (bool):\n            Indicates that the input slides are already tiled.\n\n        transform_labels (bool):\n            If enabled, transforms labels to floating-point values.\n\n        verbose (bool):\n            Enables verbose logging output.\n\n    ### Examples\n      Basic usage with default settings:\n\n        automil train /data/slides /data/annotations.csv ./results\n\n      Multi-resolution training with verbose output::\n\n        automil train -r \"Low,High\" -v /data/slides /data/annotations.csv ./results\n\n      Custom model and 5-fold configuration:\n\n        automil train -m TransMIL -k 5 /data/slides /data/annotations.csv ./results\n\n      Using pre-tiled slides::\n\n        automil train -p /data/slides /data/annotations.csv ./results\n\n    ### Annotation file requirements\n\n    The annotation file must be a CSV file containing at least the following columns:\n\n    - Patient identifiers (default column name: `patient`)\n    - Slide identifiers (default column name: `slide`; optional)\n    - Class labels (default column name: `label`)\n\n    By default, AutoMIL looks for columns named `patient`, `slide`, and `label`.\n    These defaults can be overridden using the `--patient_column`,\n    `--slide_column`, and `--label_column` options.\n\n    ### Minimal annotation file example\n        patient,slide,label\n        001,001_1,0\n        001,001_2,0\n        002,002,1\n        003,003,1\n\n    ### Expected slide directory structure\n      `SLIDE_DIR` should contain whole slide images in supported formats\n      such as .svs, .tiff, or .png.\n      Example structure:\n\n        /data/slides/\n        |-- slide1.svs\n        |-- slide2.tiff\n        |-- slide3.tiff\n\n    ??? Note \"PNG Slide Handling\"\n        If slides are in PNG, AutoMIL will first convert them to TIFF for easier processing.\n\n    ### Using pretiled data\n      If tiles have already been extracted from the slides, use the `--is_pretiled` flag.\n      In the case of pretiled data, AutoMIL expects the following directory structure for `SLIDE_DIR`:\n\n        /data/slides/\n        |-- slide1/\n        |    |-- tile_0_0.png\n        |    |-- tile_0_1.png\n        |    |-- ...\n        |-- slide2/\n        |    |-- tile_0_0.png\n        |    |-- tile_0_1.png\n        |    |-- ...\n\n    ??? Note \"Slide name matching\"\n        Tile names are arbitrary but slide subdirectories must match the slide names in ANNOTATION_FILE.\n\n    ### Providing a train-test split\n      Use the `--split-file` option to provide a JSON file defining train-test splits.\n      The JSON file will have the following structure:\n\n            {\n            \"train\": [\"slide1\", \"slide2\", ...],\n            \"test\":  [\"slide3\", \"slide4\", ...]\n            }\n\n      or:\n\n            {\n            \"train\": [\"slide1\", \"slide2\", ...],\n            \"validation\":  [\"slide3\", \"slide4\", ...]\n            }\n\n    ### Output structure\n\n        project_dir/\n        \u251c\u2500\u2500 bags/           # Extracted tile features\n        \u251c\u2500\u2500 models/         # Trained model checkpoints  \n        \u251c\u2500\u2500 ensemble/       # Ensemble predictions\n        \u251c\u2500\u2500 annotations.csv # Processed annotations\n        \u2514\u2500\u2500 results.json    # Performance metrics\n\n    \"\"\"\n\n    import slideflow as sf\n\n    from .dataset import Dataset\n    from .project import Project\n    from .trainer import Trainer\n    from .util import (INFO_CLR, RESOLUTION_PRESETS, LogLevel, ModelType,\n                       get_vlog)\n    from .util.backend import configure_image_backend, has_png_slides\n    from .util.pretiled import is_input_pretiled\n\n    # Getting a verbose logger\n    vlog = get_vlog(verbose)\n    sf.setLoggingLevel(20) # INFO: 20, DEBUG: 10\n\n    # Logging the executed command\n    command = \" \".join(sys.argv)\n    vlog(f\"Executing command: [{INFO_CLR}]{command}[/]\")\n\n    # Define some paths\n    bags_dir = Path(project_dir) / \"bags\"\n\n    # Some type coercion\n    slide_dir = Path(slide_dir)\n    annotation_file = Path(annotation_file)\n    project_dir = Path(project_dir)\n\n    try:\n\n        # === 1. Parsing === #\n        # Parse given string resolutions into list of RESOLUTION_PRESETS\n        resolution_presets: list[RESOLUTION_PRESETS] = []\n        for res in [r.strip() for r in resolutions.split(',')]: resolution_presets.append(RESOLUTION_PRESETS[res])\n        vlog(f\"Using resolution presets: [{INFO_CLR}]{[preset.name for preset in resolution_presets]}[/]\")\n\n        # Parse the model type\n        model_type = ModelType[model]\n        vlog(f\"Using model type: [{INFO_CLR}]{model_type.name}[/]\")\n\n        # === 2. Image Backend Configuration === #\n        png_slides_present = has_png_slides(slide_dir)\n\n        tiff_conversion = configure_image_backend(\n            slide_dir=slide_dir,\n            needs_png_conversion=png_slides_present,\n            verbose=verbose,\n        )\n\n        # === 3. Project Creation And Setup === #\n        project_setup = Project(\n            Path(project_dir),\n            Path(annotation_file),\n            Path(slide_dir),\n            patient_column,\n            label_column,\n            slide_column,\n            transform_labels=transform_labels,\n            verbose=verbose,\n        )\n        # Prepare slideflow project object\n        project = project_setup.prepare_project()\n        # We'll need the label map and slide ids for the dataset setup\n        label_map = project_setup.label_map\n        slide_ids = project_setup.slide_ids\n\n        project_setup.summary()\n\n        # === 4. Setup Dataset Sources ===\n        # Determine if the slide_dir has pretiled slides\n        if not is_pretiled: # is_pretiled == False means the flag was not set\n            is_pretiled = is_input_pretiled(\n                slide_dir,\n                slide_ids\n            )\n\n        datasets: dict[str, sf.Dataset] = {}\n        for resolution in resolution_presets:\n            vlog(f\"Setting up dataset for resolution preset: [{INFO_CLR}]{resolution.name}[/]\")\n\n            dataset = Dataset(\n                project,\n                resolution,\n                label_map,\n                slide_dir=Path(slide_dir),\n                bags_dir=Path(project_dir) / \"bags\",\n                is_pretiled=is_pretiled,\n                tiff_conversion=tiff_conversion,\n                verbose=verbose\n            )\n            dataset.summary()\n            datasets[resolution.name] = dataset.prepare_dataset_source()\n            vlog(f\"Dataset setup complete for resolution preset: [{INFO_CLR}]{resolution.name}[/]\")\n\n        # === 5. Model Training === #\n        for resolution in resolution_presets:\n            dataset = datasets[resolution.name]\n            vlog(f\"Train/Test split for resolution preset '[{INFO_CLR}]{resolution.name}[/]': \"\n                 f\"[{INFO_CLR}]{len(dataset.slides())}[/] train slides\"\n            )\n\n            train, val = dataset.split(\n                labels=\"label\",\n                val_fraction=0.2\n            )\n\n            trainer = Trainer(\n                bags_dir,\n                project,\n                train,\n                val,\n                model=model_type,\n                k=k,\n                epochs=300\n            )\n            trainer.train_k_fold()\n            trainer.summary()\n\n    except Exception as e:\n        tb = traceback.format_exc()\n        vlog(tb, LogLevel.ERROR)\n        vlog(f\"Error: {e}\", LogLevel.ERROR)\n        return\n</code></pre>"},{"location":"api/cli/#automil.cli.predict","title":"predict","text":"<pre><code>predict(\n    slide_dir: str | Path,\n    annotation_file: str | Path,\n    bags_dir: str | Path,\n    model_dir: str | Path,\n    output_dir: str | Path,\n    patient_column: str,\n    label_column: str,\n    slide_column: str | None,\n    verbose: bool,\n)\n</code></pre> <p>Generate predictions using one or more trained MIL models.</p> <p>This command loads trained model checkpoints and generates predictions for the slides in <code>SLIDE_DIR</code> using precomputed tile feature bags from <code>BAGS_DIR</code>. Predictions are written to the specified output directory.</p> <p>Parameters:</p> Name Type Description Default <code>slide_dir</code> <code>str | Path</code> <p>Directory containing whole-slide images.</p> required <code>annotation_file</code> <code>str | Path</code> <p>CSV file containing slide- or patient-level annotations and labels.</p> required <code>bags_dir</code> <code>str | Path</code> <p>Directory containing extracted tile feature bags.</p> required <code>model_dir</code> <code>str | Path</code> <p>Directory containing trained model checkpoints.</p> required <code>output_dir</code> <code>str | Path</code> <p>Directory to which prediction files will be written.</p> required <code>patient_column</code> <code>str</code> <p>Name of the column containing patient identifiers.</p> required <code>label_column</code> <code>str</code> <p>Name of the column containing class labels.</p> required <code>slide_column</code> <code>str | None</code> <p>Name of the column containing slide identifiers.</p> required <code>verbose</code> <code>bool</code> <p>Enables verbose logging output.</p> required"},{"location":"api/cli/#automil.cli.predict--examples","title":"Examples","text":"<p>Basic usage with multiple models:</p> <pre><code>automil predict /data/slides /data/annotations.csv /data/bags /data/models -o ./predictions\n</code></pre> <p>Generate predictions with a single model:</p> <pre><code>automil predict /data/slides /data/annotations.csv /data/bags /data/models/model_1 -v\n</code></pre> <p>Override annotation column names:</p> <pre><code>automil predict -pc \"patient_id\" -lc \"outcome\" -sc \"slide_id\"             /data/slides /data/annotations.csv /data/bags /data/models/model_1             -o ./predictions\n</code></pre>"},{"location":"api/cli/#automil.cli.predict--expected-model-directory-structure","title":"Expected model directory structure","text":"<p><code>MODEL_DIR</code> may either point to a single model directory or to a parent directory containing multiple model subdirectories.</p> <p>Single model example:</p> <pre><code>/data/models/model_1/\n|-- best_valid.pth\n|-- ...\n</code></pre> <p>Multiple models example:</p> <pre><code>/data/models/\n|-- model_1/\n|    |-- best_valid.pth\n|-- model_2/\n|    |-- best_valid.pth\n|    |-- ...\n</code></pre> Multiple models <p>When multiple models are provided, AutoMIL generates a separate prediction file for each model.</p>"},{"location":"api/cli/#automil.cli.predict--annotation-file-requirements","title":"Annotation file requirements","text":"<p>The annotation file must be a CSV file containing at least the following columns:</p> <ul> <li>Patient identifiers (default column name: <code>patient</code>)</li> <li>Slide identifiers (default column name: <code>slide</code>; optional)</li> <li>Class labels (default column name: <code>label</code>)</li> </ul> <p>By default, AutoMIL looks for columns named <code>patient</code>, <code>slide</code>, and <code>label</code>. These defaults can be overridden using the <code>--patient_column</code>, <code>--slide_column</code>, and <code>--label_column</code> options.</p>"},{"location":"api/cli/#automil.cli.predict--minimal-annotation-file-example","title":"Minimal annotation file example","text":"<pre><code>patient,slide,label\n001,001_1,0\n001,001_2,0\n002,002,1\n003,003,1\n</code></pre>"},{"location":"api/cli/#automil.cli.predict--output-directory-format","title":"Output directory format","text":"<p><code>OUTPUT_DIR</code> must be a directory path. Prediction results are saved as separate <code>.csv</code> or <code>.parquet</code> files inside this directory.</p> <p>When multiple models are used, output files include a suffix indicating the corresponding model.</p> Source code in <code>automil/cli.py</code> <pre><code>@AutoMIL.command(\n    name=\"predict\",\n    context_settings=CONTEXT_SETTINGS,\n    no_args_is_help=True,\n    help=PREDICT_HELP\n)\n@click.argument(\"slide_dir\",    type=click.Path(exists=True, file_okay=False))\n@click.argument(\"annotation_file\",  type=click.Path(exists=True, file_okay=True))\n@click.argument(\"bags_dir\",     type=click.Path(exists=True, file_okay=False))\n@click.argument(\"model_dir\",    type=click.Path(exists=True, file_okay=False))\n@click.option(\n    \"-o\", \"--output-dir\", \n    type=click.Path(file_okay=True), default=\"predictions\",\n    help=\"Directory to which to save predictions (should either be .csv or .parquet)\"\n)\n@click.option(\n    \"-pc\", \"--patient_column\", type=str, default=\"patient\",\n    help=\"Name of the column containing patient IDs\"\n)\n@click.option(\n    \"-lc\", \"--label_column\", type=str, default=\"label\",\n    help=\"Name of the column containing labels\"\n)\n@click.option(\n    \"-sc\", \"--slide_column\", type=str, default=None,\n    help=\"Name of the column containing slide names\"\n)\n@click.option(\"-v\", \"--verbose\", is_flag=True, help=\"Enables additional logging messages\")\ndef predict(\n    slide_dir:   str | Path,\n    annotation_file: str | Path,\n    bags_dir:    str | Path,\n    model_dir:   str | Path,\n    output_dir: str | Path,\n    patient_column:  str,\n    label_column:    str,\n    slide_column:    str | None,\n    verbose:     bool\n):\n    \"\"\"\n    Generate predictions using one or more trained MIL models.\n\n    This command loads trained model checkpoints and generates predictions\n    for the slides in `SLIDE_DIR` using precomputed tile feature bags from\n    `BAGS_DIR`. Predictions are written to the specified output directory.\n\n    Args:\n        slide_dir (str | Path):\n            Directory containing whole-slide images.\n\n        annotation_file (str | Path):\n            CSV file containing slide- or patient-level annotations and labels.\n\n        bags_dir (str | Path):\n            Directory containing extracted tile feature bags.\n\n        model_dir (str | Path):\n            Directory containing trained model checkpoints.\n\n        output_dir (str | Path):\n            Directory to which prediction files will be written.\n\n        patient_column (str):\n            Name of the column containing patient identifiers.\n\n        label_column (str):\n            Name of the column containing class labels.\n\n        slide_column (str | None):\n            Name of the column containing slide identifiers.\n\n        verbose (bool):\n            Enables verbose logging output.\n\n    ### Examples\n\n    Basic usage with multiple models:\n\n        automil predict /data/slides /data/annotations.csv /data/bags /data/models -o ./predictions\n\n    Generate predictions with a single model:\n\n        automil predict /data/slides /data/annotations.csv /data/bags /data/models/model_1 -v\n\n    Override annotation column names:\n\n        automil predict -pc \"patient_id\" -lc \"outcome\" -sc \"slide_id\" \\\n            /data/slides /data/annotations.csv /data/bags /data/models/model_1 \\\n            -o ./predictions\n\n    ### Expected model directory structure\n\n    `MODEL_DIR` may either point to a single model directory or to a parent\n    directory containing multiple model subdirectories.\n\n    Single model example:\n\n        /data/models/model_1/\n        |-- best_valid.pth\n        |-- ...\n\n    Multiple models example:\n\n        /data/models/\n        |-- model_1/\n        |    |-- best_valid.pth\n        |-- model_2/\n        |    |-- best_valid.pth\n        |    |-- ...\n\n    ??? Note \"Multiple models\"\n        When multiple models are provided, AutoMIL generates a separate\n        prediction file for each model.\n\n    ### Annotation file requirements\n\n    The annotation file must be a CSV file containing at least the following columns:\n\n    - Patient identifiers (default column name: `patient`)\n    - Slide identifiers (default column name: `slide`; optional)\n    - Class labels (default column name: `label`)\n\n    By default, AutoMIL looks for columns named `patient`, `slide`, and `label`.\n    These defaults can be overridden using the `--patient_column`,\n    `--slide_column`, and `--label_column` options.\n\n    ### Minimal annotation file example\n\n        patient,slide,label\n        001,001_1,0\n        001,001_2,0\n        002,002,1\n        003,003,1\n\n    ### Output directory format\n\n    `OUTPUT_DIR` must be a directory path. Prediction results are saved as\n    separate `.csv` or `.parquet` files inside this directory.\n\n    When multiple models are used, output files include a suffix indicating\n    the corresponding model.\n    \"\"\"\n\n    import slideflow as sf\n\n    from .evaluation import Evaluator\n    from .project import Project\n    from .util import INFO_CLR, LogLevel, get_vlog\n\n    # Getting a verbose logger\n    vlog = get_vlog(verbose)\n    sf.setLoggingLevel(20) # INFO: 20, DEBUG: 10\n\n    # Logging the executed command\n    command = \" \".join(sys.argv)\n    vlog(f\"Executing command: [{INFO_CLR}]{command}[/]\")\n\n    # Some type coercion\n    slide_dir = Path(slide_dir)\n    bags_dir =  Path(bags_dir)\n    model_dir = Path(model_dir)\n    output_dir = Path(output_dir)\n\n    # Setup output folder as project (modifies annotation file)\n    project = Project(\n        Path(output_dir),\n        Path(annotation_file),\n        Path(slide_dir),\n        patient_column,\n        label_column,\n        slide_column,\n        transform_labels=False,\n        verbose=verbose,\n    )\n    project.setup_project_scaffold()\n    annotation_file = project.modified_annotations_file\n\n    # Create a minimal dataset (needed for prediction)\n    dataset = sf.Dataset(\n        slides=str(slide_dir),\n        annotations=str(annotation_file)\n    )\n\n    # Generate predictions\n    try:\n        evaluator = Evaluator(\n            dataset,\n            model_dir,\n            output_dir,\n            bags_dir,\n            verbose=verbose\n        )\n        evaluator.generate_predictions()\n\n    except Exception as e:\n        tb = traceback.format_exc()\n        vlog(tb, LogLevel.ERROR)\n        vlog(f\"Error: {e}\", LogLevel.ERROR)\n        return\n</code></pre>"},{"location":"api/cli/#automil.cli.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    slide_dir: str | Path,\n    annotation_file: str | Path,\n    bags_dir: str | Path,\n    model_dir: str | Path,\n    output_dir: str | Path,\n    patient_column: str,\n    label_column: str,\n    slide_column: str | None,\n    verbose: bool,\n)\n</code></pre> <p>Evaluate one or more trained MIL models on a labeled dataset.</p> <p>This command generates predictions for the slides in <code>SLIDE_DIR</code> using trained models from <code>MODEL_DIR</code> and corresponding tile feature bags from <code>BAGS_DIR</code>. The predictions are evaluated against the provided annotations, and summary metrics and plots are generated.</p> <p>Parameters:</p> Name Type Description Default <code>slide_dir</code> <code>str | Path</code> <p>Directory containing whole-slide images.</p> required <code>annotation_file</code> <code>str | Path</code> <p>CSV file containing slide- or patient-level annotations and labels.</p> required <code>bags_dir</code> <code>str | Path</code> <p>Directory containing extracted tile feature bags.</p> required <code>model_dir</code> <code>str | Path</code> <p>Directory containing trained model checkpoints.</p> required <code>output_dir</code> <code>str | Path</code> <p>Directory to which evaluation results will be written.</p> required <code>patient_column</code> <code>str</code> <p>Name of the column containing patient identifiers.</p> required <code>label_column</code> <code>str</code> <p>Name of the column containing class labels.</p> required <code>slide_column</code> <code>str | None</code> <p>Name of the column containing slide identifiers.</p> required <code>verbose</code> <code>bool</code> <p>Enables verbose logging output.</p> required"},{"location":"api/cli/#automil.cli.evaluate--examples","title":"Examples","text":"<p>Evaluate a single model:</p> <pre><code>automil evaluate /data/slides /data/annotations.csv /data/bags /data/models/model_1 -o ./results\n</code></pre> <p>Evaluate multiple models:</p> <pre><code>automil evaluate /data/slides /data/annotations.csv /data/bags /data/models -v\n</code></pre> <p>Override annotation column names:</p> <pre><code>automil evaluate -pc \"patient_id\" -lc \"outcome\" -sc \"slide_id\"             /data/slides /data/annotations.csv /data/bags /data/models/model_1             -o ./results\n</code></pre>"},{"location":"api/cli/#automil.cli.evaluate--expected-model-directory-structure","title":"Expected model directory structure","text":"<p><code>MODEL_DIR</code> may refer either to a single model directory or to a parent directory containing multiple model subdirectories.</p> <p>Single model example:</p> <pre><code>/data/models/model_1/\n|-- best_valid.pth\n|-- ...\n</code></pre> <p>Multiple models example:</p> <pre><code>/data/models/\n|-- model_1/\n|    |-- best_valid.pth\n|-- model_2/\n|    |-- best_valid.pth\n|    |-- ...\n</code></pre> Multiple models <p>When multiple models are evaluated, AutoMIL generates separate evaluation results for each model and compares their performance.</p>"},{"location":"api/cli/#automil.cli.evaluate--annotation-file-requirements","title":"Annotation file requirements","text":"<p>The annotation file must be a CSV file containing at least the following columns:</p> <ul> <li>Patient identifiers (default column name: <code>patient</code>)</li> <li>Slide identifiers (default column name: <code>slide</code>; optional)</li> <li>Class labels (default column name: <code>label</code>)</li> </ul> <p>By default, AutoMIL looks for columns named <code>patient</code>, <code>slide</code>, and <code>label</code>. These defaults can be overridden using the <code>--patient_column</code>, <code>--slide_column</code>, and <code>--label_column</code> options.</p>"},{"location":"api/cli/#automil.cli.evaluate--minimal-annotation-file-example","title":"Minimal annotation file example","text":"<pre><code>patient,slide,label\n001,001_1,0\n001,001_2,0\n002,002,1\n003,003,1\n</code></pre>"},{"location":"api/cli/#automil.cli.evaluate--output-directory-format","title":"Output directory format","text":"<p><code>OUTPUT_DIR</code> must be a directory path. Evaluation results, metrics, and plots are written to this directory.</p> <p>When multiple models are evaluated, output files include a suffix indicating the corresponding model.</p> Source code in <code>automil/cli.py</code> <pre><code>@AutoMIL.command(\n    name=\"evaluate\",\n    context_settings=CONTEXT_SETTINGS,\n    no_args_is_help=True,\n    help=EVALUATE_HELP\n)\n@click.argument(\"slide_dir\",    type=click.Path(exists=True, file_okay=False))\n@click.argument(\"annotation_file\",  type=click.Path(exists=True, file_okay=True))\n@click.argument(\"bags_dir\",     type=click.Path(exists=True, file_okay=False))\n@click.argument(\"model_dir\",    type=click.Path(exists=True, file_okay=False))\n@click.option(\n    \"-o\", \"--output-dir\", \n    type=click.Path(file_okay=True), default=\"evaluation\",\n    help=\"Directory to which to save evaluation results\"\n)\n@click.option(\n    \"-pc\", \"--patient_column\", type=str, default=\"patient\",\n    help=\"Name of the column containing patient IDs\"\n)\n@click.option(\n    \"-lc\", \"--label_column\", type=str, default=\"label\",\n    help=\"Name of the column containing labels\"\n)\n@click.option(\n    \"-sc\", \"--slide_column\", type=str, default=None,\n    help=\"Name of the column containing slide names\"\n)\n@click.option(\"-v\", \"--verbose\", is_flag=True, help=\"Enables additional logging messages\")\ndef evaluate(\n    slide_dir:   str | Path,\n    annotation_file: str | Path,\n    bags_dir:    str | Path,\n    model_dir:   str | Path,\n    output_dir: str | Path,\n    patient_column:  str,\n    label_column:    str,\n    slide_column:    str | None,\n    verbose:     bool\n):\n    \"\"\"\n    Evaluate one or more trained MIL models on a labeled dataset.\n\n    This command generates predictions for the slides in `SLIDE_DIR` using\n    trained models from `MODEL_DIR` and corresponding tile feature bags from\n    `BAGS_DIR`. The predictions are evaluated against the provided annotations,\n    and summary metrics and plots are generated.\n\n    Args:\n        slide_dir (str | Path):\n            Directory containing whole-slide images.\n\n        annotation_file (str | Path):\n            CSV file containing slide- or patient-level annotations and labels.\n\n        bags_dir (str | Path):\n            Directory containing extracted tile feature bags.\n\n        model_dir (str | Path):\n            Directory containing trained model checkpoints.\n\n        output_dir (str | Path):\n            Directory to which evaluation results will be written.\n\n        patient_column (str):\n            Name of the column containing patient identifiers.\n\n        label_column (str):\n            Name of the column containing class labels.\n\n        slide_column (str | None):\n            Name of the column containing slide identifiers.\n\n        verbose (bool):\n            Enables verbose logging output.\n\n    ### Examples\n\n    Evaluate a single model:\n\n        automil evaluate /data/slides /data/annotations.csv /data/bags /data/models/model_1 -o ./results\n\n    Evaluate multiple models:\n\n        automil evaluate /data/slides /data/annotations.csv /data/bags /data/models -v\n\n    Override annotation column names:\n\n        automil evaluate -pc \"patient_id\" -lc \"outcome\" -sc \"slide_id\" \\\n            /data/slides /data/annotations.csv /data/bags /data/models/model_1 \\\n            -o ./results\n\n    ### Expected model directory structure\n\n    `MODEL_DIR` may refer either to a single model directory or to a parent\n    directory containing multiple model subdirectories.\n\n    Single model example:\n\n        /data/models/model_1/\n        |-- best_valid.pth\n        |-- ...\n\n    Multiple models example:\n\n        /data/models/\n        |-- model_1/\n        |    |-- best_valid.pth\n        |-- model_2/\n        |    |-- best_valid.pth\n        |    |-- ...\n\n    ??? Note \"Multiple models\"\n        When multiple models are evaluated, AutoMIL generates separate\n        evaluation results for each model and compares their performance.\n\n    ### Annotation file requirements\n\n    The annotation file must be a CSV file containing at least the following columns:\n\n    - Patient identifiers (default column name: `patient`)\n    - Slide identifiers (default column name: `slide`; optional)\n    - Class labels (default column name: `label`)\n\n    By default, AutoMIL looks for columns named `patient`, `slide`, and `label`.\n    These defaults can be overridden using the `--patient_column`,\n    `--slide_column`, and `--label_column` options.\n\n    ### Minimal annotation file example\n\n        patient,slide,label\n        001,001_1,0\n        001,001_2,0\n        002,002,1\n        003,003,1\n\n    ### Output directory format\n\n    `OUTPUT_DIR` must be a directory path. Evaluation results, metrics, and plots\n    are written to this directory.\n\n    When multiple models are evaluated, output files include a suffix indicating\n    the corresponding model.\n    \"\"\"\n    import slideflow as sf\n\n    from .evaluation import Evaluator\n    from .project import Project\n    from .util import INFO_CLR, LogLevel, get_vlog\n\n    # Getting a verbose logger\n    vlog = get_vlog(verbose)\n    sf.setLoggingLevel(20) # INFO: 20, DEBUG: 10\n\n    # Logging the executed command\n    command = \" \".join(sys.argv)\n    vlog(f\"Executing command: [{INFO_CLR}]{command}[/]\")\n\n    # Some type coercion\n    slide_dir =  Path(slide_dir)\n    bags_dir =   Path(bags_dir)\n    model_dir =  Path(model_dir)\n    output_dir = Path(output_dir)\n\n    vlog(f\"Evaluating models in: [{INFO_CLR}]{model_dir}[/]\")\n\n    # Setup output folder as project (modifies annotation file)\n    project = Project(\n        Path(output_dir),\n        Path(annotation_file),\n        Path(slide_dir),\n        patient_column,\n        label_column,\n        slide_column,\n        transform_labels=False,\n        verbose=verbose,\n    )\n    project.setup_project_scaffold()\n    annotation_file = project.modified_annotations_file\n\n    # Create a minimal dataset (needed for prediction)\n    dataset = sf.Dataset(\n        slides=str(slide_dir),\n        annotations=str(annotation_file)\n    )\n\n    # Evaluate models\n    try:\n        evaluator = Evaluator(\n            dataset,\n            model_dir,\n            output_dir,\n            bags_dir,\n            verbose=verbose\n        )\n        evaluator.evaluate_models(generate_attention_heatmaps=True)\n        evaluator.compare_models()\n        evaluator.generate_plots()\n\n    except Exception as e:\n        tb = traceback.format_exc()\n        vlog(tb, LogLevel.ERROR)\n        vlog(f\"Error: {e}\", LogLevel.ERROR)\n        return\n</code></pre>"},{"location":"api/cli/#automil.cli.create_split","title":"create_split","text":"<pre><code>create_split(\n    slide_dir: str | Path,\n    annotation_file: str | Path,\n    output_file: str | Path,\n    test_fraction: float,\n    read_only: bool,\n    verbose: bool,\n)\n</code></pre> <p>Create a train\u2013test split file from dataset annotations.</p> <p>This command reads the provided annotation file and generates a train\u2013test split, which is saved as a JSON file. The resulting split can be reused for reproducible training and evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>slide_dir</code> <code>str | Path</code> <p>Directory containing whole-slide images.</p> required <code>annotation_file</code> <code>str | Path</code> <p>CSV file containing slide- or patient-level annotations and labels.</p> required <code>output_file</code> <code>str | Path</code> <p>Path to which the split JSON file will be written.</p> required <code>test_fraction</code> <code>float</code> <p>Fraction of samples to assign to the test set.</p> required <code>read_only</code> <code>bool</code> <p>If enabled, an existing split file will not be overwritten.</p> required <code>verbose</code> <code>bool</code> <p>Enables verbose logging output.</p> required"},{"location":"api/cli/#automil.cli.create_split--examples","title":"Examples","text":"<p>Create a split with default settings:</p> <pre><code>automil create-split /data/slides /data/annotations.csv -o split.json\n</code></pre> <p>Create a split without overwriting an existing file:</p> <pre><code>automil create-split /data/slides /data/annotations.csv -o split.json --read-only\n</code></pre>"},{"location":"api/cli/#automil.cli.create_split--output-file-format","title":"Output file format","text":"<p>The output JSON file contains slide identifiers grouped by split name.</p> <p>Example structure:</p> <pre><code>{\n\"train\": [\"slide1\", \"slide2\", ...],\n\"test\":  [\"slide3\", \"slide4\", ...]\n}\n</code></pre> <p>Depending on the configuration, a <code>validation</code> split may be generated instead of or in addition to a <code>test</code> split.</p> Source code in <code>automil/cli.py</code> <pre><code>@AutoMIL.command(\n    \"create-split\",\n    context_settings=CONTEXT_SETTINGS,\n    no_args_is_help=True,\n    help=CREATE_SPLIT_HELP\n)\n@click.argument(\"slide_dir\",        type=click.Path(exists=True, file_okay=False))\n@click.argument(\"annotation_file\",  type=click.Path(exists=True, file_okay=True))\n@click.option(\n    \"-o\", \"--output-file\", type=click.Path(file_okay=True), default=\"split.json\",\n    help=\"Path to which to save the split .json file\"\n)\n@click.option(\"-f\", \"--test-fraction\", type=float, default=0.2, help=\"Fraction of slides to include in the test set\")\n@click.option(\"-r\", \"--read-only\", is_flag=True, help=\"If set, existing split file will not be overwritten\")\n@click.option(\"-v\", \"--verbose\", is_flag=True, help=\"Enables additional logging messages\")\ndef create_split(\n    slide_dir:       str | Path,\n    annotation_file: str | Path,\n    output_file:     str | Path,\n    test_fraction:   float,\n    read_only:       bool,\n    verbose:         bool\n):\n    \"\"\"\n    Create a train\u2013test split file from dataset annotations.\n\n    This command reads the provided annotation file and generates a train\u2013test\n    split, which is saved as a JSON file. The resulting split can be reused\n    for reproducible training and evaluation.\n\n    Args:\n        slide_dir (str | Path):\n            Directory containing whole-slide images.\n\n        annotation_file (str | Path):\n            CSV file containing slide- or patient-level annotations and labels.\n\n        output_file (str | Path):\n            Path to which the split JSON file will be written.\n\n        test_fraction (float):\n            Fraction of samples to assign to the test set.\n\n        read_only (bool):\n            If enabled, an existing split file will not be overwritten.\n\n        verbose (bool):\n            Enables verbose logging output.\n\n    ### Examples\n\n    Create a split with default settings:\n\n        automil create-split /data/slides /data/annotations.csv -o split.json\n\n    Create a split without overwriting an existing file:\n\n        automil create-split /data/slides /data/annotations.csv -o split.json --read-only\n\n    ### Output file format\n\n    The output JSON file contains slide identifiers grouped by split name.\n\n    Example structure:\n\n        {\n        \"train\": [\"slide1\", \"slide2\", ...],\n        \"test\":  [\"slide3\", \"slide4\", ...]\n        }\n\n    Depending on the configuration, a `validation` split may be generated\n    instead of or in addition to a `test` split.\n    \"\"\"\n\n    import slideflow as sf\n\n    from .util import INFO_CLR, LogLevel, get_vlog\n\n    # Getting a verbose logger\n    vlog = get_vlog(verbose)\n    sf.setLoggingLevel(20) # INFO: 20, DEBUG: 10\n\n    # Logging the executed command\n    command = \" \".join(sys.argv)\n    vlog(f\"Executing command: [{INFO_CLR}]{command}[/]\")\n\n    # Some type coercion\n    slide_dir = Path(slide_dir)\n    annotation_file = Path(annotation_file)\n    output_file = Path(output_file)\n\n    try:\n        # Minimal dataset for splitting\n        dataset = sf.Dataset(\n            slides=str(slide_dir),\n            annotations=str(annotation_file)\n        )\n        # Create the split and save it\n        _, _ = dataset.split(\n            labels=\"label\",\n            val_fraction=test_fraction,\n            splits=str(output_file),\n            read_only=read_only\n        )\n\n    except Exception as e:\n        tb = traceback.format_exc()\n        vlog(tb, LogLevel.ERROR)\n        vlog(f\"Error: {e}\", LogLevel.ERROR)\n        return\n</code></pre>"},{"location":"api/dataset/","title":"Dataset","text":"<p><code>automil.dataset.Dataset</code> is responsible for preparing a slideflow-compatible dataset source that can be passed to downstream pipeline stages. It supports both raw and pre-tiled whole-slide image datasets, and handles tiling, the conversion from .png to .tiff, label filtering, and feature extraction.</p> <p>The resulting datasets are stored as TFRecords and feature bags within respective folders in the project directory.</p>"},{"location":"api/dataset/#automil.dataset.Dataset","title":"Dataset","text":"<p>Prepares and manages dataset sources for downstream pipeline stages.</p> <p>This class handles and executes all dataset-related preprocessing steps, including:</p> <ul> <li>Computing microns-per-pixel (MPP) or selecting a sensible default</li> <li>Filtering slides by label</li> <li>Extracting tiles or</li> <li>Handling pretiled datasets</li> <li>Generating feature bags for model training</li> </ul> <p>The dataset lifecycle is tightly coupled to a slideflow <code>Project</code> instance and produces TFRecords and feature bags within the project directory.</p> Source code in <code>automil/dataset.py</code> <pre><code>class Dataset():\n    \"\"\"Prepares and manages dataset sources for downstream pipeline stages.\n\n    This class handles and executes all dataset-related preprocessing steps, including:\n\n    - Computing microns-per-pixel (MPP) or selecting a sensible default\n    - Filtering slides by label\n    - Extracting tiles or\n    - Handling pretiled datasets\n    - Generating feature bags for model training\n\n    The dataset lifecycle is tightly coupled to a slideflow ``Project`` instance\n    and produces TFRecords and feature bags within the project directory.\n    \"\"\"\n    def __init__(\n        self,\n        project: sf.Project,\n        resolution: RESOLUTION_PRESETS,\n        label_map: dict | list[str],\n        slide_dir: Path | None = None,\n        bags_dir:  Path | None = None,\n        is_pretiled:  bool = False,\n        tiff_conversion: bool = False,\n        verbose: bool = True\n        ) -&gt; None:\n        \"\"\"Initialize a Dataset manager.\n\n        Args:\n            project (sf.Project): Slideflow project associated with this dataset.\n            resolution (RESOLUTION_PRESETS): Resolution preset defining tile size and magnification.\n            label_map (dict | list[str]): Label mapping used to filter slides.\n            slide_dir (Path | None, optional): Directory containing raw or pretiled slides.\n            bags_dir (Path | None, optional): Output directory for generated feature bags.\n            is_pretiled (bool, optional): Whether the input slides are already tiled.\n            tiff_conversion (bool, optional): Whether slides should be converted to TIFF before tiling.\n            verbose (bool, optional): Enable verbose logging.\n        \"\"\"\n        self.project = project\n        self.resolution = resolution\n\n        self.slide_dir = slide_dir\n        self.bags_dir  = bags_dir\n\n        self.label_map = label_map\n        self.is_pretiled = is_pretiled\n        self.tiff_conversion = tiff_conversion\n\n        self.vlog = get_vlog(verbose)\n\n    @cached_property\n    def tile_px(self) -&gt; int:\n        \"\"\"Tile size in pixels derived from the resolution preset.\n\n        Returns:\n            int: Tile size in pixels.\n        \"\"\"\n        return self.resolution.tile_px\n\n    @cached_property\n    def magnification(self) -&gt; str:\n        \"\"\"Nominal magnification derived from the resolution preset.\n\n        Returns:\n            str: Magnification string (e.g., ``\"10x\"``).\n        \"\"\"\n\n        return self.resolution.magnification\n\n    @cached_property\n    def mpp(self) -&gt; float:\n        \"\"\"Computed microns-per-pixel (MPP) value.\n\n        The MPP is computed by retrieving MPP values from the slides metadata ifavailable, otherwise\n        sensible defaults are used based on magnification.\n\n        Returns:\n            float: Microns per pixel.\n        \"\"\"\n        return self._compute_mpp(by_average=True)\n\n    @cached_property\n    def tile_um(self) -&gt; int:\n        \"\"\"Tile size in micrometers.\n\n        Returns:\n            int: Tile size in micrometers.\n        \"\"\"\n\n        return int(self.tile_px * self.mpp)\n\n    @cached_property\n    def tfrecords_dir(self) -&gt; Path:\n        \"\"\"Path to directory where tfrecords will be stored\"\"\"\n        if self.is_pretiled:\n            return Path(self.project.root) / \"tfrecords\" / \"pretiled\"\n        elif self.tiff_conversion:\n            return Path(self.project.root) / \"tfrecords\" / \"tiff_buffer\"\n        else:\n            return Path(self.project.root) / \"tfrecords\"\n\n\n    def prepare_dataset_source(self) -&gt; sf.Dataset:\n        \"\"\"Prepares a single dataset source for a given resolution preset, or for a set of pretiled slides.\n\n        Performs the following steps:\n            1. Compute appropriate MPP for slides\n            2. filter slides by label map\n            3. Extract tiles (or convert pretiled to tfrecords)\n            4. Extract features\n\n        Raises:\n            ValueError: If `pretiled` is True but no `slide_dir` is provided\n\n        Returns:\n            sf.Dataset: A slideflow dataset\n        \"\"\"\n        if self.is_pretiled:\n            self.vlog(f\"Preparing dataset source from pretiled slides at [{INFO_CLR}]{self.slide_dir}[/]\")\n        else:\n            self.vlog(f\"Preparing dataset source at resolution [{INFO_CLR}]{self.resolution.name} \"\n                f\"({self.tile_px}px, {self.tile_um:.2f}um)[/]\")\n\n        # Convert pretiled to tfrecords\n        if self.is_pretiled:\n            if self.slide_dir is None:\n                raise ValueError(\"slide_dir must be provided when pretiled=True\")\n            dataset = self._convert_pretiled()\n            dataset = self._apply_label_filter(dataset)\n        else:\n            dataset = self.project.dataset(\n                sources=\"AutoMIL\",\n                tile_px=self.tile_px,\n                tile_um=self.tile_um,\n            )\n            dataset = self._apply_label_filter(dataset)\n            self._extract_tiles(dataset)\n\n        self._extract_features(dataset)\n        return dataset\n\n    def summary(self) -&gt; None:\n        rows = [\n            (\"Resolution Preset\", self.resolution.name),\n            (\"Tile Size (px)\", f\"{self.tile_px}px\"),\n            (\"Magnification\", self.magnification),\n            (\"Microns-Per-Pixel\", f\"{self.mpp:.3f}\"),\n            (\"Tile Size (\u00b5m)\", f\"{self.tile_um:.2f}\u00b5m\"),\n            (\"Pretiled Input\", self.is_pretiled),\n            (\"TIFF Conversion\", self.tiff_conversion),\n        ]\n\n        self.vlog(\"[bold underline]Dataset Summary[/]\")\n        self.vlog(render_kv_table(rows))\n\n    # === Internals === #\n    def _compute_mpp(self, by_average: bool = True) -&gt; float:\n        \"\"\"Computes an appropriate Microns Per Pixel (MPP) for the given slide images.\n        If `by_average` is True, the average MPP across all slides is computed.\n        Otherwise, the first slide's MPP is used.\n        If `slide_dir` is None, MPP is computed based on sensible defaults based on the slide magnification.\n\n        Args:\n            by_average (bool, optional): Compute MPP by calculating the average across slides. Defaults to False.\n\n        Returns:\n            float: Appropriate MPP value for the given slides \n        \"\"\"\n        global COMMON_MPP_VALUES\n        mpp = None\n\n        # Try to compute MPP from slides\n        if self.slide_dir is not None and self.slide_dir.exists():\n            # Average MPP across slides\n            if by_average:\n                mpp = calculate_average_mpp(self.slide_dir)\n                if mpp is not None:\n                    self.vlog(f\"Computed average MPP across slides: [{INFO_CLR}]{mpp:.3f}[/]\")\n            # MPP from first slide\n            else:\n                first_slide = next(self.slide_dir.glob(\"*\"))\n                mpp = get_mpp_from_slide(first_slide)\n\n        # Fallback: Default from common mpp values\n        if mpp is None:\n            mpp = COMMON_MPP_VALUES.get(self.magnification, 0.5)\n            self.vlog(f\"Using default MPP for magnification [{INFO_CLR}]{self.magnification}: {mpp:.3f}[/]\")\n\n        return mpp\n\n    def _apply_label_filter(self, dataset: sf.Dataset) -&gt; sf.Dataset:\n        \"\"\"Apply `label_map` filter to the given dataset\n\n        Args:\n            dataset (sf.Dataset): dataset\n\n        Returns:\n            sf.Dataset: filtered dataset\n        \"\"\"\n        # Extract list of unique labels\n        match self.label_map:\n            case dict():\n                unique_labels = list(self.label_map.values())\n            case list():\n                unique_labels = self.label_map\n            case _:\n                unique_labels = []\n\n        if not unique_labels:\n            return dataset\n\n        # Retrieve annotation dtypes and cast if necessary\n        annotations = dataset.annotations if dataset.annotations is not None else pd.DataFrame()\n        if not annotations.empty:\n            ann_type = type(annotations[\"label\"].iat[0])\n            unique_type = type(unique_labels[0])\n\n            if ann_type != unique_type:\n                unique_labels = [ann_type(lbl) for lbl in unique_labels]\n\n        self.vlog(f\"Filtering for unique labels {unique_labels}\")\n\n        return self.project.dataset(\n            dataset.tile_px,\n            dataset.tile_um,\n            filters={\"label\": unique_labels},\n        )\n\n    def _convert_pretiled(self) -&gt; sf.Dataset:\n        \"\"\"Converts a pretiled dataset source to tfrecords. Tiling is skipped.\n\n        Raises:\n            RuntimeError: If no project annotations file is found\n            RuntimeError: If the dataset manifest is empty after conversion\n\n        Returns:\n            sf.Dataset: A dataset source with tfrecords\n        \"\"\"\n        if not self.project.annotations:\n            raise RuntimeError(\"A project annotations file is required for pretiled datasets.\")\n\n        elif self.slide_dir is None:\n            raise ValueError(\"slide_dir must be provided when pretiled=True\")\n\n        # Prepare TFRecords directory\n        tfrecords_dir = self.tfrecords_dir\n        tfrecords_dir.mkdir(parents=True, exist_ok=True)\n\n        # Add source if not already present\n        if \"pretiled\" not in self.project.sources:\n            self.project.add_source(\n                \"pretiled\",\n                tfrecords=str(tfrecords_dir),\n                slides=str(self.slide_dir)\n            )\n        # Change source so slideflow knows where to look for tfrecords\n        dataset = self.project.dataset(\n            sources=[\"pretiled\"],\n            tile_px=self.tile_px,\n            tile_um=self.tile_um,\n        )\n\n         # Convert pretiled slides to tfrecords\n        self.vlog(f\"Converting pretiled slides to tfrecords at [{INFO_CLR}]{tfrecords_dir}[/] ...\")\n\n        pretiled_to_tfrecords(self.slide_dir, Path(dataset.tfrecords_folders()[0]))\n\n        dataset.rebuild_index()\n        dataset.update_manifest(force_update=True)\n\n        if len(dataset.manifest()) == 0:\n            raise RuntimeError(\"Pretiled dataset conversion produced an empty manifest.\")\n\n        self.vlog(f\"Pretiled dataset loaded with [{INFO_CLR}]{len(dataset.manifest())}[/] slides.\")\n\n        return dataset\n\n    def _extract_tiles(self, dataset: sf.Dataset) -&gt; None:\n        \"\"\"Extracts tiles from a given dataset source. Optionally performs prior tiff conversion.\n\n        Note:\n            The tiff conversion process is performed in batches to avoid excessive disk space usage.\n            It is recommended to use tiff conversion ONLY when working with slides in formats that are not well-suited for tiling (e.g., .png).\n\n        Args:\n            dataset (sf.Dataset): Dataset source for which to extract tiles\n\n        Raises:\n            RuntimeError: If the batchwise tiff conversion process fails or encounters a timeout.\n        \"\"\"\n        # Default Case: Normal tile extraction\n        if not self.tiff_conversion:\n            self.vlog(f\"Extracting tiles at [{INFO_CLR}]{self.magnification} | tile={self.tile_px}[/]\")\n            dataset.extract_tiles(\n                qc=qc.Otsu(),\n                normalizer=\"reinhard_mask\",\n                report=True,\n            )\n            return\n\n        # Optional: batchwise .tiff conversion\n        else:\n            self.vlog(f\"Preparing TIFF conversion pipeline [{INFO_CLR}]({self.tile_px}px @ {self.magnification})[/]\")\n\n            # Permanent tiff buffer directory\n            tiff_dir = Path(self.project.root) / \"tiffs\"\n            tiff_dir.mkdir(parents=True, exist_ok=True)\n\n            # Need to register a dataset source for the tiff buffer\n            if \"tiff_buffer\" not in self.project.sources:\n                self.project.add_source(\n                    \"tiff_buffer\",\n                    slides=str(tiff_dir),\n                )\n            dataset = self.project.dataset(\n                sources=[\"tiff_buffer\"],\n                tile_px=dataset.tile_px,\n                tile_um=dataset.tile_um,\n            )\n\n            # Prepare TFRecords directory\n            tfrecords_dir = Path(dataset.tfrecords_folders()[0])\n            tfrecords_dir.mkdir(parents=True, exist_ok=True)\n\n            # Retieve slide paths and IDs\n            slide_list: list[Path] = [path for p in dataset.slide_paths() if (path := Path(p)).exists()]\n            slide_ids:  list[str]  = list(set(slide.stem for slide in slide_list)) # Using a set to avoid duplicates\n\n            # Caution: Make sure the tfrecords dont actually exist yet (e.g., from previous runs)\n            expected_tfrecords = {sid: tfrecords_dir / f\"{sid}.tfrecords\" for sid in slide_ids}\n            existing = {sid: path for sid, path in expected_tfrecords.items() if path.exists()}\n            missing = [sid for sid in slide_ids if sid not in existing.keys()]\n\n            if not missing:\n                self.vlog(\n                    f\"All expected tfrecords already exist in {tfrecords_dir}. Skipping TIFF conversion.\",\n                    LogLevel.WARNING\n                )\n                return\n\n            self.vlog(\n                f\"Found {len(existing)} existing TFRecords \u2014 creating {len(missing)} missing ones.\"\n            )\n\n            # Only convert still missing slides\n            missing_slides = [slide for slide in slide_list if slide.stem in missing]\n\n            # Size of the tiff buffer batches\n            # TODO | Should probably be configurable\n            buffer_size = 10\n\n            # Process missing/outdated slides in batches\n            for batch_idx, slide_batch in enumerate(batch_generator(missing_slides, buffer_size)):\n                self.vlog(f\"Converting TIFF batch [{INFO_CLR}]{batch_idx+1}[/] / [{INFO_CLR}]{len(missing_slides)//buffer_size+1}[/]\")\n                # Batchwise conversion to tiff\n                batch_conversion_concurrent(slide_batch, tiff_dir)\n\n                # Extract tiles\n                try:\n                    dataset.extract_tiles(\n                        qc=qc.Otsu(),\n                        normalizer=\"reinhard_mask\",\n                        mpp_override=self.mpp\n                    )\n                except Exception as e:\n                    raise RuntimeError(f\"Error extracting tiles for TIFF batch {batch_idx}: {e}\")\n\n            self.vlog(f\"[{SUCCESS_CLR}]Finished TIFF conversion[/]\")\n\n    def _extract_features(self, dataset: sf.Dataset) -&gt; None:\n        \"\"\"Extracts features from a given (tiled) dataset source and stores them in `bags_dir`\n\n        Args:\n            dataset (sf.Dataset): Dataset source for which to generate features\n        \"\"\"\n        global FEATURE_EXTRACTOR\n\n        # Prepare bags directory\n        bag_dir = Path(self.project.root) / \"bags\" if self.bags_dir is None else self.bags_dir\n        bag_dir.mkdir(exist_ok=True)\n\n        # Build feature extractor model\n        extractor = sf.build_feature_extractor(\n            name=FEATURE_EXTRACTOR,\n            resize=224,\n        )\n        num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n        self.vlog(f\"Extracting features using [{INFO_CLR}]{num_gpus}[/] GPUs \u2026\")\n\n        # Generate feature bags\n        dataset.generate_feature_bags(\n            model=extractor,\n            outdir=str(bag_dir),\n            slide_batch_size=32,\n            num_gpus=num_gpus,\n        )\n\n        self.vlog(f\"[{SUCCESS_CLR}]Finished feature extraction.[/]\")\n</code></pre>"},{"location":"api/dataset/#automil.dataset.Dataset.magnification","title":"magnification  <code>cached</code> <code>property</code>","text":"<pre><code>magnification: str\n</code></pre> <p>Nominal magnification derived from the resolution preset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Magnification string (e.g., <code>\"10x\"</code>).</p>"},{"location":"api/dataset/#automil.dataset.Dataset.mpp","title":"mpp  <code>cached</code> <code>property</code>","text":"<pre><code>mpp: float\n</code></pre> <p>Computed microns-per-pixel (MPP) value.</p> <p>The MPP is computed by retrieving MPP values from the slides metadata ifavailable, otherwise sensible defaults are used based on magnification.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Microns per pixel.</p>"},{"location":"api/dataset/#automil.dataset.Dataset.tfrecords_dir","title":"tfrecords_dir  <code>cached</code> <code>property</code>","text":"<pre><code>tfrecords_dir: Path\n</code></pre> <p>Path to directory where tfrecords will be stored</p>"},{"location":"api/dataset/#automil.dataset.Dataset.tile_px","title":"tile_px  <code>cached</code> <code>property</code>","text":"<pre><code>tile_px: int\n</code></pre> <p>Tile size in pixels derived from the resolution preset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Tile size in pixels.</p>"},{"location":"api/dataset/#automil.dataset.Dataset.tile_um","title":"tile_um  <code>cached</code> <code>property</code>","text":"<pre><code>tile_um: int\n</code></pre> <p>Tile size in micrometers.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Tile size in micrometers.</p>"},{"location":"api/dataset/#automil.dataset.Dataset.prepare_dataset_source","title":"prepare_dataset_source","text":"<pre><code>prepare_dataset_source() -&gt; sf.Dataset\n</code></pre> <p>Prepares a single dataset source for a given resolution preset, or for a set of pretiled slides.</p> Performs the following steps <ol> <li>Compute appropriate MPP for slides</li> <li>filter slides by label map</li> <li>Extract tiles (or convert pretiled to tfrecords)</li> <li>Extract features</li> </ol> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>pretiled</code> is True but no <code>slide_dir</code> is provided</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>sf.Dataset: A slideflow dataset</p> Source code in <code>automil/dataset.py</code> <pre><code>def prepare_dataset_source(self) -&gt; sf.Dataset:\n    \"\"\"Prepares a single dataset source for a given resolution preset, or for a set of pretiled slides.\n\n    Performs the following steps:\n        1. Compute appropriate MPP for slides\n        2. filter slides by label map\n        3. Extract tiles (or convert pretiled to tfrecords)\n        4. Extract features\n\n    Raises:\n        ValueError: If `pretiled` is True but no `slide_dir` is provided\n\n    Returns:\n        sf.Dataset: A slideflow dataset\n    \"\"\"\n    if self.is_pretiled:\n        self.vlog(f\"Preparing dataset source from pretiled slides at [{INFO_CLR}]{self.slide_dir}[/]\")\n    else:\n        self.vlog(f\"Preparing dataset source at resolution [{INFO_CLR}]{self.resolution.name} \"\n            f\"({self.tile_px}px, {self.tile_um:.2f}um)[/]\")\n\n    # Convert pretiled to tfrecords\n    if self.is_pretiled:\n        if self.slide_dir is None:\n            raise ValueError(\"slide_dir must be provided when pretiled=True\")\n        dataset = self._convert_pretiled()\n        dataset = self._apply_label_filter(dataset)\n    else:\n        dataset = self.project.dataset(\n            sources=\"AutoMIL\",\n            tile_px=self.tile_px,\n            tile_um=self.tile_um,\n        )\n        dataset = self._apply_label_filter(dataset)\n        self._extract_tiles(dataset)\n\n    self._extract_features(dataset)\n    return dataset\n</code></pre>"},{"location":"api/evaluation/","title":"Evaluator","text":"<p><code>automil.evaluation.Evaluator</code> is responsible for evaluating trained MIL models, computing classification metrics, optionally generating ensemble predictions and producing comparison plots.</p>"},{"location":"api/evaluation/#automil.evaluation.Evaluator","title":"Evaluator","text":"<p>Evaluates trained MIL models.</p> The Evaluator supports <ul> <li>Single-model evaluation</li> <li>Batch evaluation of multiple trained models</li> <li>Ensemble prediction generation</li> <li>Metrics calculation (AUC, AP, Accuracy, F1)</li> <li>Comparative plotting across models</li> </ul> Source code in <code>automil/evaluation.py</code> <pre><code>class Evaluator:\n    \"\"\"\n    Evaluates trained MIL models.\n\n    The Evaluator supports:\n        - Single-model evaluation\n        - Batch evaluation of multiple trained models\n        - Ensemble prediction generation\n        - Metrics calculation (AUC, AP, Accuracy, F1)\n        - Comparative plotting across models\n    \"\"\"\n    def __init__(self,\n        dataset: sf.Dataset,\n        model_dir: Path,\n        out_dir: Path,\n        bags_dir: Path,\n        verbose: bool = True\n    ) -&gt; None:\n        \"\"\"Initializes a Evaluator Instance\n\n        Args:\n            dataset (sf.Dataset): Slideflow dataset\n            model_dir (Path): Directory in which to store trained models\n            out_dir (Path): Diectory in which to store results such as predictions\n            bags_dir (Path): Directory with feature bags\n            verbose (bool, optional): Whether to print verbose messages. Defaults to True.\n        \"\"\"\n        self.dataset = dataset\n        self.vlog = get_vlog(verbose)\n\n        # Path Setup\n        self.model_dir = model_dir\n        self.out_dir = out_dir\n        self.bags_dir = bags_dir\n\n\n    def load_predictions(self, model_path: Path) -&gt; pd.DataFrame:\n        \"\"\"\n        Loads and validates prediction outputs from a trained model directory.\n\n        The predictions file must contain:\n        - One or more probability columns starting with ``y_pred``\n        - Base columns ``slide`` and ``y_true``\n\n        Args:\n            model_path (Path): Path to a trained model directory.\n\n        Raises:\n            FileNotFoundError: If ``predictions.parquet`` is missing.\n            ValueError: If required prediction or base columns are absent.\n\n        Returns:\n            pd.DataFrame: Loaded and validated predictions.\n        \"\"\"\n        if not (predictions_path := model_path / \"predictions.parquet\").exists():\n            raise FileNotFoundError(f\"{model_path} does not contain a 'predictions.parquet' file\")\n\n        predictions = pd.read_parquet(predictions_path)\n\n        all_columns = [column for column in predictions.columns]\n        # We expect columns containing prediction probabilites to start with 'y_pred' (e.g 'y_pred0', 'y_pred1', ...)\n        pred_columns = [column for column in all_columns if column.startswith(\"y_pred\")]\n        # Similarly, we expect predictions to contain 'slide' and 'y_true' columns\n        base_columns = [\"slide\", \"y_true\"]\n\n        if not pred_columns:\n            raise ValueError(\"'predictions.parquet' does not contain the expected prediction columns\")\n        elif not all(base_column in all_columns for base_column in base_columns):\n            raise ValueError(\"'predictions.parquet' does not contain the expected base columns\")\n\n        return predictions\n\n    def calculate_metrics(\n        self,\n        predictions: pd.DataFrame | Path | str\n    ) -&gt; dict[str, float | np.ndarray]:\n        \"\"\"\n        Computes classification metrics from prediction outputs.\n\n        Supports both binary and multi-class classification and automatically\n        detects ensemble predictions when present.\n\n        Args:\n            predictions (pd.DataFrame | Path | str): Predictions DataFrame or path\n                to a ``predictions.parquet`` file.\n\n        Returns:\n            dict[str, float | np.ndarray]: Dictionary containing:\n                - Accuracy\n                - AUC\n                - Average Precision\n                - F1 score\n                - Confusion matrix\n                - Per-class accuracy\n        \"\"\"\n\n        # Make sure we're working with a loaded DataFrame\n        match predictions:\n            case Path() | str():\n                predictions = self.load_predictions(Path(predictions))\n            case pd.DataFrame():\n                pass\n\n        # Extract true labels and calculate number of classes\n        y_true = predictions[\"y_true\"].astype(int)\n        num_classes = len(y_true.unique())\n\n        # We expect columns containing prediction probabilites to start with 'y_pred' (e.g 'y_pred0', 'y_pred1', ...)\n        # Similarly, we may have ensemble predictions ending with '_ensemble' (e.g., 'y_pred0_ensemble', 'y_pred1_ensemble', ...)\n        pred_columns = [column for column in predictions.columns if column.startswith(\"y_pred\")]\n        # Case 1: Ensemble predictions (priority)\n        ensemble_columns = [col for col in pred_columns if col.endswith(\"_ensemble\")]\n        if ensemble_columns:\n            # Use ensemble predictions\n            prob_columns = [f\"y_pred{i}_ensemble\" for i in range(num_classes)]\n            prediction_type = \"ensemble\"\n        else:\n            # Case 2: Single model predictions\n            # Get regular y_pred columns (y_pred0, y_pred1, etc.)\n            prob_columns = [f\"y_pred{i}\" for i in range(num_classes)]\n            prediction_type = \"single model\"\n\n        # Verify all expected probability columns exist\n        missing_columns = [col for col in prob_columns if col not in predictions.columns]\n        if missing_columns:\n            raise ValueError(f\"Missing probability columns for {prediction_type} predictions: {missing_columns}\")\n\n        # Get probability matrix\n        prob_matrix = predictions[prob_columns].values\n\n        # Get predicted classes\n        if \"y_pred_label\" in predictions.columns:\n            y_pred = predictions[\"y_pred_label\"].astype(int)\n        else:\n            y_pred = np.argmax(prob_matrix, axis=1)\n\n        # Calculate metrics\n        accuracy = float(accuracy_score(y_true, y_pred))\n        cm = confusion_matrix(y_true, y_pred)\n\n        # Binary classification\n        if num_classes == 2:\n            y_probs = prob_matrix[:, 1] # We really only need the prediction probabilities for label 1\n\n            auc = float(roc_auc_score(y_true, y_probs))\n            ap  = float(average_precision_score(y_true, y_probs))\n            f1  = float(f1_score(y_true, y_pred))\n\n        # Multiclass\n        else:\n            auc = float(roc_auc_score(y_true, prob_matrix, multi_class=\"ovr\", average=\"macro\"))\n\n            ap_scores = []\n            for class_idx in range(num_classes):\n                # 0 if label is class_idx, 1 otherwise\n                y_true_binary = (y_true == class_idx).astype(int)\n                # Prediction probabilities for this class\n                y_probs_class = prob_matrix[:, class_idx]\n\n                if len(y_true_binary.unique()) &gt; 1:\n                    ap_class = average_precision_score(y_true_binary, y_probs_class)\n                    ap_scores.append(ap_class)\n\n            ap = float(np.mean(ap_scores)) if ap_scores else 0.0\n            f1 = float(f1_score(y_true, y_pred, average=\"macro\"))\n\n        per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n\n        return {\n            \"Accuracy\": accuracy,\n            \"AUC\": auc,\n            \"AP\": ap,\n            \"F1\": f1,\n            \"ConfusionMatrix\": cm,\n            \"PerClassAccuracy\": per_class_accuracy\n        }\n\n    def evaluate_models(\n        self,\n        model_dir: Path | None = None,\n        bags_dir: Path | None = None,\n        out_dir: Path | None = None,\n        generate_attention_heatmaps: bool = False\n    ) -&gt; None:\n        \"\"\"\n        Evaluates one or more trained models.\n\n        Detects each trained model directory inside `model_dir` and evaluates them independently.\n        Predictions and metrics are written to the output directory.\n\n        Args:\n            model_dir (Path | None, optional): Model directory or parent directory of models to evaluate.\n            bags_dir (Path | None, optional): Feature bag directory.\n            out_dir (Path | None, optional): Output directory.\n            generate_attention_heatmaps (bool, optional): Generate attention heatmaps.\n        \"\"\"\n        # Default to instance variables if none provided\n        model_dir = model_dir or self.model_dir\n        bags_dir = bags_dir or self.bags_dir\n        out_dir = out_dir or self.out_dir\n\n        # Check if model_dir is a single model directory\n        if is_model_directory(model_dir):\n            model_paths = [model_dir]\n            self.vlog(f\"Single model directory detected: {model_dir}\")\n        # Else, collect all model subdirectories\n        else:\n            if not (model_paths := [subdir for subdir in model_dir.iterdir() if subdir.is_dir() and is_model_directory(subdir)]):\n                self.vlog(f\"No model directories found in {model_dir}\", LogLevel.WARNING)\n                return\n\n        # Iterate over each model directory and evaluate\n        for model_idx, model_path in enumerate(model_paths):\n            self.vlog(f\"Evaluating model [{INFO_CLR}]{model_idx+1}[/]/[{INFO_CLR}]{len(model_paths)}[/]: [{INFO_CLR}]{model_path}[/]\")\n            try:\n                eval_mil(\n                    weights=str(model_path),\n                    bags=str(bags_dir),\n                    dataset=self.dataset,\n                    outcomes=\"label\",\n                    outdir=str(out_dir),\n                    attention_heatmaps=generate_attention_heatmaps\n                )\n                self.vlog(\"Evaluation complete.\\n\")\n            except Exception as e:\n                self.vlog(f\"Error evaluating model at {model_path}: {e}\", LogLevel.ERROR)\n                continue\n\n    def generate_predictions(\n        self,\n        model_dir: Path | None = None,\n        bags_dir: Path | None = None,\n        out_dir: Path | None = None\n    ) -&gt; None:\n        \"\"\"\n        Generates prediction outputs for one or more trained models.\n\n        Predictions are saved per model in ``predictions.parquet`` format.\n\n        Args:\n            model_dir (Path | None, optional): Directory containing model subdirectories.\n            bags_dir (Path | None, optional): Feature bag directory.\n            out_dir (Path | None, optional): Output directory.\n        \"\"\"\n        # Default to instance variables if none provided\n        model_dir = model_dir or self.model_dir\n        bags_dir = bags_dir or self.bags_dir\n        out_dir = out_dir or self.out_dir\n\n        # Check if model_dir is a single model directory\n        if is_model_directory(model_dir):\n            model_paths = [model_dir]\n            self.vlog(f\"Single model directory detected: [{INFO_CLR}]{model_dir}[/]\")\n        # Else, collect all model subdirectories\n        else:\n            if not (model_paths := [subdir for subdir in model_dir.iterdir() if subdir.is_dir() and is_model_directory(subdir)]):\n                self.vlog(f\"No model directories found in [{INFO_CLR}]{model_dir}[/]\", LogLevel.WARNING)\n                return\n\n        # Iterate over each model directory and generate predictions\n        for model_idx, model_path in enumerate(model_paths):\n            self.vlog(f\"Generating predictions with model [{INFO_CLR}]{model_idx+1}[/]/[{INFO_CLR}]{len(model_paths)}[/]: [{INFO_CLR}]{model_path}[/]\")\n            try:\n                predictions = predict_mil(\n                    model=str(model_path),\n                    bags=str(bags_dir),\n                    dataset=self.dataset,\n                    outcomes=\"label\",\n                )\n                # Cast to DataFrame\n                # Can do this safely since predict_mil always returns a DataFrame if attention==False\n                predictions = pd.DataFrame(predictions)\n\n                # Save predictions to out_dir/model_name/predictions.parquet\n                model_out_dir = out_dir / model_path.name\n                model_out_dir.mkdir(parents=True, exist_ok=True)\n                predictions_path = model_out_dir / \"predictions.parquet\"\n                predictions.to_parquet(predictions_path, index=False)\n                self.vlog(f\"Predictions saved to [{INFO_CLR}]{predictions_path}[/]\")\n\n            except Exception as e:\n                self.vlog(f\"Error evaluating model at {model_path}: {e}\", LogLevel.ERROR)\n                continue\n\n    def create_ensemble_predictions(\n        self,\n        model_dir: Path | None = None,\n        output_path: Path | None = None,\n        print_summary: bool = True\n    ) -&gt; tuple[pd.DataFrame, dict[str, float | np.ndarray]]:\n        \"\"\"\n        Generates ensemble predictions by averaging outputs across multiple models.\n\n        Ensemble probabilities are computed per class and used to derive final\n        predictions and evaluation metrics.\n\n        Args:\n            model_dir (Path | None, optional): Directory containing trained models.\n            output_path (Path | None, optional): Output file path (.csv or .parquet).\n            print_summary (bool, optional): Print a formatted metric summary.\n\n        Raises:\n            ValueError: If no valid prediction files are found.\n\n        Returns:\n            tuple:\n                - Ensemble predictions DataFrame\n                - Dictionary of evaluation metrics\n        \"\"\"\n\n        model_dir = model_dir or self.model_dir\n        output_path = output_path or (self.out_dir / \"ensemble_predictions.parquet\")\n\n        # Check if model_dir is a single model directory\n        if is_model_directory(model_dir):\n            model_paths = [model_dir]\n            self.vlog(f\"Single model directory detected: [{INFO_CLR}]{model_dir}[/]\")\n        # Else, collect all model subdirectories\n        else:\n            if not (model_paths := [subdir for subdir in model_dir.iterdir() if subdir.is_dir() and is_model_directory(subdir)]):\n                self.vlog(f\"No model directories found in [{INFO_CLR}]{model_dir}[/]\", LogLevel.WARNING)\n                raise ValueError(\"No model directories found for ensembling\")\n\n        # Try to load predictions from each model that has been evaluated (should all be in model_dir)\n        predictions_list: list[pd.DataFrame] = []\n        for model_idx, submodel_dir in enumerate(model_paths):\n            try:\n                predictions = self.load_predictions(submodel_dir)\n\n                # Add the model index to predictions columns so we can merge later\n                pred_columns = [column for column in predictions.columns if column.startswith(\"y_pred\")]\n                rename_map = {pred_column: f\"{pred_column}_model{model_idx}\" for pred_column in pred_columns}\n                predictions = predictions.rename(columns=rename_map)\n                predictions_list.append(predictions)\n\n                self.vlog(f\"Loaded predictions from model [{INFO_CLR}]{submodel_dir.name}[/] ([{INFO_CLR}]{model_idx+1}[/]/[{INFO_CLR}]{len(os.listdir(model_dir))}[/])\")\n            except Exception as e:\n                self.vlog(f\"Error loading predictions from {submodel_dir}: {e}\", LogLevel.WARNING)\n                continue\n\n        if not predictions_list:\n            raise ValueError(\"Failed to load any predictions from model directory\")\n\n        # Merge predictions on the base columns\n        merged = predictions_list[0].copy()\n\n        for predictions in predictions_list[1:]:\n            merged = merged.merge(\n                predictions,\n                on=[\"slide\", \"y_true\"],\n                how=\"inner\"\n            )\n\n        # Get all prediction columns\n        all_pred_columns = [\n            column for column in merged.columns\n            if column.startswith(\"y_pred\")\n        ]\n\n        if not all_pred_columns:\n            raise ValueError(\"No prediction columns found for ensembling\")\n\n        unique_classes = sorted(merged[\"y_true\"].unique())\n        n_classes = len(unique_classes)\n\n        # Get prediction columns per class\n        class_prediction_columns = {}\n        for class_idx in range(n_classes):\n            class_prediction_columns[class_idx] = [\n                column for column in all_pred_columns\n                if column.startswith(f\"y_pred{class_idx}_\")\n            ]\n\n        # Calculate ensemble (average) probabilities\n        ensemble_probs = {}\n        for class_idx in range(n_classes):\n            if class_prediction_columns[class_idx]:\n                ensemble_probs[f\"y_pred{class_idx}_ensemble\"] = merged[\n                    class_prediction_columns[class_idx]\n                ].mean(axis=1)\n            else:\n                self.vlog(f\"No prediction columns found for class [{INFO_CLR}]{class_idx}[/]\")\n                ensemble_probs[f\"y_pred{class_idx}_ensemble\"] = 0.0\n\n        # Add ensemble probabilities to DataFrame\n        for column, probability in ensemble_probs.items():\n            merged[column] = probability\n\n        # Get probability matrix and make final predictions\n        ensemble_probability_columns = [f\"y_pred{class_idx}_ensemble\" for class_idx in range(n_classes)]\n        prob_matrix = merged[ensemble_probability_columns].values\n        predicted_classes = np.argmax(prob_matrix, axis=1)\n        merged[\"y_pred_label\"] = predicted_classes\n\n        # calculate metrics and print summary\n        metrics = self.calculate_metrics(merged)\n\n        # Optional summary\n        if print_summary:\n            summary = format_ensemble_summary(\n                len(predictions_list),\n                metrics[\"ConfusionMatrix\"],  # type: ignore\n                float(metrics[\"AUC\"]),\n                float(metrics[\"AP\"]),\n                float(metrics[\"Accuracy\"]),\n                float(metrics[\"F1\"])\n            )\n            self.vlog(summary)\n\n        # Save results\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        if output_path.suffix == \".csv\":\n            merged.to_csv(output_path, index=False)\n        else:\n            merged.to_parquet(output_path, index=False)\n        self.vlog(f\"Ensemble predictions saved to [{INFO_CLR}]{output_path}[/]\")\n\n        return merged, metrics\n\n    def compare_models(\n        self,\n        model_dir: Path | None = None,\n        metrics: list[str] = [\"Accuracy\", \"AUC\", \"F1\"]\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Compares evaluation metrics across multiple trained models.\n\n        Args:\n            model_dir (Path | None, optional): Directory containing model subdirectories.\n            metrics (list[str], optional): Metrics to include in the comparison.\n\n        Returns:\n            pd.DataFrame: Model-wise metric comparison table.\n        \"\"\"\n\n        model_dir = model_dir or self.model_dir\n\n        # Check if model_dir is a single model directory\n        if is_model_directory(model_dir):\n            model_paths = [model_dir]\n            self.vlog(f\"Single model directory detected: [{INFO_CLR}]{model_dir}[/]\")\n        # Else, collect all model subdirectories\n        else:\n            if not (model_paths := [subdir for subdir in model_dir.iterdir() if subdir.is_dir() and is_model_directory(subdir)]):\n                self.vlog(f\"No model directories found in [{INFO_CLR}]{model_dir}[/]\", LogLevel.WARNING)\n                raise ValueError(\"No model directories found for comparison\")\n\n        comparison_data = []\n        for model_path in model_paths:\n            try:\n                predictions = self.load_predictions(model_path)\n                model_metrics = self.calculate_metrics(predictions)\n\n                row: dict[str, str | float] = {\"model\": model_path.name}\n                for metric in metrics:\n                    if metric in model_metrics:\n                        value = model_metrics[metric]\n                        # Convert numpy arrays and other types to string representation\n                        if isinstance(value, np.ndarray):\n                            row[metric] = round(float(value), 2)\n                        else:\n                            row[metric] = round(float(value), 2)\n                    else:\n                        row[metric] = \"N/A\"\n\n                comparison_data.append(row)\n\n            except Exception as e:\n                self.vlog(f\"Failed to evaluate [{INFO_CLR}]{model_path.name}[/]: {e}\", LogLevel.WARNING)\n                continue\n\n        comparison_df = pd.DataFrame(comparison_data)\n\n        if not comparison_df.empty:\n            self.vlog(\"Model Comparison:\")\n            self.vlog(comparison_df.to_string(index=False))\n\n        return comparison_df\n\n    # === Plotting === #\n    def generate_plots(\n        self,\n        model_paths: list[Path] | None = None,\n        save_path: Path | None = None,\n        figsize: tuple[int, int] = (10, 10)\n    ) -&gt; None:\n        \"\"\"Generate all comparison plots and save them to `self.project_dir/figures`\"\"\"\n        # Collect models from expected folder if not provided\n        if model_paths is None:\n            model_paths = sorted(\n                [path for path in self.out_dir.iterdir() if path.is_dir()]\n            )\n\n        # Calculate and collect metrics for all models\n        combined_metrics = {}\n        for model_path in model_paths:\n            try:\n                predictions = self.load_predictions(model_path)\n                model_metrics = self.calculate_metrics(predictions)\n                combined_metrics[model_path.name] = model_metrics\n            except Exception as e:\n                self.vlog(f\"Failed to load metrics for {model_path.name}: {e}\")\n                continue\n\n        if not combined_metrics:\n            self.vlog(\"No valid model data found for generating plots\")\n            return\n\n        # Collect and execute all plotting methods\n        plots = cast(\n            dict[str, Figure], # Make sure the type annotation is correct\n            {\n                method_name.removeprefix('_plot_'): plot_method(\n                    combined_metrics,\n                    figsize=figsize,\n                )\n                for method_name in dir(self)\n                if (\n                    method_name.startswith('_plot_')\n                    and callable((plot_method := getattr(self, method_name)))\n                    and signature(plot_method).return_annotation == Figure\n                )\n            }\n        )\n\n        if not save_path:\n            save_path = self.out_dir / \"figures\"\n            save_path.mkdir(parents=True, exist_ok=True)\n\n        # Save all generated plots\n        for plot_name, fig in plots.items():\n            plot_file = save_path / f\"{plot_name}.png\"\n            fig.savefig(plot_file, dpi=300, bbox_inches='tight')\n            self.vlog(f\"Saved plot '[{INFO_CLR}]{plot_name}[/]' to [{INFO_CLR}]{plot_file}[/]\")\n        return\n\n    def _plot_roc_curves(\n        self,\n        combined_metrics: dict[str, dict[str, float | np.ndarray]],\n        figsize: tuple[int, int] = (10, 8)\n    ) -&gt; Figure:\n        \"\"\"Plot ROC curves for all models\"\"\"\n        from sklearn.metrics import auc, roc_curve\n\n        plt.figure(figsize=figsize)\n\n        colors = plt.cm.get_cmap('Set1')(np.linspace(0, 1, len(combined_metrics)))\n\n        for i, (model_name, _) in enumerate(combined_metrics.items()):\n            try:\n                # Load predictions for this model\n                model_path = self.out_dir / model_name\n                predictions = self.load_predictions(model_path)\n\n                y_true = predictions[\"y_true\"].astype(int)\n                num_classes = len(y_true.unique())\n\n                # Get prediction probabilities\n                pred_columns = [column for column in predictions.columns if column.startswith(\"y_pred\")]\n                ensemble_columns = [col for col in pred_columns if col.endswith(\"_ensemble\")]\n\n                if ensemble_columns:\n                    prob_columns = [f\"y_pred{i}_ensemble\" for i in range(num_classes)]\n                else:\n                    prob_columns = [f\"y_pred{i}\" for i in range(num_classes)]\n\n                prob_matrix = predictions[prob_columns].values\n\n                if num_classes == 2:\n                    # Binary classification - single ROC curve\n                    y_probs = prob_matrix[:, 1]  # Probabilities for positive class\n\n                    fpr, tpr, _ = roc_curve(y_true, y_probs)\n                    roc_auc = auc(fpr, tpr)\n\n                    plt.plot(\n                        fpr, tpr, \n                        color=colors[i], \n                        linewidth=2,\n                        label=f'{model_name} (AUC = {roc_auc:.3f})'\n                    )\n\n                else:\n                    # Multiclass - plot ROC curve for each class\n                    for class_idx in range(num_classes):\n                        y_true_binary = (y_true == class_idx).astype(int)\n                        y_probs_class = prob_matrix[:, class_idx]\n\n                        # Only plot if we have both classes\n                        if len(y_true_binary.unique()) &gt; 1:\n                            fpr, tpr, _ = roc_curve(y_true_binary, y_probs_class)\n                            roc_auc = auc(fpr, tpr)\n\n                            # Use different line styles for different classes\n                            line_style = ['-', '--', '-.', ':'][class_idx % 4]\n\n                            plt.plot(\n                                fpr, tpr,\n                                color=colors[i],\n                                linestyle=line_style,\n                                linewidth=2,\n                                label=f'{model_name} Class {class_idx} (AUC = {roc_auc:.3f})'\n                            )\n\n            except Exception as e:\n                self.vlog(f\"Could not plot ROC curve for {model_name}: {e}\", LogLevel.WARNING)\n                continue\n\n        # Plot diagonal line (random classifier)\n        plt.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random')\n\n        plt.xlabel('False Positive Rate', fontsize=12)\n        plt.ylabel('True Positive Rate', fontsize=12)\n        plt.title('ROC Curves', fontsize=14, fontweight='bold')\n        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        plt.grid(True, alpha=0.3)\n        plt.xlim([0, 1])\n        plt.ylim([0, 1])\n\n        plt.tight_layout()\n        return plt.gcf()\n\n    def _plot_model_comparison(\n        self,\n        combined_metrics: dict[str, dict[str, float | np.ndarray]],\n        figsize: tuple[int, int] = (12, 8)\n    ) -&gt; Figure:\n        data = pd.DataFrame(combined_metrics)\n        metrics = [col for col in data.index if col != \"ConfusionMatrix\" and col != \"PerClassAccuracy\"]\n        n_metrics = len(metrics)\n\n        # Create subplots\n        fig, axes = plt.subplots(1, n_metrics, figsize=figsize, sharey=False)\n        # n_metrics == 1 means only 1 subplot, cast to list for consistency\n        if n_metrics == 1:\n            axes = cast(\n                list[Axes],\n                [axes]\n            )\n        # Otherwise axes is a list of subplots\n        else:\n            axes = cast(\n                list[Axes],\n                axes\n            )\n\n        colors = plt.cm.get_cmap('Set1')(np.linspace(0, 1, len(data)))\n        x_positions = np.arange(len(data.columns))\n        model_names = list(data.columns)\n\n        for i, metric in enumerate(metrics):\n            ax = axes[i]\n            # Plot single metric\n            bars = ax.bar(\n                x_positions,\n                data.loc[metric],\n                color=colors,\n                alpha=0.8,\n                edgecolor='black',\n                linewidth=0.5,\n            )\n\n            bar: Rectangle # Iterating over a BarContainer gives Rectangle objects\n            for bar, value in zip(bars, data.loc[metric]):\n                height = bar.get_height()\n                # Place actual value above bar\n                ax.text(\n                    bar.get_x() + bar.get_width()/2., \n                    height + 0.005,\n                    f'{value:.3f}',\n                    ha='center',\n                    va='bottom',\n                    fontsize=9\n                )\n\n            ax.set_xticks(x_positions)\n            # Set model names as x-tick labels\n            # Since model names can be long, center them to the right\n            # To avoid any offset issues\n            ax.set_xticklabels(\n                model_names,\n                rotation=45,\n                ha=\"right\",\n                rotation_mode=\"anchor\"\n            )\n\n            ax.set_title(f'{metric}', fontsize=12, fontweight='bold')\n            ax.set_ylabel(metric, fontsize=10)\n            ax.set_ylim(0, 1.1)\n            ax.grid(True, alpha=0.3, axis='y')\n\n        plt.tight_layout()\n        return fig\n\n    def _plot_box_plots(\n        self,\n        combined_metrics: dict[str, dict[str, float | np.ndarray]],\n        figsize: tuple[int, int] = (10, 8)\n    ) -&gt; Figure:\n        # Collect data in long format for box plots\n        plot_data = []\n        for _, metrics in combined_metrics.items():\n            for metric_name, metric_value in metrics.items():\n                if metric_name in [\"ConfusionMatrix\", \"PerClassAccuracy\"]:\n                    continue\n\n                plot_data.append({\n                    'Metric': metric_name,\n                    'Value': float(metric_value)\n                })\n\n        df = pd.DataFrame(plot_data)\n\n        # Create the plot\n        plt.figure(figsize=figsize)\n\n        sns.boxplot(\n            data=df,\n            x='Metric',\n            y='Value',\n            palette='Set2',\n            width=0.5\n        )\n\n        sns.stripplot(\n            data=df,\n            x='Metric',\n            y='Value',\n            color='black',\n            size=6,\n            jitter=True,\n            alpha=0.7\n        )\n\n        plt.title('Metric Distributions', fontsize=14, fontweight='bold')\n        plt.ylabel('Value', fontsize=12)\n        plt.xlabel('Metric', fontsize=12)\n        plt.ylim(0, 1.1)\n        plt.grid(True, alpha=0.3, axis='y')\n\n        plt.tight_layout()\n        return plt.gcf()\n\n    def _plot_per_class_accuracy(\n        self,\n        combined_metrics: dict[str, dict[str, float | np.ndarray]],\n        figsize: tuple[int, int] = (12, 8)\n    ) -&gt; Figure:\n        # Prepare data for plotting\n        data = []\n        for model_name, metrics in combined_metrics.items():\n            per_class_acc = metrics.get(\"PerClassAccuracy\")\n            if isinstance(per_class_acc, np.ndarray):\n                for class_idx, acc in enumerate(per_class_acc):\n                    data.append({\n                        \"Model\": model_name,\n                        \"Class\": f\"Class {class_idx}\",\n                        \"Accuracy\": acc\n                    })\n\n        df = pd.DataFrame(data)\n\n        # Create the plot\n        plt.figure(figsize=figsize)\n\n        # Create a grouped bar plot\n        ax = sns.barplot(data=df, x='Class', y='Accuracy', hue='Model', alpha=0.8)\n\n        plt.title('Per-Class Accuracy Comparison', fontsize=14, fontweight='bold')\n        plt.ylabel('Accuracy', fontsize=12)\n        plt.xlabel('Class', fontsize=12)\n        plt.ylim(0, 1.1)\n        plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n        plt.grid(True, alpha=0.3, axis='y')\n\n        # Add value labels on bars\n        for container in ax.containers:\n            if isinstance(container, BarContainer):\n                ax.bar_label(container, fmt='%.2f', fontsize=9)\n\n        plt.tight_layout()\n        return plt.gcf()\n</code></pre>"},{"location":"api/evaluation/#automil.evaluation.Evaluator.calculate_metrics","title":"calculate_metrics","text":"<pre><code>calculate_metrics(\n    predictions: DataFrame | Path | str,\n) -&gt; dict[str, float | np.ndarray]\n</code></pre> <p>Computes classification metrics from prediction outputs.</p> <p>Supports both binary and multi-class classification and automatically detects ensemble predictions when present.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>DataFrame | Path | str</code> <p>Predictions DataFrame or path to a <code>predictions.parquet</code> file.</p> required <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>dict[str, float | np.ndarray]: Dictionary containing: - Accuracy - AUC - Average Precision - F1 score - Confusion matrix - Per-class accuracy</p> Source code in <code>automil/evaluation.py</code> <pre><code>def calculate_metrics(\n    self,\n    predictions: pd.DataFrame | Path | str\n) -&gt; dict[str, float | np.ndarray]:\n    \"\"\"\n    Computes classification metrics from prediction outputs.\n\n    Supports both binary and multi-class classification and automatically\n    detects ensemble predictions when present.\n\n    Args:\n        predictions (pd.DataFrame | Path | str): Predictions DataFrame or path\n            to a ``predictions.parquet`` file.\n\n    Returns:\n        dict[str, float | np.ndarray]: Dictionary containing:\n            - Accuracy\n            - AUC\n            - Average Precision\n            - F1 score\n            - Confusion matrix\n            - Per-class accuracy\n    \"\"\"\n\n    # Make sure we're working with a loaded DataFrame\n    match predictions:\n        case Path() | str():\n            predictions = self.load_predictions(Path(predictions))\n        case pd.DataFrame():\n            pass\n\n    # Extract true labels and calculate number of classes\n    y_true = predictions[\"y_true\"].astype(int)\n    num_classes = len(y_true.unique())\n\n    # We expect columns containing prediction probabilites to start with 'y_pred' (e.g 'y_pred0', 'y_pred1', ...)\n    # Similarly, we may have ensemble predictions ending with '_ensemble' (e.g., 'y_pred0_ensemble', 'y_pred1_ensemble', ...)\n    pred_columns = [column for column in predictions.columns if column.startswith(\"y_pred\")]\n    # Case 1: Ensemble predictions (priority)\n    ensemble_columns = [col for col in pred_columns if col.endswith(\"_ensemble\")]\n    if ensemble_columns:\n        # Use ensemble predictions\n        prob_columns = [f\"y_pred{i}_ensemble\" for i in range(num_classes)]\n        prediction_type = \"ensemble\"\n    else:\n        # Case 2: Single model predictions\n        # Get regular y_pred columns (y_pred0, y_pred1, etc.)\n        prob_columns = [f\"y_pred{i}\" for i in range(num_classes)]\n        prediction_type = \"single model\"\n\n    # Verify all expected probability columns exist\n    missing_columns = [col for col in prob_columns if col not in predictions.columns]\n    if missing_columns:\n        raise ValueError(f\"Missing probability columns for {prediction_type} predictions: {missing_columns}\")\n\n    # Get probability matrix\n    prob_matrix = predictions[prob_columns].values\n\n    # Get predicted classes\n    if \"y_pred_label\" in predictions.columns:\n        y_pred = predictions[\"y_pred_label\"].astype(int)\n    else:\n        y_pred = np.argmax(prob_matrix, axis=1)\n\n    # Calculate metrics\n    accuracy = float(accuracy_score(y_true, y_pred))\n    cm = confusion_matrix(y_true, y_pred)\n\n    # Binary classification\n    if num_classes == 2:\n        y_probs = prob_matrix[:, 1] # We really only need the prediction probabilities for label 1\n\n        auc = float(roc_auc_score(y_true, y_probs))\n        ap  = float(average_precision_score(y_true, y_probs))\n        f1  = float(f1_score(y_true, y_pred))\n\n    # Multiclass\n    else:\n        auc = float(roc_auc_score(y_true, prob_matrix, multi_class=\"ovr\", average=\"macro\"))\n\n        ap_scores = []\n        for class_idx in range(num_classes):\n            # 0 if label is class_idx, 1 otherwise\n            y_true_binary = (y_true == class_idx).astype(int)\n            # Prediction probabilities for this class\n            y_probs_class = prob_matrix[:, class_idx]\n\n            if len(y_true_binary.unique()) &gt; 1:\n                ap_class = average_precision_score(y_true_binary, y_probs_class)\n                ap_scores.append(ap_class)\n\n        ap = float(np.mean(ap_scores)) if ap_scores else 0.0\n        f1 = float(f1_score(y_true, y_pred, average=\"macro\"))\n\n    per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n\n    return {\n        \"Accuracy\": accuracy,\n        \"AUC\": auc,\n        \"AP\": ap,\n        \"F1\": f1,\n        \"ConfusionMatrix\": cm,\n        \"PerClassAccuracy\": per_class_accuracy\n    }\n</code></pre>"},{"location":"api/evaluation/#automil.evaluation.Evaluator.compare_models","title":"compare_models","text":"<pre><code>compare_models(\n    model_dir: Path | None = None,\n    metrics: list[str] = [\"Accuracy\", \"AUC\", \"F1\"],\n) -&gt; pd.DataFrame\n</code></pre> <p>Compares evaluation metrics across multiple trained models.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Path | None</code> <p>Directory containing model subdirectories.</p> <code>None</code> <code>metrics</code> <code>list[str]</code> <p>Metrics to include in the comparison.</p> <code>['Accuracy', 'AUC', 'F1']</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Model-wise metric comparison table.</p> Source code in <code>automil/evaluation.py</code> <pre><code>def compare_models(\n    self,\n    model_dir: Path | None = None,\n    metrics: list[str] = [\"Accuracy\", \"AUC\", \"F1\"]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compares evaluation metrics across multiple trained models.\n\n    Args:\n        model_dir (Path | None, optional): Directory containing model subdirectories.\n        metrics (list[str], optional): Metrics to include in the comparison.\n\n    Returns:\n        pd.DataFrame: Model-wise metric comparison table.\n    \"\"\"\n\n    model_dir = model_dir or self.model_dir\n\n    # Check if model_dir is a single model directory\n    if is_model_directory(model_dir):\n        model_paths = [model_dir]\n        self.vlog(f\"Single model directory detected: [{INFO_CLR}]{model_dir}[/]\")\n    # Else, collect all model subdirectories\n    else:\n        if not (model_paths := [subdir for subdir in model_dir.iterdir() if subdir.is_dir() and is_model_directory(subdir)]):\n            self.vlog(f\"No model directories found in [{INFO_CLR}]{model_dir}[/]\", LogLevel.WARNING)\n            raise ValueError(\"No model directories found for comparison\")\n\n    comparison_data = []\n    for model_path in model_paths:\n        try:\n            predictions = self.load_predictions(model_path)\n            model_metrics = self.calculate_metrics(predictions)\n\n            row: dict[str, str | float] = {\"model\": model_path.name}\n            for metric in metrics:\n                if metric in model_metrics:\n                    value = model_metrics[metric]\n                    # Convert numpy arrays and other types to string representation\n                    if isinstance(value, np.ndarray):\n                        row[metric] = round(float(value), 2)\n                    else:\n                        row[metric] = round(float(value), 2)\n                else:\n                    row[metric] = \"N/A\"\n\n            comparison_data.append(row)\n\n        except Exception as e:\n            self.vlog(f\"Failed to evaluate [{INFO_CLR}]{model_path.name}[/]: {e}\", LogLevel.WARNING)\n            continue\n\n    comparison_df = pd.DataFrame(comparison_data)\n\n    if not comparison_df.empty:\n        self.vlog(\"Model Comparison:\")\n        self.vlog(comparison_df.to_string(index=False))\n\n    return comparison_df\n</code></pre>"},{"location":"api/evaluation/#automil.evaluation.Evaluator.create_ensemble_predictions","title":"create_ensemble_predictions","text":"<pre><code>create_ensemble_predictions(\n    model_dir: Path | None = None,\n    output_path: Path | None = None,\n    print_summary: bool = True,\n) -&gt; tuple[pd.DataFrame, dict[str, float | np.ndarray]]\n</code></pre> <p>Generates ensemble predictions by averaging outputs across multiple models.</p> <p>Ensemble probabilities are computed per class and used to derive final predictions and evaluation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Path | None</code> <p>Directory containing trained models.</p> <code>None</code> <code>output_path</code> <code>Path | None</code> <p>Output file path (.csv or .parquet).</p> <code>None</code> <code>print_summary</code> <code>bool</code> <p>Print a formatted metric summary.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid prediction files are found.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, dict[str, float | ndarray]]</code> <ul> <li>Ensemble predictions DataFrame</li> <li>Dictionary of evaluation metrics</li> </ul> Source code in <code>automil/evaluation.py</code> <pre><code>def create_ensemble_predictions(\n    self,\n    model_dir: Path | None = None,\n    output_path: Path | None = None,\n    print_summary: bool = True\n) -&gt; tuple[pd.DataFrame, dict[str, float | np.ndarray]]:\n    \"\"\"\n    Generates ensemble predictions by averaging outputs across multiple models.\n\n    Ensemble probabilities are computed per class and used to derive final\n    predictions and evaluation metrics.\n\n    Args:\n        model_dir (Path | None, optional): Directory containing trained models.\n        output_path (Path | None, optional): Output file path (.csv or .parquet).\n        print_summary (bool, optional): Print a formatted metric summary.\n\n    Raises:\n        ValueError: If no valid prediction files are found.\n\n    Returns:\n        tuple:\n            - Ensemble predictions DataFrame\n            - Dictionary of evaluation metrics\n    \"\"\"\n\n    model_dir = model_dir or self.model_dir\n    output_path = output_path or (self.out_dir / \"ensemble_predictions.parquet\")\n\n    # Check if model_dir is a single model directory\n    if is_model_directory(model_dir):\n        model_paths = [model_dir]\n        self.vlog(f\"Single model directory detected: [{INFO_CLR}]{model_dir}[/]\")\n    # Else, collect all model subdirectories\n    else:\n        if not (model_paths := [subdir for subdir in model_dir.iterdir() if subdir.is_dir() and is_model_directory(subdir)]):\n            self.vlog(f\"No model directories found in [{INFO_CLR}]{model_dir}[/]\", LogLevel.WARNING)\n            raise ValueError(\"No model directories found for ensembling\")\n\n    # Try to load predictions from each model that has been evaluated (should all be in model_dir)\n    predictions_list: list[pd.DataFrame] = []\n    for model_idx, submodel_dir in enumerate(model_paths):\n        try:\n            predictions = self.load_predictions(submodel_dir)\n\n            # Add the model index to predictions columns so we can merge later\n            pred_columns = [column for column in predictions.columns if column.startswith(\"y_pred\")]\n            rename_map = {pred_column: f\"{pred_column}_model{model_idx}\" for pred_column in pred_columns}\n            predictions = predictions.rename(columns=rename_map)\n            predictions_list.append(predictions)\n\n            self.vlog(f\"Loaded predictions from model [{INFO_CLR}]{submodel_dir.name}[/] ([{INFO_CLR}]{model_idx+1}[/]/[{INFO_CLR}]{len(os.listdir(model_dir))}[/])\")\n        except Exception as e:\n            self.vlog(f\"Error loading predictions from {submodel_dir}: {e}\", LogLevel.WARNING)\n            continue\n\n    if not predictions_list:\n        raise ValueError(\"Failed to load any predictions from model directory\")\n\n    # Merge predictions on the base columns\n    merged = predictions_list[0].copy()\n\n    for predictions in predictions_list[1:]:\n        merged = merged.merge(\n            predictions,\n            on=[\"slide\", \"y_true\"],\n            how=\"inner\"\n        )\n\n    # Get all prediction columns\n    all_pred_columns = [\n        column for column in merged.columns\n        if column.startswith(\"y_pred\")\n    ]\n\n    if not all_pred_columns:\n        raise ValueError(\"No prediction columns found for ensembling\")\n\n    unique_classes = sorted(merged[\"y_true\"].unique())\n    n_classes = len(unique_classes)\n\n    # Get prediction columns per class\n    class_prediction_columns = {}\n    for class_idx in range(n_classes):\n        class_prediction_columns[class_idx] = [\n            column for column in all_pred_columns\n            if column.startswith(f\"y_pred{class_idx}_\")\n        ]\n\n    # Calculate ensemble (average) probabilities\n    ensemble_probs = {}\n    for class_idx in range(n_classes):\n        if class_prediction_columns[class_idx]:\n            ensemble_probs[f\"y_pred{class_idx}_ensemble\"] = merged[\n                class_prediction_columns[class_idx]\n            ].mean(axis=1)\n        else:\n            self.vlog(f\"No prediction columns found for class [{INFO_CLR}]{class_idx}[/]\")\n            ensemble_probs[f\"y_pred{class_idx}_ensemble\"] = 0.0\n\n    # Add ensemble probabilities to DataFrame\n    for column, probability in ensemble_probs.items():\n        merged[column] = probability\n\n    # Get probability matrix and make final predictions\n    ensemble_probability_columns = [f\"y_pred{class_idx}_ensemble\" for class_idx in range(n_classes)]\n    prob_matrix = merged[ensemble_probability_columns].values\n    predicted_classes = np.argmax(prob_matrix, axis=1)\n    merged[\"y_pred_label\"] = predicted_classes\n\n    # calculate metrics and print summary\n    metrics = self.calculate_metrics(merged)\n\n    # Optional summary\n    if print_summary:\n        summary = format_ensemble_summary(\n            len(predictions_list),\n            metrics[\"ConfusionMatrix\"],  # type: ignore\n            float(metrics[\"AUC\"]),\n            float(metrics[\"AP\"]),\n            float(metrics[\"Accuracy\"]),\n            float(metrics[\"F1\"])\n        )\n        self.vlog(summary)\n\n    # Save results\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    if output_path.suffix == \".csv\":\n        merged.to_csv(output_path, index=False)\n    else:\n        merged.to_parquet(output_path, index=False)\n    self.vlog(f\"Ensemble predictions saved to [{INFO_CLR}]{output_path}[/]\")\n\n    return merged, metrics\n</code></pre>"},{"location":"api/evaluation/#automil.evaluation.Evaluator.evaluate_models","title":"evaluate_models","text":"<pre><code>evaluate_models(\n    model_dir: Path | None = None,\n    bags_dir: Path | None = None,\n    out_dir: Path | None = None,\n    generate_attention_heatmaps: bool = False,\n) -&gt; None\n</code></pre> <p>Evaluates one or more trained models.</p> <p>Detects each trained model directory inside <code>model_dir</code> and evaluates them independently. Predictions and metrics are written to the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Path | None</code> <p>Model directory or parent directory of models to evaluate.</p> <code>None</code> <code>bags_dir</code> <code>Path | None</code> <p>Feature bag directory.</p> <code>None</code> <code>out_dir</code> <code>Path | None</code> <p>Output directory.</p> <code>None</code> <code>generate_attention_heatmaps</code> <code>bool</code> <p>Generate attention heatmaps.</p> <code>False</code> Source code in <code>automil/evaluation.py</code> <pre><code>def evaluate_models(\n    self,\n    model_dir: Path | None = None,\n    bags_dir: Path | None = None,\n    out_dir: Path | None = None,\n    generate_attention_heatmaps: bool = False\n) -&gt; None:\n    \"\"\"\n    Evaluates one or more trained models.\n\n    Detects each trained model directory inside `model_dir` and evaluates them independently.\n    Predictions and metrics are written to the output directory.\n\n    Args:\n        model_dir (Path | None, optional): Model directory or parent directory of models to evaluate.\n        bags_dir (Path | None, optional): Feature bag directory.\n        out_dir (Path | None, optional): Output directory.\n        generate_attention_heatmaps (bool, optional): Generate attention heatmaps.\n    \"\"\"\n    # Default to instance variables if none provided\n    model_dir = model_dir or self.model_dir\n    bags_dir = bags_dir or self.bags_dir\n    out_dir = out_dir or self.out_dir\n\n    # Check if model_dir is a single model directory\n    if is_model_directory(model_dir):\n        model_paths = [model_dir]\n        self.vlog(f\"Single model directory detected: {model_dir}\")\n    # Else, collect all model subdirectories\n    else:\n        if not (model_paths := [subdir for subdir in model_dir.iterdir() if subdir.is_dir() and is_model_directory(subdir)]):\n            self.vlog(f\"No model directories found in {model_dir}\", LogLevel.WARNING)\n            return\n\n    # Iterate over each model directory and evaluate\n    for model_idx, model_path in enumerate(model_paths):\n        self.vlog(f\"Evaluating model [{INFO_CLR}]{model_idx+1}[/]/[{INFO_CLR}]{len(model_paths)}[/]: [{INFO_CLR}]{model_path}[/]\")\n        try:\n            eval_mil(\n                weights=str(model_path),\n                bags=str(bags_dir),\n                dataset=self.dataset,\n                outcomes=\"label\",\n                outdir=str(out_dir),\n                attention_heatmaps=generate_attention_heatmaps\n            )\n            self.vlog(\"Evaluation complete.\\n\")\n        except Exception as e:\n            self.vlog(f\"Error evaluating model at {model_path}: {e}\", LogLevel.ERROR)\n            continue\n</code></pre>"},{"location":"api/evaluation/#automil.evaluation.Evaluator.generate_plots","title":"generate_plots","text":"<pre><code>generate_plots(\n    model_paths: list[Path] | None = None,\n    save_path: Path | None = None,\n    figsize: tuple[int, int] = (10, 10),\n) -&gt; None\n</code></pre> <p>Generate all comparison plots and save them to <code>self.project_dir/figures</code></p> Source code in <code>automil/evaluation.py</code> <pre><code>def generate_plots(\n    self,\n    model_paths: list[Path] | None = None,\n    save_path: Path | None = None,\n    figsize: tuple[int, int] = (10, 10)\n) -&gt; None:\n    \"\"\"Generate all comparison plots and save them to `self.project_dir/figures`\"\"\"\n    # Collect models from expected folder if not provided\n    if model_paths is None:\n        model_paths = sorted(\n            [path for path in self.out_dir.iterdir() if path.is_dir()]\n        )\n\n    # Calculate and collect metrics for all models\n    combined_metrics = {}\n    for model_path in model_paths:\n        try:\n            predictions = self.load_predictions(model_path)\n            model_metrics = self.calculate_metrics(predictions)\n            combined_metrics[model_path.name] = model_metrics\n        except Exception as e:\n            self.vlog(f\"Failed to load metrics for {model_path.name}: {e}\")\n            continue\n\n    if not combined_metrics:\n        self.vlog(\"No valid model data found for generating plots\")\n        return\n\n    # Collect and execute all plotting methods\n    plots = cast(\n        dict[str, Figure], # Make sure the type annotation is correct\n        {\n            method_name.removeprefix('_plot_'): plot_method(\n                combined_metrics,\n                figsize=figsize,\n            )\n            for method_name in dir(self)\n            if (\n                method_name.startswith('_plot_')\n                and callable((plot_method := getattr(self, method_name)))\n                and signature(plot_method).return_annotation == Figure\n            )\n        }\n    )\n\n    if not save_path:\n        save_path = self.out_dir / \"figures\"\n        save_path.mkdir(parents=True, exist_ok=True)\n\n    # Save all generated plots\n    for plot_name, fig in plots.items():\n        plot_file = save_path / f\"{plot_name}.png\"\n        fig.savefig(plot_file, dpi=300, bbox_inches='tight')\n        self.vlog(f\"Saved plot '[{INFO_CLR}]{plot_name}[/]' to [{INFO_CLR}]{plot_file}[/]\")\n    return\n</code></pre>"},{"location":"api/evaluation/#automil.evaluation.Evaluator.generate_predictions","title":"generate_predictions","text":"<pre><code>generate_predictions(\n    model_dir: Path | None = None,\n    bags_dir: Path | None = None,\n    out_dir: Path | None = None,\n) -&gt; None\n</code></pre> <p>Generates prediction outputs for one or more trained models.</p> <p>Predictions are saved per model in <code>predictions.parquet</code> format.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Path | None</code> <p>Directory containing model subdirectories.</p> <code>None</code> <code>bags_dir</code> <code>Path | None</code> <p>Feature bag directory.</p> <code>None</code> <code>out_dir</code> <code>Path | None</code> <p>Output directory.</p> <code>None</code> Source code in <code>automil/evaluation.py</code> <pre><code>def generate_predictions(\n    self,\n    model_dir: Path | None = None,\n    bags_dir: Path | None = None,\n    out_dir: Path | None = None\n) -&gt; None:\n    \"\"\"\n    Generates prediction outputs for one or more trained models.\n\n    Predictions are saved per model in ``predictions.parquet`` format.\n\n    Args:\n        model_dir (Path | None, optional): Directory containing model subdirectories.\n        bags_dir (Path | None, optional): Feature bag directory.\n        out_dir (Path | None, optional): Output directory.\n    \"\"\"\n    # Default to instance variables if none provided\n    model_dir = model_dir or self.model_dir\n    bags_dir = bags_dir or self.bags_dir\n    out_dir = out_dir or self.out_dir\n\n    # Check if model_dir is a single model directory\n    if is_model_directory(model_dir):\n        model_paths = [model_dir]\n        self.vlog(f\"Single model directory detected: [{INFO_CLR}]{model_dir}[/]\")\n    # Else, collect all model subdirectories\n    else:\n        if not (model_paths := [subdir for subdir in model_dir.iterdir() if subdir.is_dir() and is_model_directory(subdir)]):\n            self.vlog(f\"No model directories found in [{INFO_CLR}]{model_dir}[/]\", LogLevel.WARNING)\n            return\n\n    # Iterate over each model directory and generate predictions\n    for model_idx, model_path in enumerate(model_paths):\n        self.vlog(f\"Generating predictions with model [{INFO_CLR}]{model_idx+1}[/]/[{INFO_CLR}]{len(model_paths)}[/]: [{INFO_CLR}]{model_path}[/]\")\n        try:\n            predictions = predict_mil(\n                model=str(model_path),\n                bags=str(bags_dir),\n                dataset=self.dataset,\n                outcomes=\"label\",\n            )\n            # Cast to DataFrame\n            # Can do this safely since predict_mil always returns a DataFrame if attention==False\n            predictions = pd.DataFrame(predictions)\n\n            # Save predictions to out_dir/model_name/predictions.parquet\n            model_out_dir = out_dir / model_path.name\n            model_out_dir.mkdir(parents=True, exist_ok=True)\n            predictions_path = model_out_dir / \"predictions.parquet\"\n            predictions.to_parquet(predictions_path, index=False)\n            self.vlog(f\"Predictions saved to [{INFO_CLR}]{predictions_path}[/]\")\n\n        except Exception as e:\n            self.vlog(f\"Error evaluating model at {model_path}: {e}\", LogLevel.ERROR)\n            continue\n</code></pre>"},{"location":"api/evaluation/#automil.evaluation.Evaluator.load_predictions","title":"load_predictions","text":"<pre><code>load_predictions(model_path: Path) -&gt; pd.DataFrame\n</code></pre> <p>Loads and validates prediction outputs from a trained model directory.</p> <p>The predictions file must contain: - One or more probability columns starting with <code>y_pred</code> - Base columns <code>slide</code> and <code>y_true</code></p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>Path to a trained model directory.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>predictions.parquet</code> is missing.</p> <code>ValueError</code> <p>If required prediction or base columns are absent.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Loaded and validated predictions.</p> Source code in <code>automil/evaluation.py</code> <pre><code>def load_predictions(self, model_path: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads and validates prediction outputs from a trained model directory.\n\n    The predictions file must contain:\n    - One or more probability columns starting with ``y_pred``\n    - Base columns ``slide`` and ``y_true``\n\n    Args:\n        model_path (Path): Path to a trained model directory.\n\n    Raises:\n        FileNotFoundError: If ``predictions.parquet`` is missing.\n        ValueError: If required prediction or base columns are absent.\n\n    Returns:\n        pd.DataFrame: Loaded and validated predictions.\n    \"\"\"\n    if not (predictions_path := model_path / \"predictions.parquet\").exists():\n        raise FileNotFoundError(f\"{model_path} does not contain a 'predictions.parquet' file\")\n\n    predictions = pd.read_parquet(predictions_path)\n\n    all_columns = [column for column in predictions.columns]\n    # We expect columns containing prediction probabilites to start with 'y_pred' (e.g 'y_pred0', 'y_pred1', ...)\n    pred_columns = [column for column in all_columns if column.startswith(\"y_pred\")]\n    # Similarly, we expect predictions to contain 'slide' and 'y_true' columns\n    base_columns = [\"slide\", \"y_true\"]\n\n    if not pred_columns:\n        raise ValueError(\"'predictions.parquet' does not contain the expected prediction columns\")\n    elif not all(base_column in all_columns for base_column in base_columns):\n        raise ValueError(\"'predictions.parquet' does not contain the expected base columns\")\n\n    return predictions\n</code></pre>"},{"location":"api/model/","title":"ModelManager","text":"<p><code>automil.model.ModelManager</code> provides model management utilities and is responsible for instantiating MIL models and validating hyperparameters against model-specific contraints and limits</p>"},{"location":"api/model/#automil.model.ModelManager","title":"ModelManager","text":"<p>Manages the instantiation and configuration of MIL models.</p> This class provides <ul> <li>An interface for creating automil supported MIL models</li> <li>Model-specific hyperparameter validation and adjustments should they be outside of recommended model-limits</li> <li>dummy input generation for debugging and validation</li> </ul> Source code in <code>automil/model.py</code> <pre><code>class ModelManager:\n    \"\"\"\n    Manages the instantiation and configuration of MIL models.\n\n    This class provides:\n        - An interface for creating automil supported MIL models\n        - Model-specific hyperparameter validation and adjustments should they be outside of recommended model-limits\n        - dummy input generation for debugging and validation\n    \"\"\"\n    # Baseline internal model configurations\n    _MODEL_CONFIGS: dict[ModelType, ModelConfig] = {\n        ModelType.Attention_MIL: ModelConfig(\n            model_cls=Attention_MIL,\n            slideflow_model_name=\"attention_mil\",\n            input_params={\"input_dim\": \"n_feats\", \"num_classes\": \"n_out\"},\n            min_lr=1e-5,\n            max_lr=1e-4,\n            max_batch_size=MAX_BATCH_SIZE,\n            max_tiles_per_bag=1000, # Can be quite large for Attention_MIL\n        ),\n        # We use smaller batch sizes for TransMIL due to memory constraints\n        ModelType.TransMIL: ModelConfig(\n            model_cls=TransMIL,\n            slideflow_model_name=\"transmil\",\n            input_params={\"input_dim\": \"n_feats\", \"num_classes\": \"n_out\"},\n            min_lr=1e-5,\n            max_lr=1e-4,\n            max_batch_size=32,\n            max_tiles_per_bag=500,\n        ),\n        ModelType.BistroTransformer: ModelConfig(\n            model_cls=BistroTransformer,\n            slideflow_model_name=\"bistro_transformer\",\n            input_params={\"input_dim\": \"dim\", \"num_classes\": \"heads\"},\n            min_lr=1e-5,\n            max_lr=1e-4,\n            max_batch_size=MAX_BATCH_SIZE,\n            max_tiles_per_bag=1000,\n        )\n    }\n\n    def __init__(self, model_type: ModelType) -&gt; None:\n        f\"\"\"Instantiates a ModelManager object\n\n        Args:\n            model_type (ModelType): Type of model to instantiate. Can be one of: {\n                [model.name for model in ModelType]\n            }\n        \"\"\"\n        self.model_type = model_type\n        self.config = self._MODEL_CONFIGS[model_type]\n\n    @property\n    def slideflow_name(self) -&gt; str:\n        \"\"\"\n        Slideflow-internal identifier for the managed model.\n\n        Returns:\n            str:\n                Slideflow model name.\n        \"\"\"\n        return self.config.slideflow_model_name\n\n    @property\n    def model_class(self) -&gt; type[nn.Module]:\n        \"\"\"\n        Corresponding python class implementing the model.\n\n        Returns:\n            type[nn.Module]:\n                Model class.\n        \"\"\"\n        return self.config.model_cls\n\n    def create_model(self, input_dim: int = 1024, num_classes: int = 2, **kwargs) -&gt; nn.Module:\n        \"\"\"Instantiates the model with validated hyperparameters.\n\n        Args:\n            input_dim (int, optional): Feature dimensions. Defaults to 1024.\n            num_classes (int, optional): Number of classes. Defaults to 2.\n\n        Returns:\n            nn.Module: Instantiated model\n        \"\"\"\n        # Map standardized parameter names to model-specific names\n        model_params = {\n            self.config.input_params[\"input_dim\"]: input_dim,\n            self.config.input_params[\"num_classes\"]: num_classes,\n        }\n        # Update with remaining kwargs\n        model_params.update(kwargs)\n\n        # Fallback: If instantiation fails, try with defaults\n        try:\n            return self.model_class(**model_params)\n        except TypeError as e:\n            return self.model_class()\n\n    def create_dummy_input(\n        self, \n        batch_size: int, \n        tiles_per_bag: int, \n        input_dim: int\n    ) -&gt; tuple:\n        \"\"\"Creates an appropriate dummy input for the model\n\n        Dummy input tensors can be used for a variety of tasks. Primarily they are used\n        to perform `dry runs`, for example to measure the memory reservation of a model instance\n\n        Args:\n            batch_size: Number of samples in batch\n            tiles_per_bag: Number of tiles per bag\n            input_dim: Feature dimension\n\n        Returns:\n            Tuple of tensors to pass to model forward()\n        \"\"\"\n        match self.model_type:\n\n            case ModelType.Attention_MIL:\n                # Both expect a lens tensor in addition to input\n                dummy_input = torch.randn(batch_size, tiles_per_bag, input_dim).cuda()\n                lens = torch.tensor([tiles_per_bag] * batch_size).cuda()\n                return (dummy_input, lens)\n\n            case ModelType.TransMIL | ModelType.BistroTransformer:\n                # BistroTransformer expects only input (no lens)\n                dummy_input = torch.randn(batch_size, tiles_per_bag, input_dim).cuda()\n                return (dummy_input,)\n\n\n    def validate_hyperparameters(self, lr: float, batch_size: int, max_tiles_per_bag: int) -&gt; dict[str, float | int]:\n        \"\"\"\n        Validates a set of hyperparameters against model-specific constraints.\n\n        Args:\n            lr (float):\n                Learning rate.\n            batch_size (int):\n                Batch size.\n            max_tiles_per_bag (int):\n                Maximum tiles per bag.\n\n        Returns:\n            dict[str, float | int]:\n                Suggested parameter adjustments for out-of-range values.\n        \"\"\"\n        suggestions = {}\n\n        # TODO | Better tuning logic / strategy (probably for all but definitely for lr)\n        if not (self.config.min_lr &lt;= lr &lt;= self.config.max_lr):\n            suggestions[\"lr\"] = (self.config.min_lr + self.config.max_lr) / 2\n\n        if batch_size &gt; self.config.max_batch_size:\n            suggestions[\"batch_size\"] = self.config.max_batch_size\n\n        if max_tiles_per_bag &gt; self.config.max_tiles_per_bag:\n            suggestions[\"max_tiles_per_bag\"] = self.config.max_tiles_per_bag\n\n        return suggestions\n\n    @classmethod\n    def compare_models(cls) -&gt; str:\n        \"\"\"Generates a comparison table for all available models\n\n        Returns:\n            str: A comparison table as string\n        \"\"\"\n        from tabulate import tabulate\n\n        table = []\n        for model_type, config in cls._MODEL_CONFIGS.items():\n            table.append([\n                model_type.name,\n                config.slideflow_model_name,\n                config.max_batch_size,\n                config.max_tiles_per_bag,\n                f\"{config.min_lr:.0e}-{config.max_lr:.0e}\",\n            ])\n\n        headers = [\n            \"Model Type\", \"Slideflow Name\", \"Max Batch Size\", \n            \"Max Tiles Per Bag\", \"LR Range\"\n        ]\n\n        return tabulate(table, headers=headers, tablefmt=\"fancy_outline\")\n</code></pre>"},{"location":"api/model/#automil.model.ModelManager.model_class","title":"model_class  <code>property</code>","text":"<pre><code>model_class: type[Module]\n</code></pre> <p>Corresponding python class implementing the model.</p> <p>Returns:</p> Type Description <code>type[Module]</code> <p>type[nn.Module]: Model class.</p>"},{"location":"api/model/#automil.model.ModelManager.slideflow_name","title":"slideflow_name  <code>property</code>","text":"<pre><code>slideflow_name: str\n</code></pre> <p>Slideflow-internal identifier for the managed model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Slideflow model name.</p>"},{"location":"api/model/#automil.model.ModelManager.compare_models","title":"compare_models  <code>classmethod</code>","text":"<pre><code>compare_models() -&gt; str\n</code></pre> <p>Generates a comparison table for all available models</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A comparison table as string</p> Source code in <code>automil/model.py</code> <pre><code>@classmethod\ndef compare_models(cls) -&gt; str:\n    \"\"\"Generates a comparison table for all available models\n\n    Returns:\n        str: A comparison table as string\n    \"\"\"\n    from tabulate import tabulate\n\n    table = []\n    for model_type, config in cls._MODEL_CONFIGS.items():\n        table.append([\n            model_type.name,\n            config.slideflow_model_name,\n            config.max_batch_size,\n            config.max_tiles_per_bag,\n            f\"{config.min_lr:.0e}-{config.max_lr:.0e}\",\n        ])\n\n    headers = [\n        \"Model Type\", \"Slideflow Name\", \"Max Batch Size\", \n        \"Max Tiles Per Bag\", \"LR Range\"\n    ]\n\n    return tabulate(table, headers=headers, tablefmt=\"fancy_outline\")\n</code></pre>"},{"location":"api/model/#automil.model.ModelManager.create_dummy_input","title":"create_dummy_input","text":"<pre><code>create_dummy_input(\n    batch_size: int, tiles_per_bag: int, input_dim: int\n) -&gt; tuple\n</code></pre> <p>Creates an appropriate dummy input for the model</p> <p>Dummy input tensors can be used for a variety of tasks. Primarily they are used to perform <code>dry runs</code>, for example to measure the memory reservation of a model instance</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of samples in batch</p> required <code>tiles_per_bag</code> <code>int</code> <p>Number of tiles per bag</p> required <code>input_dim</code> <code>int</code> <p>Feature dimension</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of tensors to pass to model forward()</p> Source code in <code>automil/model.py</code> <pre><code>def create_dummy_input(\n    self, \n    batch_size: int, \n    tiles_per_bag: int, \n    input_dim: int\n) -&gt; tuple:\n    \"\"\"Creates an appropriate dummy input for the model\n\n    Dummy input tensors can be used for a variety of tasks. Primarily they are used\n    to perform `dry runs`, for example to measure the memory reservation of a model instance\n\n    Args:\n        batch_size: Number of samples in batch\n        tiles_per_bag: Number of tiles per bag\n        input_dim: Feature dimension\n\n    Returns:\n        Tuple of tensors to pass to model forward()\n    \"\"\"\n    match self.model_type:\n\n        case ModelType.Attention_MIL:\n            # Both expect a lens tensor in addition to input\n            dummy_input = torch.randn(batch_size, tiles_per_bag, input_dim).cuda()\n            lens = torch.tensor([tiles_per_bag] * batch_size).cuda()\n            return (dummy_input, lens)\n\n        case ModelType.TransMIL | ModelType.BistroTransformer:\n            # BistroTransformer expects only input (no lens)\n            dummy_input = torch.randn(batch_size, tiles_per_bag, input_dim).cuda()\n            return (dummy_input,)\n</code></pre>"},{"location":"api/model/#automil.model.ModelManager.create_model","title":"create_model","text":"<pre><code>create_model(\n    input_dim: int = 1024, num_classes: int = 2, **kwargs\n) -&gt; nn.Module\n</code></pre> <p>Instantiates the model with validated hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Feature dimensions. Defaults to 1024.</p> <code>1024</code> <code>num_classes</code> <code>int</code> <p>Number of classes. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: Instantiated model</p> Source code in <code>automil/model.py</code> <pre><code>def create_model(self, input_dim: int = 1024, num_classes: int = 2, **kwargs) -&gt; nn.Module:\n    \"\"\"Instantiates the model with validated hyperparameters.\n\n    Args:\n        input_dim (int, optional): Feature dimensions. Defaults to 1024.\n        num_classes (int, optional): Number of classes. Defaults to 2.\n\n    Returns:\n        nn.Module: Instantiated model\n    \"\"\"\n    # Map standardized parameter names to model-specific names\n    model_params = {\n        self.config.input_params[\"input_dim\"]: input_dim,\n        self.config.input_params[\"num_classes\"]: num_classes,\n    }\n    # Update with remaining kwargs\n    model_params.update(kwargs)\n\n    # Fallback: If instantiation fails, try with defaults\n    try:\n        return self.model_class(**model_params)\n    except TypeError as e:\n        return self.model_class()\n</code></pre>"},{"location":"api/model/#automil.model.ModelManager.validate_hyperparameters","title":"validate_hyperparameters","text":"<pre><code>validate_hyperparameters(\n    lr: float, batch_size: int, max_tiles_per_bag: int\n) -&gt; dict[str, float | int]\n</code></pre> <p>Validates a set of hyperparameters against model-specific constraints.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>Learning rate.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>max_tiles_per_bag</code> <code>int</code> <p>Maximum tiles per bag.</p> required <p>Returns:</p> Type Description <code>dict[str, float | int]</code> <p>dict[str, float | int]: Suggested parameter adjustments for out-of-range values.</p> Source code in <code>automil/model.py</code> <pre><code>def validate_hyperparameters(self, lr: float, batch_size: int, max_tiles_per_bag: int) -&gt; dict[str, float | int]:\n    \"\"\"\n    Validates a set of hyperparameters against model-specific constraints.\n\n    Args:\n        lr (float):\n            Learning rate.\n        batch_size (int):\n            Batch size.\n        max_tiles_per_bag (int):\n            Maximum tiles per bag.\n\n    Returns:\n        dict[str, float | int]:\n            Suggested parameter adjustments for out-of-range values.\n    \"\"\"\n    suggestions = {}\n\n    # TODO | Better tuning logic / strategy (probably for all but definitely for lr)\n    if not (self.config.min_lr &lt;= lr &lt;= self.config.max_lr):\n        suggestions[\"lr\"] = (self.config.min_lr + self.config.max_lr) / 2\n\n    if batch_size &gt; self.config.max_batch_size:\n        suggestions[\"batch_size\"] = self.config.max_batch_size\n\n    if max_tiles_per_bag &gt; self.config.max_tiles_per_bag:\n        suggestions[\"max_tiles_per_bag\"] = self.config.max_tiles_per_bag\n\n    return suggestions\n</code></pre>"},{"location":"api/model/#helpers","title":"Helpers","text":""},{"location":"api/model/#automil.model.create_model_instance","title":"create_model_instance","text":"<pre><code>create_model_instance(\n    model_type: ModelType, input_dim: int, n_out: int = 2\n) -&gt; nn.Module\n</code></pre> <p>Safely creates a model instance with the correct parameters.</p> <p>This method instantiates a model corresponding to the provided :class:<code>ModelType</code> with the specified input and output dimensions</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>ModelType</code> <p>The ModelType enum</p> required <code>input_dim</code> <code>int</code> <p>Input feature dimension</p> required <code>n_out</code> <code>int</code> <p>Number of output classes</p> <code>2</code> <p>Returns:</p> Type Description <code>Module</code> <p>Instantiated model</p> Source code in <code>automil/model.py</code> <pre><code>def create_model_instance(\n    model_type: ModelType,\n    input_dim: int,\n    n_out: int = 2\n) -&gt; nn.Module:\n    \"\"\"Safely creates a model instance with the correct parameters.\n\n    This method instantiates a model corresponding to the provided\n    :class:`ModelType` with the specified input and output dimensions\n\n    Args:\n        model_type: The ModelType enum\n        input_dim: Input feature dimension\n        n_out: Number of output classes\n\n    Returns:\n        Instantiated model\n    \"\"\"\n    try:\n        match model_type:\n\n            case ModelType.Attention_MIL:\n                model_cls = Attention_MIL\n                return model_cls(n_feats=input_dim, n_out=n_out)\n\n            case ModelType.TransMIL:\n                model_cls = TransMIL\n                return model_cls(n_feats=input_dim, n_out=n_out)\n\n            case ModelType.BistroTransformer:\n                model_cls = BistroTransformer\n                return model_cls(dim=input_dim)\n\n            case _:\n                return model_cls()\n    except Exception as e:\n        slideflow_log.error(f\"Error while creating model instance: {e}\")\n        raise e\n</code></pre>"},{"location":"api/project/","title":"Project","text":"<p><code>automil.project.Project</code> handles the setup of the project directory in which results such as trained models and evaluation reports are stored. It makes sure the directory is created, the annotations file conforms to slideflows expected format and a corresponding slideflow Project instance is created</p>"},{"location":"api/project/#automil.project.Project","title":"Project","text":"<p>Manages the setup of an AutoMIL project.</p> The Project class is responsible for <ul> <li>Modifying the annotation file to conform to the expected slideflow format</li> <li>Creating the project directory structure</li> <li>Creating or loading a Slideflow project instance</li> <li>Exposing project attributes to downstream processes</li> </ul> <p>A Project instance must be prepared before training, evaluation, or prediction can be performed.</p> Source code in <code>automil/project.py</code> <pre><code>class Project:\n    \"\"\"\n    Manages the setup of an AutoMIL project.\n\n    The Project class is responsible for:\n        - Modifying the annotation file to conform to the expected slideflow format\n        - Creating the project directory structure\n        - Creating or loading a Slideflow project instance\n        - Exposing project attributes to downstream processes\n\n    A Project instance must be prepared before training, evaluation,\n    or prediction can be performed.\n    \"\"\"\n\n    def __init__(\n        self,\n        project_dir: Path | str,\n        annotations_file: Path | str,\n        slide_dir: Path | str,\n        patient_column: str,\n        label_column: str,\n        slide_column: str | None = None,\n        transform_labels: bool = False,\n        verbose: bool = True\n    ) -&gt; None:\n        \"\"\"Initializes a Project instance.\n\n        This metod itself does not create or modify files or directories. To prepare a directory to house\n        a project, call :meth:`prepare_project`\n\n        Args:\n            project_dir (Path | str): Directory in which to set up project\n            annotations_file (Path | str): annotations file\n            slide_dir (Path | str): Slide directory\n            patient_column (str): column containing patient identifiers\n            label_column (str): column containing labels\n            slide_column (str | None, optional): column containing slide identifiers. Defaults to None.\n            transform_labels (bool, optional): Whether to transform labels to a float mapping. Defaults to False.\n            verbose (bool, optional): Whether to log verbose messages. Defaults to True.\n        \"\"\"\n        self.project_dir: Path = Path(project_dir)\n        self.annotations_file: Path = Path(annotations_file)\n        self.slide_dir: Path = Path(slide_dir)\n        self.modified_annotations_file: Path = self.project_dir / \"annotations.csv\"\n\n        self.patient_column = patient_column\n        self.label_column = label_column\n        self.slide_column = slide_column\n\n        self.transform_labels = transform_labels\n        self.vlog = get_vlog(verbose)\n\n    # === Properties === #\n    @cached_property\n    def required_columns(self) -&gt; set[str]:\n        \"\"\"\n        Set of required columns expected in the annotation file.\n\n        Includes:\n            - Patient identifier column\n            - Label column\n            - Slide identifier column (if provided)\n\n        Returns:\n            Set of required columns\n        \"\"\"\n        required = {self.patient_column, self.label_column}\n        if self.slide_column:\n            required.add(self.slide_column)\n        return required\n\n    @property\n    def label_map(self) -&gt; dict | list[str]:\n        \"\"\"\n        Mapping between original labels and model-ready labels.\n\n        The mapping is created during project scaffold setup.\n\n        Returns:\n            dict:\n                Mapping from label to float if ``transform_labels=True`` or a list of unique labels otherwise.\n\n        Raises:\n            AttributeError:\n                If the project scaffold has not been set up yet.\n        \"\"\"\n        if not hasattr(self, '_label_map'):\n            raise AttributeError(\n                \"Label map has not been set up yet. Call setup_project_scaffold() first.\"\n            )\n        return self._label_map\n\n    @property\n    def slide_ids(self) -&gt; list[str]:\n        \"\"\"List of unique slide identifiers from the modified annotations file.\n\n        Returns:\n            List of unique slide IDs.\n        \"\"\"\n        if not hasattr(self, 'modified_annotations'):\n            raise AttributeError(\n                \"Modified annotations have not been set up yet. Call setup_project_scaffold() first.\"\n            )\n        return self.modified_annotations[\"slide\"].astype(str).unique().tolist()\n\n    # === Public Methods === #\n    def setup_project_scaffold(self) -&gt; None:\n        \"\"\"\n        Creates the project directory and normalizes annotations.\n\n        This method:\n            - Creates the project directory if it does not exist\n            - Normalizes the annotation file to Slideflow format\n            - Generates and stores the label mapping\n        \"\"\"\n        self._setup_project_folder()\n        self.modified_annotations = self._setup_annotations()\n        self._label_map = self._setup_label_map()\n        self.vlog(f\"[{SUCCESS_CLR}]Project scaffold setup complete[/]\")\n\n    def prepare_project(self) -&gt; sf.Project:\n        \"\"\"\n        Sets up the project directory structure, modifies and stores annotations, and creates or loads\n        a Slideflow project.\n\n        This method:\n            1. Creates the project folder if necessary.\n            2. Normalizes and saves annotations to project_dir/annotations.csv.\n            3. Creates a new Slideflow project or loads an existing one.\n\n        Returns:\n            sf.Project: A slideflow project instance\n        \"\"\"\n        # Setup project folder and annotations\n        self.setup_project_scaffold()\n\n        # Load or create project\n        if is_project(str(self.project_dir)):\n            self.vlog(f\"Loading existing project at [{INFO_CLR}]{self.project_dir}[/]\")\n            self.project = sf.load_project(str(self.project_dir))\n        else:\n            self.vlog(f\"Creating new project at [{INFO_CLR}]{self.project_dir}[/]\")\n            self.project = sf.create_project(\n                name=\"AutoMIL\",\n                root=str(self.project_dir),\n                slides=str(self.slide_dir),\n                annotations=str(self.modified_annotations_file),\n            )\n        return self.project\n\n    def summary(self) -&gt; None:\n        \"\"\"Prints a simple summary of the Project Instance in a tabular format\"\"\"\n        vlog = self.vlog\n        rows = [\n            (\"Project Directory:\", str(self.project_dir)),\n            (\"Slide Directory:\", str(self.slide_dir)),\n            (\"Annotations File:\", str(self.annotations_file)),\n            (\"Patient Column:\", self.patient_column),\n            (\"Label Column:\", self.label_column),\n            (\"Slide Column:\", self.slide_column or \"None (using patient ID)\"),\n            (\"Transform Labels:\", str(self.transform_labels)),\n            (\"Modified Annotations:\", str(self.modified_annotations_file) or \"Not yet created\"),\n            (\"Slideflow Project:\", \"Loaded\" if self.project else \"Not initialized\"),\n        ]\n\n\n        vlog(\"[bold underline]Project Summary[/]\")\n        vlog(render_kv_table(rows, width=256))\n\n    # === Internals === #\n    def _setup_project_folder(self) -&gt; None:\n        \"\"\"\n        Ensures the project directory exists.\n\n        Creates the directory and parent directories if necessary.\n        \"\"\"\n        if not self.project_dir.exists():\n            self.project_dir.mkdir(parents=True, exist_ok=True)\n            self.vlog(f\"Created project directory at [{INFO_CLR}]{self.project_dir}[/]\")\n        else:\n            self.vlog(f\"Project directory [{INFO_CLR}]{self.project_dir}[/] already exists\")\n\n    def _setup_annotations(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Normalizes the input annotations file to the required format and set up label map.\n\n        This includes:\n            - Validating the presence of required columns.\n            - Renaming the patient and label columns to `patient` and `label`.\n            - Creating or renaming the `slide` column.\n            - Optionally transforming labels to float encodings.\n            - Creating and storing the label map for later use.\n            - Saving the normalized file to project_dir/annotations.csv.\n\n        AutoMIL requires the annotations file to have the following columns:\n            - patient | contains patient identifiers\n            - slide   | contains slide identifiers\n            - label   | contains labels\n\n        Raises:\n            ValueError:\n                If required columns are missing.\n            IOError:\n                If the output annotations file cannot be written.\n        \"\"\"\n        # Make sure given columns exist\n        if (missing := contains_columns(self.annotations_file, self.required_columns, return_missing=True)):\n            raise ValueError(f\"Annotations file is missing required columns: {missing}\")\n\n        # Load annotations\n        annotations = pd.read_csv(self.annotations_file, index_col=self.patient_column)\n        annotations.index.name = \"patient\"\n\n        # Renaming the slide column if provided, otherwise just use the patient column as slide identifier\n        if not self.slide_column:\n            annotations[\"slide\"] = annotations.index\n        else:\n            annotations.rename(columns={self.slide_column: \"slide\"}, inplace=True)\n        # Rename label column\n        annotations.rename(columns={self.label_column: \"label\"}, inplace=True)\n\n        # Save modified annotations\n        out_path = self.modified_annotations_file\n        annotations.to_csv(out_path, index=True)\n\n        if not out_path.exists():\n            raise IOError(f\"Failed to write annotations file: {out_path}\")\n\n        if annotations.empty:\n            self.vlog(\"Warning: annotation file written but is empty.\")\n\n        self.vlog(f\"Annotations saved to [{INFO_CLR}]{out_path}[/]\")\n        return annotations\n\n    def _setup_label_map(self) -&gt; dict | list[str]:\n        \"\"\"Sets up the label map based on the modified annotations file.\n\n        Returns:\n            dict | list[str]: The label map (dict if transform_labels=True, else list of unique labels).\n        \"\"\"\n        annotations = self.modified_annotations\n        labels = annotations[\"label\"].unique()\n\n        # Transform labels to float values and store the mapping\n        if self.transform_labels:\n            label_map = {label: float(i) for i, label in enumerate(sorted(labels))}\n            pretty = \", \".join(f\"{k}: {v}\" for k, v in label_map.items())\n            self.vlog(f\"Transformed labels to float values: [{INFO_CLR}]{pretty}[/]\")\n        else:\n            # Store unique labels as sorted list\n            label_map = sorted(labels.astype(str).tolist())\n\n        return label_map\n</code></pre>"},{"location":"api/project/#automil.project.Project.label_map","title":"label_map  <code>property</code>","text":"<pre><code>label_map: dict | list[str]\n</code></pre> <p>Mapping between original labels and model-ready labels.</p> <p>The mapping is created during project scaffold setup.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict | list[str]</code> <p>Mapping from label to float if <code>transform_labels=True</code> or a list of unique labels otherwise.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the project scaffold has not been set up yet.</p>"},{"location":"api/project/#automil.project.Project.required_columns","title":"required_columns  <code>cached</code> <code>property</code>","text":"<pre><code>required_columns: set[str]\n</code></pre> <p>Set of required columns expected in the annotation file.</p> Includes <ul> <li>Patient identifier column</li> <li>Label column</li> <li>Slide identifier column (if provided)</li> </ul> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of required columns</p>"},{"location":"api/project/#automil.project.Project.slide_ids","title":"slide_ids  <code>property</code>","text":"<pre><code>slide_ids: list[str]\n</code></pre> <p>List of unique slide identifiers from the modified annotations file.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of unique slide IDs.</p>"},{"location":"api/project/#automil.project.Project.prepare_project","title":"prepare_project","text":"<pre><code>prepare_project() -&gt; sf.Project\n</code></pre> <p>Sets up the project directory structure, modifies and stores annotations, and creates or loads a Slideflow project.</p> This method <ol> <li>Creates the project folder if necessary.</li> <li>Normalizes and saves annotations to project_dir/annotations.csv.</li> <li>Creates a new Slideflow project or loads an existing one.</li> </ol> <p>Returns:</p> Type Description <code>Project</code> <p>sf.Project: A slideflow project instance</p> Source code in <code>automil/project.py</code> <pre><code>def prepare_project(self) -&gt; sf.Project:\n    \"\"\"\n    Sets up the project directory structure, modifies and stores annotations, and creates or loads\n    a Slideflow project.\n\n    This method:\n        1. Creates the project folder if necessary.\n        2. Normalizes and saves annotations to project_dir/annotations.csv.\n        3. Creates a new Slideflow project or loads an existing one.\n\n    Returns:\n        sf.Project: A slideflow project instance\n    \"\"\"\n    # Setup project folder and annotations\n    self.setup_project_scaffold()\n\n    # Load or create project\n    if is_project(str(self.project_dir)):\n        self.vlog(f\"Loading existing project at [{INFO_CLR}]{self.project_dir}[/]\")\n        self.project = sf.load_project(str(self.project_dir))\n    else:\n        self.vlog(f\"Creating new project at [{INFO_CLR}]{self.project_dir}[/]\")\n        self.project = sf.create_project(\n            name=\"AutoMIL\",\n            root=str(self.project_dir),\n            slides=str(self.slide_dir),\n            annotations=str(self.modified_annotations_file),\n        )\n    return self.project\n</code></pre>"},{"location":"api/project/#automil.project.Project.setup_project_scaffold","title":"setup_project_scaffold","text":"<pre><code>setup_project_scaffold() -&gt; None\n</code></pre> <p>Creates the project directory and normalizes annotations.</p> This method <ul> <li>Creates the project directory if it does not exist</li> <li>Normalizes the annotation file to Slideflow format</li> <li>Generates and stores the label mapping</li> </ul> Source code in <code>automil/project.py</code> <pre><code>def setup_project_scaffold(self) -&gt; None:\n    \"\"\"\n    Creates the project directory and normalizes annotations.\n\n    This method:\n        - Creates the project directory if it does not exist\n        - Normalizes the annotation file to Slideflow format\n        - Generates and stores the label mapping\n    \"\"\"\n    self._setup_project_folder()\n    self.modified_annotations = self._setup_annotations()\n    self._label_map = self._setup_label_map()\n    self.vlog(f\"[{SUCCESS_CLR}]Project scaffold setup complete[/]\")\n</code></pre>"},{"location":"api/project/#automil.project.Project.summary","title":"summary","text":"<pre><code>summary() -&gt; None\n</code></pre> <p>Prints a simple summary of the Project Instance in a tabular format</p> Source code in <code>automil/project.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Prints a simple summary of the Project Instance in a tabular format\"\"\"\n    vlog = self.vlog\n    rows = [\n        (\"Project Directory:\", str(self.project_dir)),\n        (\"Slide Directory:\", str(self.slide_dir)),\n        (\"Annotations File:\", str(self.annotations_file)),\n        (\"Patient Column:\", self.patient_column),\n        (\"Label Column:\", self.label_column),\n        (\"Slide Column:\", self.slide_column or \"None (using patient ID)\"),\n        (\"Transform Labels:\", str(self.transform_labels)),\n        (\"Modified Annotations:\", str(self.modified_annotations_file) or \"Not yet created\"),\n        (\"Slideflow Project:\", \"Loaded\" if self.project else \"Not initialized\"),\n    ]\n\n\n    vlog(\"[bold underline]Project Summary[/]\")\n    vlog(render_kv_table(rows, width=256))\n</code></pre>"},{"location":"api/trainer/","title":"Trainer","text":"<p><code>automil.trainer.Trainer</code> orchestrates training MIL models using Slideflow and FastAI. It provides automatic batch size adjustment based on GPU memory constraints, optional early stopping and k-fold cross-validation with the ability of providing additional callback.</p>"},{"location":"api/trainer/#automil.trainer.Trainer","title":"Trainer","text":"<p>Orchestrates MIL model training and evaluation.</p> Trainer wraps the entire training workflow and provides a couple of related functionalities <ul> <li>Automatic batch size optimization based on VRAM usage</li> <li>Early stopping (via FastAI callback <code>EarlyStoppingCallback</code>)</li> <li>Optional k-fold cross-validation</li> </ul> Source code in <code>automil/trainer.py</code> <pre><code>class Trainer:\n    \"\"\"\n    Orchestrates MIL model training and evaluation.\n\n    Trainer wraps the entire training workflow and provides a couple of related functionalities:\n        - Automatic batch size optimization based on VRAM usage\n        - Early stopping (via FastAI callback `EarlyStoppingCallback`)\n        - Optional k-fold cross-validation\n    \"\"\"\n\n    def __init__(\n        self,\n        bags_path: Path,\n        project: sf.Project,\n        train_dataset: sf.Dataset,\n        val_dataset: sf.Dataset,\n        model: ModelType,\n        model_outdir: Path | None = None,\n        lr: float = LEARNING_RATE,\n        epochs: int = EPOCHS,\n        batch_size: int = BATCH_SIZE,\n        k: int = 3,\n        enable_early_stopping: bool = True,\n        early_stop_patience: int = 10,\n        early_stop_monitor: str = \"valid_loss\",\n        attention_heatmaps: bool = True,\n        additional_callbacks: list[Callback] | None = None,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize a Trainer Instance. Sets up training configuration and optimizes hyperparameters.\n\n        Args:\n            bags_path (Path): Path to feature bags directory\n            project (sf.Project): Slideflow project instance\n            train_dataset (sf.Dataset): Training dataset\n            val_dataset (sf.Dataset): Validation dataset\n            model (ModelType): Model to use\n            model_outdir (Path | None, optional): Output directory for trained models. If none, will use `project_root / models`. Defaults to None.\n            lr (float, optional): Learning rate. Defaults to LEARNING_RATE.\n            epochs (int, optional): (Maximum number of) Epochs to train for. Defaults to EPOCHS.\n            batch_size (int, optional): Batch size. Defaults to BATCH_SIZE.\n            k (int, optional): Number of folds to train. Defaults to 3.\n            enable_early_stopping (bool, optional): Whether to use early stopping. Adds an ealry stopping callback to the FastAI Learner. Defaults to True.\n            early_stop_patience (int, optional): Number of epochs without performance improvement before early stopping kicks in. Defaults to 10.\n            early_stop_monitor (str, optional): Metric to monitor for early stopping. Defaults to \"valid_loss\".\n            attention_heatmaps (bool, optional): Whether to generate attention heatmaps. Defaults to True.\n            verbose (bool, optional): Whether to print verbose messages. Defaults to True.\n        \"\"\"\n        self.bags_path = bags_path\n        self.project = project\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        self.model = model\n        self.model_outdir = model_outdir or Path(self.project.root) / \"models\"\n        self.lr = lr\n        self.epochs = epochs\n        self.initial_batch_size = batch_size\n        self.attention_heatmaps = attention_heatmaps\n        self.additional_callbacks = additional_callbacks\n        self.k = k\n        self.enable_early_stopping = enable_early_stopping\n        self.early_stop_patience = early_stop_patience\n        self.early_stop_monitor = early_stop_monitor\n\n        self.vlog = get_vlog(verbose)\n\n        # Hyperparameter validation\n        self.model_manager = ModelManager(self.model)\n        suggestions = self.model_manager.validate_hyperparameters(\n            self.lr,\n            self.initial_batch_size,\n            self.bag_avg\n        )\n\n        for suggestion, value in suggestions.items():\n            self.vlog(\n                f\"[yellow]Warning:[/] {suggestion} value out of bounds for model \"\n                f\"[cyan]{self.model_manager.model_class.__name__}[/]. \"\n                f\"Suggested value: [cyan]{value}[/]\"\n            )\n            setattr(self, suggestion, value)\n\n    @cached_property\n    def num_classes(self) -&gt; int:\n        \"\"\"Number of target classes derived from dataset annotations\"\"\"\n        if self.train_dataset.annotations is not None:\n            return self.train_dataset.annotations[\"label\"].nunique()\n        elif self.val_dataset.annotations is not None:\n            return self.val_dataset.annotations[\"label\"].nunique()\n        else:\n            return 2  # Assume binary classification as fallback\n\n    @cached_property\n    def num_slides(self) -&gt; int:\n        \"\"\"Number of slides in training and validation dataset\"\"\"\n        return get_num_slides(self.train_dataset) + get_num_slides(self.val_dataset)\n\n    @cached_property\n    def bag_avg(self) -&gt; int:\n        \"\"\"Average number of tiles per bag\"\"\"\n        return get_bag_avg_and_num_features(self.bags_path)[0]\n\n    @cached_property\n    def num_features(self) -&gt; int:\n        \"\"\"Average number of features per tile\"\"\"\n        return get_bag_avg_and_num_features(self.bags_path)[1]\n\n    @cached_property\n    def adjusted_batch_size(self) -&gt; int:\n        \"\"\"Optimal batch size adjusted for VRAM constraints\"\"\"\n        return self._compute_optimal_batch_size()\n\n    @cached_property\n    def estimated_size_mb(self) -&gt; float:\n        \"\"\"Estimated model size in MB\"\"\"\n        return self._estimate_model_size()\n\n    @cached_property\n    def config(self) -&gt; TrainerConfig:\n        \"\"\"FastAI trainer configuration\"\"\"\n        return self._build_config()\n\n    @cached_property\n    def device(self) -&gt; torch.device:\n        \"\"\"The device to use for training\"\"\"\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    @cached_property\n    def callbacks(self) -&gt; list[Callback]:\n        \"\"\"List of FastAI Callbacks to use during training\"\"\"\n        return self._setup_callbacks(self.additional_callbacks)\n\n    # === Public Methods === #\n    def train(\n        self,\n        model_label_override: str | None = None,\n    ) -&gt; Learner:\n        \"\"\"\n        Trains a single MIL model.\n\n        This method performs model training and validation.\n\n        ??? Note\n            This method closely mirrors Slideflow's internal training workflow (see `slideflow.mil._train_mil_mode`),\n            but extends the routine with additional functionalities such as passing callbacks to enable early stopping,\n            more type safety checks and additional logging, amongst others\n\n        Args:\n            model_label_override (str | None, optional):\n                Custom directory name for the trained model. If ``None``, Slideflow's\n                default naming scheme is used.\n\n        Returns:\n            Learner:\n                Trained FastAI learner instance.\n        \"\"\"\n        # Determine output directory\n        if model_label_override:\n            outdir = self.model_outdir / model_label_override\n        else:\n            outdir = Path(self.config.prepare_training(\"label\", exp_label=None, outdir=str(self.model_outdir)))\n        self.vlog(f\"Output directory: [{INFO_CLR}]{outdir}[/]\")\n\n        # Prepare validation bags\n        val_bags = self._prepare_validation_bags()\n\n        # Build learner with shape information\n        result = build_fastai_learner(\n            self.config,\n            self.train_dataset,\n            self.val_dataset,\n            outcomes=\"label\",\n            bags=str(self.bags_path),\n            outdir=str(outdir),\n            device=self.device,\n            return_shape=True\n        )\n\n        # with return_shape=True, result is a tuple\n        if isinstance(result, tuple):\n            learner, (n_in, n_out) = result\n        else:\n            learner = result\n            n_in, n_out = 0, 0  # Shape info not available\n\n        # Save MIL parameters\n        self._log_mil_params(\"label\", learner, n_in, n_out, str(outdir))\n\n        # Add custom callbacks if needed\n        callbacks = self._setup_callbacks()\n        for callback in callbacks:\n            learner.add_cb(callback)\n\n        # Train the model using fastai\n        self.vlog(\n            f\"Starting training: {self.model.model_name} \"\n            f\"(epochs={self.epochs}, batch_size={self.adjusted_batch_size})\"\n        )\n        _fastai.train(learner, self.config)\n\n        # Generate validation predictions with attention\n        self.vlog(\"Generating validation predictions...\")\n        from slideflow.mil import predict_mil\n        df, attention = predict_mil(\n            learner.model,\n            dataset=self.val_dataset,\n            config=self.config,\n            outcomes=\"label\",\n            bags=val_bags,\n            attention=True\n        )\n\n        # Saving predictions, calculating metrics, exporting attention, and generating heatmaps\n        # Really only feasible if we get a sensible return dataframe from `predict_mil`\n        if isinstance(df, pd.DataFrame):\n            # Save predictions\n            pred_out = outdir / 'predictions.parquet'\n            df.to_parquet(pred_out)\n            self.vlog(f\"Predictions saved to [{INFO_CLR}]{pred_out}[/]\")\n\n            # Calculate and display metrics\n            self._run_metrics(df, \"label\", str(outdir))\n\n            # Export attention arrays\n            if attention and isinstance(attention, dict):\n                self._export_attention(attention, val_bags, str(outdir))\n\n            # Generate attention heatmaps\n            if attention and isinstance(attention, dict) and self.attention_heatmaps:\n                self._generate_heatmaps(val_bags, attention, str(outdir))\n        else:\n            self.vlog(\"Unable to generate predictions; skipping metrics and attention export.\")\n\n        # Get actual memory usage during inference\n        dummy_input = self.model_manager.create_dummy_input(\n            self.adjusted_batch_size,\n            self.bag_avg,\n            self.num_features\n        )\n\n        torch.cuda.reset_peak_memory_stats()\n        with torch.no_grad():\n            _ = learner.model(*dummy_input)\n        self.actual_mem_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n\n        self.vlog(f\"Training completed: [{INFO_CLR}]{self.model.model_name}[/]\")\n        return learner\n\n    def train_k_fold(self, base_model_label_override: str | None = None) -&gt; list[Learner]:\n        \"\"\"\n        Performs k-fold cross-validation training.\n\n        Trains ``k`` independent models and stores them in separate subdirectories.\n\n        Args:\n            base_model_label_override (str | None, optional):\n                Base directory name for k-fold outputs.\n\n        Returns:\n            list[Learner]:\n                Trained learners, one per fold.\n        \"\"\"\n        outdir = self.model_outdir\n        if base_model_label_override:\n            outdir = outdir / base_model_label_override\n        self.vlog(f\"K-Fold output directory: [{INFO_CLR}]{outdir}[/]\")\n\n        learners = []\n        for fold in range(self.k):\n            self.vlog(f\"=\" * 50)\n            self.vlog(f\"Training fold [{INFO_CLR}]{fold + 1}[/]/[{INFO_CLR}]{self.k}[/]\")\n            self.vlog(f\"=\" * 50)\n\n            # Create fold-specific paths and labels\n            if base_model_label_override:\n                fold_label = f\"{base_model_label_override}_fold{fold}\"\n            else:\n                fold_label = None\n\n            # Train this fold\n            learner = self.train(\n                model_label_override=fold_label\n            )\n            learners.append(learner)\n\n        self.vlog(f\"Completed [{INFO_CLR}]{self.k}[/]-fold training\")\n        return learners\n\n    def summary(self) -&gt; None:\n        \"\"\"Print a summary of the trainer configuration\"\"\"\n        # Safe way to handle actual memory attribute\n        # Since it may not have been set yet (Only after training)\n        actual_mem_mb = getattr(self, 'actual_mem_mb', None)\n        if actual_mem_mb is not None:\n            actual_mem_str = f\"{actual_mem_mb:.2f} MB\"\n        else:\n            actual_mem_str = \"N/A\"\n\n        rows = [\n            (\"Model Type\", self.model.model_name),\n            (\"Learning Rate\", f\"{self.lr:.0e}\"),\n            (\"Epochs\", self.epochs),\n            (\"Initial Batch Size\", self.initial_batch_size),\n            (\"Adjusted Batch Size\", self.adjusted_batch_size),\n            (\"Estimated Model Size\", f\"{self.estimated_size_mb:.2f} MB\"),\n            (\"Actual Inference Memory\", actual_mem_str),\n            (\"Number of Slides\", self.num_slides),\n            (\"Average Bag Size\", self.bag_avg),\n            (\"Feature Dimensions\", self.num_features),\n            (\"K-Fold\", self.k),\n            (\"Early Stopping\", self.enable_early_stopping),\n            (\"Attention Heatmaps\", self.attention_heatmaps),\n            (\"Device\", str(self.device)),\n        ]\n\n        self.vlog(\"[bold underline]Trainer Summary:[/]\")\n        self.vlog(render_kv_table(rows))\n\n    # === Internals === #\n    def _debug_dataset_labels(self) -&gt; None:\n        \"\"\"Debug helper to inspect dataset labels\"\"\"\n        train_ann = self.train_dataset.annotations\n        val_ann = self.val_dataset.annotations\n\n        if train_ann is not None and val_ann is not None:\n            self.vlog(f\"Train labels: [{INFO_CLR}]{train_ann['label'].unique()}[/]\")\n            self.vlog(f\"Train label types: [{INFO_CLR}]{[type(x) for x in train_ann['label'].unique()]}[/]\")\n            self.vlog(f\"Val labels: [{INFO_CLR}]{val_ann['label'].unique()}[/]\")  \n            self.vlog(f\"Val label types: [{INFO_CLR}]{[type(x) for x in val_ann['label'].unique()]}[/]\")\n        else:\n            self.vlog(\"WARNING: One or both datasets have no annotations\")\n\n\n    def _prepare_validation_bags(self) -&gt; list:\n        \"\"\"Simple helper method that emulates how slideflow generates validation feature bags.\n\n        Note:\n            See `slideflow.mil._train_mil` for reference.\n        Returns:\n            list: list of validation bag paths\n        \"\"\"\n        val_bags = self.val_dataset.get_bags(str(self.bags_path))\n        self.vlog(f\"Found [{INFO_CLR}]{len(val_bags)}[/] validation bags\")\n        return val_bags.tolist()\n\n    def _log_mil_params(\n        self,\n        outcomes: str,\n        learner: Learner,\n        n_in: int,\n        n_out: int,\n        outdir: str\n    ) -&gt; None:\n        \"\"\"Simple helper method that emulates how slideflow logs and saves MIL parameters\n\n        Note:\n            See `slideflow.mil._train_mil` for reference.\n        Args:\n            outcomes (str): Label column name\n            learner (Learner): FastAI Learner object\n            n_in (int): input feature dimensions\n            n_out (int): output dimensions / number of classes\n            outdir (str): Output directory\n        \"\"\"\n        # Attempt to read unique categories from learner\n        if hasattr(learner.dls.train_ds, 'encoder'):\n            encoder = learner.dls.train_ds.encoder\n            if encoder is not None:\n                unique = encoder.categories_[0].tolist()\n            else:\n                unique = None\n        else:\n            unique = None\n\n        # Use Slideflow's internal logging function\n        _log_mil_params(self.config, outcomes, unique, str(self.bags_path), n_in, n_out, outdir)\n\n    def _run_metrics(self, df: pd.DataFrame, outcomes: str, outdir: str) -&gt; None:\n        \"\"\"Simple helper method that emulates how slideflow caculates and logs metrics.\n\n        Note:\n            See `slideflow.mil._train_mil` for reference.\n\n        Args:\n            df (pd.DataFrame): DataFrame containing predictions\n            outcomes (str): Label column name\n            outdir (str): Output directory\n        \"\"\"\n        # Rename columns for metrics calculation\n        utils.rename_df_cols(df, outcomes, categorical=self.config.is_classification(), inplace=True)\n\n        # Run metrics using Slideflow's method\n        self.config.run_metrics(df, level='slide', outdir=outdir)\n\n    def _export_attention(self, attention: dict, val_bags: list, outdir: str) -&gt; None:\n        \"\"\"Simple helper method that emulates how slideflow exports attention arrays.\n\n        Note:\n            See `slideflow.mil._train_mil` for reference.\n        Args:\n            attention (dict): Dictionary mapping bag paths to attention arrays\n            val_bags (list): List of validation bag paths\n            outdir (str): Output directory\n        \"\"\"\n        attention_dir = join(outdir, 'attention')\n        bag_names = [path_to_name(b) for b in val_bags]\n\n        # Convert attention dict to list of arrays\n        attention_arrays = list(attention.values())\n        utils._export_attention(attention_dir, attention_arrays, bag_names)\n        self.vlog(f\"Attention arrays exported to [{INFO_CLR}]{attention_dir}[/]\")\n\n    def _generate_heatmaps(self, val_bags: list, attention: dict, outdir: str) -&gt; None:\n        \"\"\"Generate a heatmap using slideflow\n\n        Args:\n            val_bags (list): List of validation bag paths\n            attention (dict): Dictionary mapping bag paths to attention arrays\n            outdir (str): Output directory\n        \"\"\"\n        heatmap_dir = join(outdir, 'heatmaps')\n        generate_attention_heatmaps(\n            outdir=heatmap_dir,\n            dataset=self.val_dataset,\n            bags=val_bags,\n            attention=list(attention.values()),\n        )\n        self.vlog(f\"Attention heatmaps generated in [{INFO_CLR}]{heatmap_dir}[/]\")\n\n    def _compute_optimal_batch_size(self) -&gt; int:\n        \"\"\"Compute VRAM-Optimal batch size based on estimated model memory usage and free memory\n\n        Returns:\n            int: adjusted batch size\n        \"\"\"\n        adjusted_batch_size = adjust_batch_size(\n            self.model,\n            self.initial_batch_size,\n            self.num_slides,\n            self.num_features,\n            self.bag_avg,\n        )\n\n        # Ensure we do not exceed model-specific maximum batch size\n        adjusted_batch_size = min(\n            self.model_manager.config.max_batch_size,\n            adjusted_batch_size\n        )\n\n        self.vlog(\n            f\"Adjusted batch size to [{INFO_CLR}]{adjusted_batch_size}[/] \"\n            f\"(tiles/bag={self.bag_avg}, dim={self.num_features})\"\n        )\n\n        return adjusted_batch_size\n\n    def _estimate_model_size(self) -&gt; float:\n        \"\"\"Estimate model size (reserved memory) in MB\n\n        Returns:\n            float: Estimated model size in MB\n        \"\"\"\n        return estimate_model_size(\n            model_type=self.model,\n            batch_size=self.adjusted_batch_size,\n            bag_size=self.bag_avg,\n            input_dim=self.num_features,\n            num_classes=self.num_classes\n        )\n\n    def _build_config(self) -&gt; TrainerConfig:\n        \"\"\"Builds a MIL model configuration using slideflow's `mil_config` method\n\n        Returns:\n            TrainerConfig: TrainerConfig object\n        \"\"\"\n        cfg = mil_config(\n            model=self.model.model_name,\n            trainer=\"fastai\",\n            lr=self.lr,\n            epochs=self.epochs,\n            batch_size=self.adjusted_batch_size,\n        )\n\n        # Casting because mil_config should always return a TrainerConfig\n        return cast(TrainerConfig, cfg)\n\n    def _setup_callbacks(self, additional_callbacks: list[Callback] | None = None) -&gt; list[Callback]:\n        \"\"\"Sets up callbacks for the FastAI Learner, including an EarlyStopping Callback\n\n        Args:\n            additional_callbacks (list[Callback]): A list of additional callbacks to include\n\n        Returns:\n            list: List of callbacks\n        \"\"\"\n        callbacks = []\n\n        # Early stopping\n        if self.enable_early_stopping:\n            callbacks.append(\n                EarlyStoppingCallback(\n                    monitor=self.early_stop_monitor,\n                    patience=self.early_stop_patience,\n                )\n            )\n        callbacks.extend(additional_callbacks if additional_callbacks else [])\n\n        return callbacks\n</code></pre>"},{"location":"api/trainer/#automil.trainer.Trainer.adjusted_batch_size","title":"adjusted_batch_size  <code>cached</code> <code>property</code>","text":"<pre><code>adjusted_batch_size: int\n</code></pre> <p>Optimal batch size adjusted for VRAM constraints</p>"},{"location":"api/trainer/#automil.trainer.Trainer.bag_avg","title":"bag_avg  <code>cached</code> <code>property</code>","text":"<pre><code>bag_avg: int\n</code></pre> <p>Average number of tiles per bag</p>"},{"location":"api/trainer/#automil.trainer.Trainer.callbacks","title":"callbacks  <code>cached</code> <code>property</code>","text":"<pre><code>callbacks: list[Callback]\n</code></pre> <p>List of FastAI Callbacks to use during training</p>"},{"location":"api/trainer/#automil.trainer.Trainer.config","title":"config  <code>cached</code> <code>property</code>","text":"<pre><code>config: TrainerConfig\n</code></pre> <p>FastAI trainer configuration</p>"},{"location":"api/trainer/#automil.trainer.Trainer.device","title":"device  <code>cached</code> <code>property</code>","text":"<pre><code>device: device\n</code></pre> <p>The device to use for training</p>"},{"location":"api/trainer/#automil.trainer.Trainer.estimated_size_mb","title":"estimated_size_mb  <code>cached</code> <code>property</code>","text":"<pre><code>estimated_size_mb: float\n</code></pre> <p>Estimated model size in MB</p>"},{"location":"api/trainer/#automil.trainer.Trainer.num_classes","title":"num_classes  <code>cached</code> <code>property</code>","text":"<pre><code>num_classes: int\n</code></pre> <p>Number of target classes derived from dataset annotations</p>"},{"location":"api/trainer/#automil.trainer.Trainer.num_features","title":"num_features  <code>cached</code> <code>property</code>","text":"<pre><code>num_features: int\n</code></pre> <p>Average number of features per tile</p>"},{"location":"api/trainer/#automil.trainer.Trainer.num_slides","title":"num_slides  <code>cached</code> <code>property</code>","text":"<pre><code>num_slides: int\n</code></pre> <p>Number of slides in training and validation dataset</p>"},{"location":"api/trainer/#automil.trainer.Trainer.summary","title":"summary","text":"<pre><code>summary() -&gt; None\n</code></pre> <p>Print a summary of the trainer configuration</p> Source code in <code>automil/trainer.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Print a summary of the trainer configuration\"\"\"\n    # Safe way to handle actual memory attribute\n    # Since it may not have been set yet (Only after training)\n    actual_mem_mb = getattr(self, 'actual_mem_mb', None)\n    if actual_mem_mb is not None:\n        actual_mem_str = f\"{actual_mem_mb:.2f} MB\"\n    else:\n        actual_mem_str = \"N/A\"\n\n    rows = [\n        (\"Model Type\", self.model.model_name),\n        (\"Learning Rate\", f\"{self.lr:.0e}\"),\n        (\"Epochs\", self.epochs),\n        (\"Initial Batch Size\", self.initial_batch_size),\n        (\"Adjusted Batch Size\", self.adjusted_batch_size),\n        (\"Estimated Model Size\", f\"{self.estimated_size_mb:.2f} MB\"),\n        (\"Actual Inference Memory\", actual_mem_str),\n        (\"Number of Slides\", self.num_slides),\n        (\"Average Bag Size\", self.bag_avg),\n        (\"Feature Dimensions\", self.num_features),\n        (\"K-Fold\", self.k),\n        (\"Early Stopping\", self.enable_early_stopping),\n        (\"Attention Heatmaps\", self.attention_heatmaps),\n        (\"Device\", str(self.device)),\n    ]\n\n    self.vlog(\"[bold underline]Trainer Summary:[/]\")\n    self.vlog(render_kv_table(rows))\n</code></pre>"},{"location":"api/trainer/#automil.trainer.Trainer.train","title":"train","text":"<pre><code>train(model_label_override: str | None = None) -&gt; Learner\n</code></pre> <p>Trains a single MIL model.</p> <p>This method performs model training and validation.</p> Note <p>This method closely mirrors Slideflow's internal training workflow (see <code>slideflow.mil._train_mil_mode</code>), but extends the routine with additional functionalities such as passing callbacks to enable early stopping, more type safety checks and additional logging, amongst others</p> <p>Parameters:</p> Name Type Description Default <code>model_label_override</code> <code>str | None</code> <p>Custom directory name for the trained model. If <code>None</code>, Slideflow's default naming scheme is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Learner</code> <code>Learner</code> <p>Trained FastAI learner instance.</p> Source code in <code>automil/trainer.py</code> <pre><code>def train(\n    self,\n    model_label_override: str | None = None,\n) -&gt; Learner:\n    \"\"\"\n    Trains a single MIL model.\n\n    This method performs model training and validation.\n\n    ??? Note\n        This method closely mirrors Slideflow's internal training workflow (see `slideflow.mil._train_mil_mode`),\n        but extends the routine with additional functionalities such as passing callbacks to enable early stopping,\n        more type safety checks and additional logging, amongst others\n\n    Args:\n        model_label_override (str | None, optional):\n            Custom directory name for the trained model. If ``None``, Slideflow's\n            default naming scheme is used.\n\n    Returns:\n        Learner:\n            Trained FastAI learner instance.\n    \"\"\"\n    # Determine output directory\n    if model_label_override:\n        outdir = self.model_outdir / model_label_override\n    else:\n        outdir = Path(self.config.prepare_training(\"label\", exp_label=None, outdir=str(self.model_outdir)))\n    self.vlog(f\"Output directory: [{INFO_CLR}]{outdir}[/]\")\n\n    # Prepare validation bags\n    val_bags = self._prepare_validation_bags()\n\n    # Build learner with shape information\n    result = build_fastai_learner(\n        self.config,\n        self.train_dataset,\n        self.val_dataset,\n        outcomes=\"label\",\n        bags=str(self.bags_path),\n        outdir=str(outdir),\n        device=self.device,\n        return_shape=True\n    )\n\n    # with return_shape=True, result is a tuple\n    if isinstance(result, tuple):\n        learner, (n_in, n_out) = result\n    else:\n        learner = result\n        n_in, n_out = 0, 0  # Shape info not available\n\n    # Save MIL parameters\n    self._log_mil_params(\"label\", learner, n_in, n_out, str(outdir))\n\n    # Add custom callbacks if needed\n    callbacks = self._setup_callbacks()\n    for callback in callbacks:\n        learner.add_cb(callback)\n\n    # Train the model using fastai\n    self.vlog(\n        f\"Starting training: {self.model.model_name} \"\n        f\"(epochs={self.epochs}, batch_size={self.adjusted_batch_size})\"\n    )\n    _fastai.train(learner, self.config)\n\n    # Generate validation predictions with attention\n    self.vlog(\"Generating validation predictions...\")\n    from slideflow.mil import predict_mil\n    df, attention = predict_mil(\n        learner.model,\n        dataset=self.val_dataset,\n        config=self.config,\n        outcomes=\"label\",\n        bags=val_bags,\n        attention=True\n    )\n\n    # Saving predictions, calculating metrics, exporting attention, and generating heatmaps\n    # Really only feasible if we get a sensible return dataframe from `predict_mil`\n    if isinstance(df, pd.DataFrame):\n        # Save predictions\n        pred_out = outdir / 'predictions.parquet'\n        df.to_parquet(pred_out)\n        self.vlog(f\"Predictions saved to [{INFO_CLR}]{pred_out}[/]\")\n\n        # Calculate and display metrics\n        self._run_metrics(df, \"label\", str(outdir))\n\n        # Export attention arrays\n        if attention and isinstance(attention, dict):\n            self._export_attention(attention, val_bags, str(outdir))\n\n        # Generate attention heatmaps\n        if attention and isinstance(attention, dict) and self.attention_heatmaps:\n            self._generate_heatmaps(val_bags, attention, str(outdir))\n    else:\n        self.vlog(\"Unable to generate predictions; skipping metrics and attention export.\")\n\n    # Get actual memory usage during inference\n    dummy_input = self.model_manager.create_dummy_input(\n        self.adjusted_batch_size,\n        self.bag_avg,\n        self.num_features\n    )\n\n    torch.cuda.reset_peak_memory_stats()\n    with torch.no_grad():\n        _ = learner.model(*dummy_input)\n    self.actual_mem_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n\n    self.vlog(f\"Training completed: [{INFO_CLR}]{self.model.model_name}[/]\")\n    return learner\n</code></pre>"},{"location":"api/trainer/#automil.trainer.Trainer.train_k_fold","title":"train_k_fold","text":"<pre><code>train_k_fold(\n    base_model_label_override: str | None = None,\n) -&gt; list[Learner]\n</code></pre> <p>Performs k-fold cross-validation training.</p> <p>Trains <code>k</code> independent models and stores them in separate subdirectories.</p> <p>Parameters:</p> Name Type Description Default <code>base_model_label_override</code> <code>str | None</code> <p>Base directory name for k-fold outputs.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Learner]</code> <p>list[Learner]: Trained learners, one per fold.</p> Source code in <code>automil/trainer.py</code> <pre><code>def train_k_fold(self, base_model_label_override: str | None = None) -&gt; list[Learner]:\n    \"\"\"\n    Performs k-fold cross-validation training.\n\n    Trains ``k`` independent models and stores them in separate subdirectories.\n\n    Args:\n        base_model_label_override (str | None, optional):\n            Base directory name for k-fold outputs.\n\n    Returns:\n        list[Learner]:\n            Trained learners, one per fold.\n    \"\"\"\n    outdir = self.model_outdir\n    if base_model_label_override:\n        outdir = outdir / base_model_label_override\n    self.vlog(f\"K-Fold output directory: [{INFO_CLR}]{outdir}[/]\")\n\n    learners = []\n    for fold in range(self.k):\n        self.vlog(f\"=\" * 50)\n        self.vlog(f\"Training fold [{INFO_CLR}]{fold + 1}[/]/[{INFO_CLR}]{self.k}[/]\")\n        self.vlog(f\"=\" * 50)\n\n        # Create fold-specific paths and labels\n        if base_model_label_override:\n            fold_label = f\"{base_model_label_override}_fold{fold}\"\n        else:\n            fold_label = None\n\n        # Train this fold\n        learner = self.train(\n            model_label_override=fold_label\n        )\n        learners.append(learner)\n\n    self.vlog(f\"Completed [{INFO_CLR}]{self.k}[/]-fold training\")\n    return learners\n</code></pre>"},{"location":"getstarted/installation/","title":"Installation","text":"<p>This guide provides a quick overview of how to install AutoMIL on your system</p>"},{"location":"getstarted/installation/#github","title":"Github","text":"<p>AutoMIL can be installed directly from its public  GitHub repository. To download the source code, open a terminal, navigate to any directory and run:</p> <pre><code>git clone https://github.com/frankkramer-lab/AutoMIL\n</code></pre> <p>This will clone the projects source code inside a new directory called <code>./automil</code>. Navigate to this directory and install AutoMIL in your current python environment:</p> <pre><code>pip install .\n</code></pre> <p>Use a Virtual Environment</p> <p>We strongly recommend creating a virtual environment inside the project directory. This allows you to install dependencies in isolation without affecting your system-wide Python installation.</p> <p>Inside the project directory, create a virtual environment before installing AutoMIL:</p> <pre><code>python3 -m venv .venv\n</code></pre> <p>Activate the virtual environment:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>Then install AutoMIL:</p> <pre><code>pip install .\n</code></pre>"},{"location":"getstarted/installation/#verify-installation","title":"Verify Installation","text":"<p>To verify correct Installation, try calling the help page via:</p> <pre><code>automil -h\n</code></pre> <p>You can also check the installation path with:</p> <pre><code>which automil\n</code></pre> <p>If the command resolves to a path, the installation was successful</p>"},{"location":"getstarted/quickstart/","title":"Quickstart","text":"<p>This guide walks you through a minimal, end-to-end AutoMIL workflow: from preparing your data to running a first model evaluation. It is intended to provide a high-level overview of the typical AutoMIL pipeline.</p> Dataset Infos <p>For the purposes of this demonstration, we use an example set of whole-slide images from The Cancer Genome Atlas Program (TCGA) consisting of lung tissue samples. This dataset can be downloaded via slideflows project module, which provides the slides in the form of a preconfigured project named <code>LungAdenoSquam</code>. Since the indiviual image files are quite large and the full project contains 941 slides, we restrict this example to a randomly sampled subset of 100 slides. The subset can be replicated using this annotation file. Provide this file to slideflows API to make sure only the annotated slides are downloaded:</p> <pre><code>    #!/usr/bin/env python3\n    import slideflow as sf\n    from slideflow.project_utils import LungAdenoSquam\n\n    if __name__ == \"__main__\":\n\n        project = sf.create_project(\n            root='LungAdenoSquam',\n            cfg=LungAdenoSquam().__dict__,\n            annotations=\"path/to/lung_labels.csv\",\n            download=True\n        )\n</code></pre>"},{"location":"getstarted/quickstart/#optional-1-activate-your-environment","title":"(Optional) 1. Activate Your Environment","text":"<p>If you installed AutoMIL in a virtual environment, ensure your venv is activated before running AutoMIL.</p> <pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"getstarted/quickstart/#2-prepare-the-dataset","title":"2. Prepare the Dataset","text":"<p>AutoMIL expects your WSI dataset to consist of slide images in one of many supported formats (.tiff, .svs, .tif etc) and a file containing slide-level label information </p> <p>A minimal dataset consists of:</p> <ul> <li>A directory containing slide images</li> <li>A .csv metadata file with slide-level annotations</li> </ul> <p>Example directory structure:</p> <pre><code>./LungAdenoSquam/\n\u251c\u2500\u2500 slides/\n    \u251c\u2500\u2500 TCGA-05-4430-01Z-00-DX1.95659bbb-3091-4370-bc1d-6c6c1baa7b3d.svs\n    \u251c\u2500\u2500 TCGA-55-A48Z-01Z-00-DX1.0867DC6A-2A51-4CF1-AE3F-0526CE2DD740.svs\n    \u251c\u2500\u2500 TCGA-55-A4DG-01Z-00-DX1.9CE9B7BE-48EF-44F1-9C25-F15700A3E5DE.svs\n    \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 lung_labels.csv\n</code></pre> <p>With annotations.csv:</p> <pre><code>patient,subtype,site,slide\nTCGA-05-4430,adenocarcinoma,Site-61,TCGA-05-4430-01Z-00-DX1.95659bbb-3091-4370-bc1d-6c6c1baa7b3d\nTCGA-55-A48Z,adenocarcinoma,Site-67,TCGA-55-A48Z-01Z-00-DX1.0867DC6A-2A51-4CF1-AE3F-0526CE2DD740\nTCGA-55-A4DG,adenocarcinoma,Site-67,TCGA-55-A4DG-01Z-00-DX1.9CE9B7BE-48EF-44F1-9C25-F15700A3E5DE\n</code></pre>"},{"location":"getstarted/quickstart/#3-run-the-basic-training-pipeline","title":"3. Run the basic training pipeline","text":"<p>To train a basic Attention_MIL model on the dataset, run the <code>automil train</code> command with default parameters:</p> <pre><code>automil train ./LungAdenoSquam/slides ./LungAdenoSquam/lung_labels.csv results -v -lc \"subtype\" -sc \"slide\"\n</code></pre> <p>AutoMIL expects the column containing labels to be named <code>label</code> and the slide names to be identical to the patient identifiers in the <code>patient</code> column. Using the <code>-lc | --label-colum</code> option, the label column name can be overriden and using the <code>-sc | --slide-column</code> option, a specific column containing slide identifiers can be provided</p> <p>Using the verbose flag <code>-v</code>, automil will display additional information messages, giving an overview of the pipelines progress:</p> <pre><code>INFO     Executing command:                                          \n        /data/jonas/Master/AutoMIL/.venv/bin/automil train          \n        ./Datasets/LungAdenoSquam/slides                            \n        ./Datasets/LungAdenoSquam/annotations_balanced.csv results  \n        -v -lc subtype -sc slide                                    \nINFO     Using resolution presets: ['Low']                           \nINFO     Using model type: Attention_MIL                             \nINFO     Using default image backend cucim                           \nINFO     Project directory results already exists                    \nINFO     Annotations saved to results/annotations.csv                \nINFO     Project scaffold setup complete                             \nINFO     Loading existing project at results                         \nINFO     Project Summary                                             \nINFO     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                              \n        \u2502Project Directory:    \u2502 results                                         \u2502                                                           \n        \u2502Slide Directory:      \u2502 Datasets/LungAdenoSquam/slides                  \u2502                                                           \n        \u2502Annotations File:     \u2502 Datasets/LungAdenoSquam/annotations_balanced.csv\u2502           \n        \u2502Patient Column:       \u2502 patient                                         \u2502                                                           \n        \u2502Label Column:         \u2502 subtype                                         \u2502                                                           \n        \u2502Slide Column:         \u2502 slide                                           \u2502                                                           \n        \u2502Transform Labels:     \u2502 False                                           \u2502                                                           \n        \u2502Modified Annotations: \u2502 results/annotations.csv                         \u2502                                                           \n        \u2502Slideflow Project:    \u2502 Loaded                                          \u2502                                                           \n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                              \n\nINFO     Setting up dataset for resolution preset: Low               \nINFO     Computed average MPP across slides: 0.260                   \nINFO     Dataset Summary                                             \nINFO     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                              \n        \u2502Resolution Preset \u2502 Low     \u2502                              \n        \u2502Tile Size (px)    \u2502 1000px  \u2502                              \n        \u2502Magnification     \u2502 10x     \u2502                              \n        \u2502Microns-Per-Pixel \u2502 0.260   \u2502                              \n        \u2502Tile Size (\u00b5m)    \u2502 260.00\u00b5m\u2502                              \n        \u2502Pretiled Input    \u2502 False   \u2502                              \n        \u2502TIFF Conversion   \u2502 False   \u2502                              \n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \n\nINFO     Preparing dataset source at resolution Low (1000px,         \n        260.00um) \n</code></pre> <p>If not already done, tile extraction and feature bag generation will commence, and the resulting data will be stored <code>results/tfrecords</code> and <code>`results/bags</code> respectively</p> <pre><code>[16:24:01] INFO     Finished tile extraction for TCGA-85-8664 (1605 tiles of    \n                    1605 possible)                                              \n           INFO     No ROI for                                                  \n                    TCGA-98-A53J-01Z-00-DX1.EEC6256E-D331-4731-B00C-08622C725F61\n                    , using whole slide.                                        \n[16:27:14] INFO     Finished tile extraction for TCGA-98-A53J (1595 tiles of    \n                    1595 possible)                                              \n           INFO     No ROI for                                                  \n                    TCGA-44-8119-01Z-00-DX1.1EBEBFA7-22DB-4365-9DF8-C4E679C11312\n                    , using whole slide.                                        \n[16:27:41] INFO     Finished tile extraction for TCGA-44-8119 (211 tiles of 211 \n                    possible)                                                   \n[16:27:42] INFO     No ROI for                                                  \n                    TCGA-21-1071-01Z-00-DX1.a9bba825-1c92-4101-9086-c4d1c91117af\n                    , using whole slide.                                        \n[16:28:55] INFO     Finished tile extraction for TCGA-21-1071 (674 tiles of 674 \n                    possible)                                                   \n           INFO     No ROI for                                                  \n                    TCGA-21-1075-01Z-00-DX1.937872ae-4d6f-4d7a-b54f-b7e797cb84b0\n                    , using whole slide.                                        \n[16:29:53] INFO     Finished tile extraction for TCGA-21-1075 (457 tiles of 457 \n                    possible)                                                   \n           INFO     No ROI for                                                  \n                    TCGA-O2-A52V-01Z-00-DX1.561ADDAE-EC55-461A-84B5-535C93E39C56\n                    , using whole slide.                                        \n[16:36:20] INFO     Finished tile extraction for TCGA-O2-A52V (3460 tiles of    \n                    3460 possible) \n</code></pre> <p>Tile Extraction Takes Time</p> <p>Depending on the slide size and the amount of tissue that can be tiled, the tile extraction may take some time (see the timestamps in the log above). If possible, consider using a pretiled dataset</p> <p>Finally, the trained model will be saved in the <code>results/</code> directory under <code>results/models/</code>.</p>"},{"location":"getstarted/quickstart/#4-evaluate-the-trained-model","title":"4. Evaluate the trained model","text":"<p>To evaluate the trained model on the same dataset, run the <code>automil evaluate</code> command:</p> <pre><code>automil evaluate ./LungAdenoSquam/slides ./LungAdenoSquam/lung_labels.csv ./results/bags ./results/models -v -lc \"subtype\" -sc \"slide\" -o \"./results/evaluation\"\n</code></pre> <p>Once again, the verbose flag provides detailed information on the pipelines progress and current state:</p> <pre><code>INFO     Evaluation complete.                                                                                               \nINFO     Model Comparison:                                           \nINFO                         model  Accuracy  AUC   F1               \n        00002-attention_mil-label      0.85 0.94 0.86               \n        00001-attention_mil-label      0.90 0.92 0.90               \n        00000-attention_mil-label      0.85 0.90 0.84               \nINFO     Saved plot 'box_plots' to                                   \n        results/evaluation/figures/box_plots.png                    \nINFO     Saved plot 'model_comparison' to                            \n        results/evaluation/figures/model_comparison.png             \nINFO     Saved plot 'per_class_accuracy' to                          \n        results/evaluation/figures/per_class_accuracy.png           \nINFO     Saved plot 'roc_curves' to                                  \n        results/evaluation/figures/roc_curves.png   \n</code></pre> <p>This will create an evaluation report inside the <code>./results/evaluation</code> directory, containing metrics and plots of the model performance:</p> <p></p> <p>Metric Comparison across all models</p> <p></p> <p>Per Class Accuracy scores across all model</p> <p></p> <p>The ROC curves for all models</p>"},{"location":"getstarted/requirements/","title":"Requirements","text":"<p>This section outlines the software and system requirements needed to run AutoMIL.</p>"},{"location":"getstarted/requirements/#platform-operating-system","title":"Platform / Operating System","text":"<p>AutoMIL builds on top of the  Slideflow framework for WSI processing, dataset management, and model training. While AutoMIL itself is platform-agnostic, slideflow is primarily developed and tested on Linux and depends on system-level libraries such as cuCIM. cuCIM is part of the RAPIDS collection of GPU-accelerated software solutions for data science, all of which are developed for usage on Linux. Consequently, full functionality, stability, and performance of AutoMIL can only be guaranteed on Linux, which is therefore strongly recommended.</p>"},{"location":"getstarted/requirements/#python-version","title":"Python Version","text":"<p>AutoMIL requires Python \u2265 3.11. A 64-bit Python installation is strongly recommended, especially when working with large whole-slide images.</p>"},{"location":"getstarted/requirements/#core-python-dependencies","title":"Core Python Dependencies","text":"<p>The following Python packages are required and are installed automatically when installing AutoMIL via <code>pip install .</code>:</p>"},{"location":"getstarted/requirements/#deep-learning-and-numerical-computing","title":"Deep Learning and Numerical Computing","text":"<ul> <li><code>torch</code></li> <li><code>torchvision</code></li> <li><code>numpy</code></li> <li><code>pandas</code></li> <li><code>scikit-learn</code></li> <li><code>fastai</code></li> </ul>"},{"location":"getstarted/requirements/#visualization-and-interactive-analysis","title":"Visualization and Interactive Analysis","text":"<ul> <li><code>matplotlib</code></li> <li><code>seaborn</code></li> <li><code>imgui</code></li> <li><code>ipython</code></li> </ul>"},{"location":"getstarted/requirements/#whole-slide-image-processing-and-mil-utilities","title":"Whole-Slide Image Processing and MIL Utilities","text":"<ul> <li><code>openslide-python</code></li> <li><code>openslide-bin</code></li> <li><code>slideflow</code></li> <li><code>nystrom-attention</code></li> <li><code>cucim</code></li> </ul>"},{"location":"getstarted/requirements/#general-utilities-and-command-line-interface","title":"General Utilities and Command Line Interface","text":"<ul> <li><code>click</code></li> <li><code>pyrfr</code></li> <li><code>swig</code></li> </ul> <p>No manual installation of these dependencies is required when installing AutoMIL from source.</p>"},{"location":"getstarted/requirements/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getstarted/requirements/#pyvipslibvips","title":"<code>pyvips</code>/<code>libvips</code>","text":"<p>By default, Slideflow uses the image processing library  cuCIM for handling WSIs. In certain edge cases, however, cuCIM is not a reliable solution for image processing tasks. To our knowledge, these cases include:</p> <ol> <li> <p>Working with OME-TIFF files OME-TIFF is a common WSI format that combines TIFF image data with XML metadata. As of now, OME-TIFF is not supported by cuCIM.</p> </li> <li> <p>Using AutoMIL\u2019s PNG \u2192 TIFF conversion pipeline    For PNG-based datasets (which are generally not recommended for WSIs, but are sometimes used in practice), AutoMIL provides an opt-in preprocessing step to convert PNG images to TIFF. cuCIM is not well suited for processing very large PNG images in this context.</p> </li> </ol> <p>For both cases, AutoMIL provides an optional dependency group that installs the pyvips library as an alternative image processing backend. Install it via:</p> <pre><code>pip install .[vips]\n</code></pre> <p>System dependency required for pyvips</p> <p><code>pyvips</code> is only the Python binding for the image processing library   libvips, which must be installed separately   on your system.</p> <pre><code>sudo apt install libvips\n</code></pre>"}]}